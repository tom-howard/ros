{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#com2009-ros-labs","title":"COM2009 ROS Labs","text":"<p>ROS Labs for the COM2009-3009 Robotics Course at The University of Sheffield (and more) </p> <p> </p> <p>By Tom Howard Department of Multidisciplinary Engineering Education  </p> <p>(Image courtesy of Andy Brown)</p> <p>Find out more...</p> <p></p>"},{"location":"about/","title":"Welcome","text":"<p>This is the home of the COM2009 (&amp; 3009) ROS lab course: a practical course designed for the \"COM2009-3009 Robotics\" module, a second-year undergraduate module for Computer Science students in the Department of Computer Science at The University of Sheffield. The lab course is developed by Dr Tom Howard, a University Teacher in the Department of Multidisciplinary Engineering Education.</p> <p>The course here is designed to teach students how to use ROS (the Robot Operating System) to program robots, using a mix of simulation-based learning and real robot hardware. Most of the initial learning is done in simulation, and we've got a WSL-based simulation environment that we use for this. Everything that is taught in simulation is applicable to real robots too, and - through this course - students are able to apply their new-found ROS knowledge to our real TurtleBot3 Waffle Robots in The Diamond.</p>"},{"location":"about/#other-courses","title":"Other Courses","text":"<p>This site is now also used to support the teaching of labs for an AMRC Training Centre module (AMR31001), and the course has also been adapted for a masters-level module for the department of Automatic Control and Systems Engineering (ACS6121). See Other Courses for more details.</p>"},{"location":"about/acknowledgements/","title":"Acknowledgements","text":"<p>These course materials have been informed by a range of other (mostly free and open-access) resources. We recommend you check out these as well, if you're doing one of our courses and want to dig a little deeper:</p> <ul> <li>The Official ROS Tutorials.</li> <li>The Gaitech Online ROS Tutorials.</li> <li>\"What Is ROS?\" and other blogs from the Robotics Back-End.</li> <li>Another excellent (and free) eBook: A Gentle Introduction to ROS by Jason M. O'Kane.</li> <li>The huge range of online ROS courses provided by The Construct.</li> </ul>"},{"location":"about/changelog/","title":"Version History","text":""},{"location":"about/changelog/#iteration-4","title":"Iteration 4","text":"<p>Academic Year: 2023-24 </p> <ul> <li>Overhaul of both COM2009 Assignments due to changes to Diamond Teaching Spaces...</li> <li>Assignment #1 is now completed asynchronously, and the course notes have been changed to \"parts\" rather than \"weeks.\"</li> <li>Content has been moved around slightly but is broadly the same except for an addition of more formal PID section in Part 6 (formerly \"Week 6\") to tie in with lecture material more closely.</li> <li>Overall, assumptions that everyone is working within WSL-ROS are no longer correct (again, due to Diamond PC room changes), so references to this have been removed and made more generic.</li> <li>Assignment #2 is now only 4 tasks rather than 5, but all are now completed with real robots, where previously 3/5 were assessed in simulation instead.</li> <li>Students now have 12 weeks of labs to work on Assignment #2, where previously it was only 6.</li> </ul>"},{"location":"about/changelog/#iteration-3","title":"Iteration 3","text":"<p>Academic Year: 2022-23 </p> <ul> <li>Moved everything (from the Wiki) across to this new site and made lots of tweaks and improvements along the way.</li> <li>Two new labs for AMR31001 have been added.</li> </ul>"},{"location":"about/changelog/#iteration-2","title":"Iteration 2","text":"<p>Academic Year: 2021-22 </p> <ul> <li>Updated for a new release of the WSL-ROS Environment: now running Ubuntu 20.04 and ROS Noetic.</li> <li>All code templates now updated for Python 3, including nice things like f-strings for all string formatting.</li> <li>Week 3 Exercise 4 (Autonomous Navigation) has been revised to use command-line calls to the <code>/move_base_simple</code> action server to make the robot move to navigation goals (rather than using the GUI tools in RViz), in the hope that this will make it easier to see how the same thing could be achieved programmatically instead.</li> <li>Removed an exercise in Week 5 to make it shorter (because it was a bit of a long one originally), but introduced a couple of optional ones instead for those who wish to delve further.</li> </ul>"},{"location":"about/changelog/#iteration-1","title":"Iteration 1","text":"<p>Academic Year: 2020-21 </p> <ul> <li>Initial release of the COM2009 practical ROS course and the COM2009 Wiki.</li> <li>Based on the brand new WSL-ROS environment (running Ubuntu 18.04 and ROS Melodic).</li> </ul>"},{"location":"about/license/","title":"License","text":"<p> This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. </p> <p>You are free to distribute, remix, adapt, and build upon this work (for non-commercial purposes only) as long as credit is given to the original author.</p>"},{"location":"about/robots/","title":"Introducing the Robots","text":""},{"location":"about/robots/#the-turtlebot3-waffle","title":"The TurtleBot3 Waffle","text":""},{"location":"about/robots/#turtlebot-what","title":"Turtlebot what?!","text":"<p>To teach ROS here we use the TurtleBot3 Waffle robot, made by Robotis. This is the 3rd Generation Robot in the TurtleBot family (which has been the reference hardware platform for ROS since 2010). The TurtleBot Robot family exists to provide accessible and relatively low-cost hardware and open-source software on a robot platform, to encourage people to learn robotics and ROS and make it as easy as possible to do so.</p>"},{"location":"about/robots/#ebook","title":"The (Free) TurtleBot3 eBook","text":"<p>The TurtleBot3 Waffle developers (Robotis) have written a book on programming robots with ROS. This is available as a free eBook, which you can download here, and we recommend that you do so! This is a great resource which provides a detailed introduction to what ROS is and how it works, as well as a comprehensive \"Beginners Guide\" to ROS programming. The other great thing about this is that it is tailored to the TurtleBot3 Robot specifically, providing examples of how to use a range of TurtleBot3 packages along with a detailed description of how they work.</p> <p>We recommend that you have a look at this book to learn more about the concepts that you are exploring in this course.</p>"},{"location":"about/robots/#tb3","title":"Our Waffles","text":"<p>Here in the Diamond we have a total of 50 customised TurtleBot3 Waffles specifically for teaching this course:</p> <p> </p> <p>Whether we're working in simulation or with the real thing, the ROS applications that we develop as part of the course are directly transferable between the two (mostly!) </p> <p>The robots that we have are slightly different to the standard TurtleBot3 WafflePi that you can buy from Robotis. We've made a few adjustments, and the full specifications are as follows:</p> <p></p> <p>The robots have the following core hardware elements:</p> <ul> <li>An OpenCR Micro-Controller Board to power and control the wheel motors, distribute power to other hardware elements and provide an interface for additional sensors.</li> <li>An UP Squared Single-Board Computer (SBC) with an Intel Processor and 32GB of on-board eMMC storage. This board acts as the \"brain\" of the robot.</li> <li>Independent left and right wheel motors (DYNAMIXEL XM430\u2019s) to drive the robot using a differential drive configuration.</li> </ul> <p>This drive configuration allows the robots to move with the following maximum velocities: </p> <p> Velocity Component Upper Limit Units Linear 0.26 m/s Angular 1.82 rad/s <p></p> <p>In addition to this, the robots are equipped with the following sensors:</p> <ul> <li>A Light Detection and Ranging (or LiDAR) sensor, which spins continuously when the robot is in operation. This uses light in the form of laser pulses to allow the robot to measure the distance to surrounding objects, providing it with a 360\u00b0 view of its environment.</li> <li>An Intel RealSense D435 Camera with left and right imaging sensors, allowing depth sensing as well as standard image capture.</li> <li>A 9-Axis Inertial Measurement Unit (or IMU) on-board the OpenCR Micro Controller board, which uses an accelerometer, gyroscope and magnetometer to measure the robot's specific force, acceleration and orientation. </li> <li>Encoders in each of the DYNAMIXEL wheel motors, allowing measurement of speed and rotation count for each of the wheels.</li> </ul>"},{"location":"about/robots/#ros-version","title":"ROS Version","text":"<p>Our robots run on the most up-to-date version of ROS1: ROS Noetic Ninjemys (or \"ROS Noetic\" for short). Our courses here are therefore based around this version of ROS. ROS1 is best installed on the Ubuntu Operating System for stability, reliability and ease, and ROS Noetic runs on Ubuntu 20.04 (Focal Fossa).</p>"},{"location":"about/robots/#other-tech-in-the-diamond","title":"Other Tech in the Diamond","text":""},{"location":"about/robots/#laptops","title":"Laptops","text":"<p>In the Diamond, we have dedicated Robot Laptops running the same OS &amp; ROS version as above (Ubuntu 20.04 and ROS Noetic). We use these when working with the robots in the lab. See here for more details. </p>"},{"location":"about/robots/#simulation-environment","title":"Simulation Environment","text":"<p>To deliver the simulation-based parts of this course, we've created a custom simulation environment using the Windows Subsystem for Linux (WSL). This has been developed primarily to run on University of Sheffield Managed Desktop Computers, which run Windows 10, but it's also possible to run this on other machines too. We call this simulation environment \"WSL-ROS\". See here for more details.</p>"},{"location":"com2009/","title":"COM2009: Robotics","text":"<p>For the COM2009 Robotics course you must complete two lab assignments:</p> <ul> <li> <p>Assignment #1: \"An Introduction to ROS (the Robot Operating System)\".</p> <p>Here you will learn what ROS is and how to use it. You will complete this assignment individually, and in your own time.</p> <p>Weighting: 25% of the overall COM2009 module mark.</p> </li> <li> <p>Assignment #2: \"Team Robotics Project\".</p> <p>Here you will work in teams of 3-4 to complete a series of real-world robotics tasks using our Tutlebot3 Waffle Robots in the Lab (Diamond Computer Room 5).</p> <p>Weighting: 30% of the overall COM2009 module mark</p> </li> </ul>"},{"location":"com2009/assignment1/","title":"Assignment #1: An Introduction to ROS","text":""},{"location":"com2009/assignment1/#overview","title":"Overview","text":"<p>This assignment is essentially a 6-part course, which you should complete in full and in order. The course is designed to be completed in simulation, so you will therefore need access to a ROS installation which can either be installed on your own machine, or accessed on a range of managed computers across the University of Sheffield campus. See here for more information on how to install or access ROS.</p> <p>Each part of the course comprises a series of step-by-step instructions and exercises to teach you how ROS works and introduce you to the core principles of the framework. The exercises give you the opportunity to see how to apply these principles to practical robotic applications. Completing this course is essential for obtaining all the necessary skills for Assignment #2: the Team Robotics Project, where you will work in teams to program our real TurtleBot3 Waffle robots.  </p>"},{"location":"com2009/assignment1/#the-course","title":"The Course","text":"<ul> <li> <p>Part 1: ROS &amp; Linux Basics</p> <p>In this first part you will learn the basics of ROS and become familiar with some key tools and principles of this framework, allowing you to program robots and work with ROS applications effectively.</p> </li> <li> <p>Part 2: Odometry &amp; Navigation</p> <p>In this session you'll learn about Odometry data, which informs us of a robot's position in an environment. You'll also learn how to control a ROS robot's velocity (and thus its position) using both open and closed-loop control methods.</p> </li> <li> <p>Part 3: SLAM &amp; Autonomous Navigation</p> <p>Here you'll take your first look at the LiDAR sensor, the data that it generates, and how this can be of huge benefit for robotics applications. You'll see this in practice by leveraging the mapping and autonomous navigation tools within ROS.</p> </li> <li> <p>Part 4: ROS Services</p> <p>In this part of the course you'll learn about ROS Services, which offer an alternative way for nodes to communicate in ROS. You will see how this framework can be used to control a robot or invoke certain behaviours more effectively for certain tasks.</p> </li> <li> <p>Part 5: ROS Actions</p> <p>Building on what you learnt about ROS Services in Part 4, here you will look at ROS Actions, which are similar to Services, but with a few key differences.</p> </li> <li> <p>Part 6: Cameras, Machine Vision &amp; OpenCV</p> <p>Here you'll learn how to work with images from an on-board camera. You will look at techniques to detect features within these images, and use this to inform robot decision-making.</p> </li> </ul>"},{"location":"com2009/assignment1/#assessment","title":"Assessment","text":"<p>This assignment is worth 25% of the overall mark for the COM2009 course, and is assessed via an on-campus Blackboard-based test taking place in week 7 or 8 of the Spring Semester. </p>"},{"location":"com2009/assignment1/part1/","title":"Part 1: ROS & Linux Basics","text":""},{"location":"com2009/assignment1/part1/#introduction","title":"Introduction","text":"<p> Exercises: 8 Estimated Completion Time: 2 hours</p>"},{"location":"com2009/assignment1/part1/#aims","title":"Aims","text":"<p>In the first part of this lab course you will learn the basics of ROS and become familiar with some key tools and principles of the framework which will allow you to program robots and work with ROS applications effectively.  For the most part, you will interact with ROS using the Linux command line and so you will also become familiar with some key Linux command line tools that will help you.  Finally, you will learn how to create some basic ROS Nodes using Python and get a taste of how ROS topics and messages work.</p>"},{"location":"com2009/assignment1/part1/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:  </p> <ol> <li>Control a TurtleBot3 Robot, in simulation, using ROS.</li> <li>Launch ROS applications using <code>roslaunch</code> and <code>rosrun</code>.</li> <li>Interrogate running ROS applications using key ROS command line tools.</li> <li>Create a ROS package comprised of multiple nodes and program these nodes (in Python) to communicate with one another using ROS Communication Methods.</li> <li>Navigate a Linux filesystem and learn how to do various filesystem operations from within a Linux Terminal.</li> </ol>"},{"location":"com2009/assignment1/part1/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Launching a simulation and making a ROS robot move</li> <li>Exercise 2: Visualising the ROS Network</li> <li>Exercise 3: Exploring ROS Topics and Messages</li> <li>Exercise 4: Creating your own ROS Package</li> <li>Exercise 5: Creating a publisher node</li> <li>Exercise 6: Creating a subscriber node</li> <li>Exercise 7: Exploring a ROS Package and Launch File</li> <li>Exercise 8: Creating a launch file</li> </ul>"},{"location":"com2009/assignment1/part1/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Publisher Code (for Exercise 5)</li> <li>The Subscriber Code (for Exercise 6)</li> </ul>"},{"location":"com2009/assignment1/part1/#first-steps","title":"First Steps","text":"<p>Step 1: Accessing A ROS Environment for this Course</p> <p>If you haven't done so already, see here for all the details on how to install or access a ROS environment for this course.</p> <p>Step 2: Launch ROS</p> <p>Launch your ROS environment.</p> <ol> <li>If you're using WSL-ROS on a university managed desktop machine then follow the instructions here to launch it.</li> <li>If you're running WSL-ROS on your own machine, then you'll need to launch the Windows Terminal to access a WSL-ROS terminal instance.</li> <li>If you opted for a manual install option, then we trust that you know what you're doing already .</li> </ol> <p>Either way, you should now have access to a Linux terminal instance, and we'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 3: Download The Course Repo</p> <p></p> <p>We've put together a few ROS packages of our own that you'll use throughout this course. These all live within this GitHub repo, and you'll need to download and install this into your ROS environment now, before going any further.</p> <ol> <li> <p>In TERMINAL 1, navigate to a folder called the \"Catkin Workspace.\" We'll talk more about this later on but, for now, just run the following command:</p> <pre><code>cd ~/catkin_ws/src/\n</code></pre> </li> <li> <p>Then, run the following command to clone the Course Repo from GitHub:</p> <p>TERMINAL 1: <pre><code>git clone https://github.com/tom-howard/tuos_ros.git\n</code></pre></p> </li> <li> <p>Once this is done, you'll need to compile everything:</p> <p>TERMINAL 1: <pre><code>catkin build &amp;&amp; source ~/.bashrc\n</code></pre></p> <p>Don't worry too much about what you just did, for now. We'll cover this in more detail throughout the course. That's it for now though, we'll start using some of the packages that we've just installed a bit later on...</p> </li> </ol>"},{"location":"com2009/assignment1/part1/#ex1","title":"Exercise 1: Launching a simulation and making a ROS robot move","text":"<p>Now that you're all up and running, let's launch ROS and fire up a simulation of our robot... </p> <ol> <li> <p>In the terminal enter the following command to launch a simulation of a TurtleBot3 Waffle in an empty world:  </p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre></p> </li> <li> <p>A Gazebo simulation window should open and within this you should see a TurtleBot3 Waffle (very similar to our real robots):</p> <p> </p> </li> <li> <p>With your Gazebo Simulation up and running, return to your terminal and open up a second terminal instance (TERMINAL 2). In the Windows Terminal (for instance) you can press the New Tab button: </p> <p> </p> <p>(or press the Ctrl+Shift+T keyboard shortcut).</p> </li> <li> <p>In the new terminal instance enter the following command:</p> <p>TERMINAL 2: <pre><code>rosrun turtlebot3_teleop turtlebot3_teleop_key\n</code></pre></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on your keyboard:</p> <p> </p> </li> </ol>"},{"location":"com2009/assignment1/part1/#summary","title":"Summary","text":"<p>You have just launched a number of different applications on a ROS Network using two different ROS commands - <code>roslaunch</code> and <code>rosrun</code>: </p> <ol> <li><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch</code></li> <li><code>rosrun turtlebot3_teleop turtlebot3_teleop_key</code></li> </ol> <p>These two commands have a similar structure, but work slightly differently. </p> <p>The first command you used was a <code>roslaunch</code> command, which has the following two parts to it (after the <code>roslaunch</code> bit):</p> <pre><code>roslaunch {[1] Package name} {[2] Launch file}\n</code></pre> <p>Part [1] specifies the name of the ROS package containing the functionality that we want to execute. Part [2] is a file within that package that tells ROS exactly what scripts ('nodes') that we want to launch. We can launch multiple nodes at the same time from a single launch file.</p> <p>The second command was a <code>rosrun</code> command, which has a structure similar to <code>roslaunch</code>:</p> <pre><code>rosrun {[1] Package name} {[2] Node name}\n</code></pre> <p>Here, Part [1] is the same as the <code>roslaunch</code> command, but Part [2] is slightly different: <code>{[2] Node name}</code>. Here we are directly specifying a single script that we want to execute. We therefore use <code>rosrun</code> if we only want to launch a single node on the ROS network (<code>turtlebot3_teleop_key</code> in this case, which is a Python script).</p>"},{"location":"com2009/assignment1/part1/#packages-nodes","title":"Packages &amp; Nodes","text":""},{"location":"com2009/assignment1/part1/#packages","title":"ROS Packages","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations), all of which relate to some common robot functionality. ROS uses packages as a way to organise all the programs running on a robot. </p> <p>Info</p> <p>The package system is a fundamental concept in ROS and all ROS programs are organised in this way.</p> <p>You will create a number of packages throughout this course, each containing different nodes, launch files and other things too. We'll start to explore this later on in this part of the course.</p>"},{"location":"com2009/assignment1/part1/#nodes","title":"ROS Nodes","text":"<p>ROS Nodes are executable programs that perform specific robot tasks and operations. Earlier on we used <code>rosrun</code> to launch a node called <code>turtlebot3_teleop_key</code>, which allowed us to remotely control (or \"teleoperate\") the robot, for example. </p> <p>Question</p> <p>What was the name of the ROS package that contained the <code>turtlebot3_teleop_key</code> node? (Remember: <code>rosrun {[1] Package name} {[2] Node name}</code>)</p> <p>A ROS Robot might have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to communicate and share data with the other nodes on the ROS Network.</p>"},{"location":"com2009/assignment1/part1/#the-ros-network","title":"The ROS Network","text":"<p>We can use the <code>rosnode</code> ROS command to view all the nodes that are currently active on a ROS Network.</p>"},{"location":"com2009/assignment1/part1/#ex2","title":"Exercise 2: Visualising the ROS Network","text":"<p>You should currently have two terminal windows active: the first in which you launched the Gazebo simulation (TERMINAL 1) and the second with your <code>turtlebot3_teleop_key</code> node active (TERMINAL 2).</p> <ol> <li>Open up a new terminal instance now (TERMINAL 3).</li> <li> <p>Use the following command to have a look at which nodes are currently active on the network:</p> <p>TERMINAL 3: <pre><code>rosnode list\n</code></pre></p> <p>Only a handful of nodes should be listed:</p> <pre><code>/gazebo\n/gazebo_gui\n/rosout\n/turtlebot3_teleop_keyboard\n</code></pre> </li> <li> <p>We can visualise the connections between the active nodes by using the <code>rqt_graph</code> node within the <code>rqt_graph</code> package. We can use <code>rosrun</code> to launch this node directly (you might see some error messages when you do this, but don't worry about them):</p> <p>TERMINAL 3: <pre><code>rosrun rqt_graph rqt_graph\n</code></pre></p> <p>A new window should then open, displaying something similar to the following (hover over the diagram to enable colour highlighting):</p> <p> </p> <p>This tool shows us that the <code>/turtlebot3_teleop_keyboard</code> and <code>/gazebo</code> nodes are communicating with one another. The direction of the arrow tells us that the <code>/turtlebot3_teleop_keyboard</code> node is a Publisher and the <code>/gazebo</code> node is a Subscriber. The two nodes communicate via a ROS Topic, in this case the <code>/cmd_vel</code> topic, and on this topic the <code>/turtlebot3_teleop_keyboard</code> node publishes messages.</p> </li> </ol>"},{"location":"com2009/assignment1/part1/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics using ROS Messages. We were actually publishing messages to a topic when we made the robot move using the <code>turtlebot3_teleop_key</code> node previously.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"com2009/assignment1/part1/#ex3","title":"Exercise 3: Exploring ROS Topics and Messages","text":"<p>We can find out more about the <code>/cmd_vel</code> topic by using the <code>rostopic</code> ROS command.</p> <ol> <li> <p>Open up yet another new terminal instance (TERMINAL 4) and type the following:</p> <p>TERMINAL 4: <pre><code>rostopic info /cmd_vel\n</code></pre></p> <p>This should provide an output similar to the following:</p> <pre><code>Type: geometry_msgs/Twist\n\nPublishers:\n    * /turtlebot3_teleop_keyboard (http://localhost:#####/)\n\nSubscribers:\n    * /gazebo (http://localhost:#####/)\n</code></pre> <p>This confirms what we discovered earlier about the publisher(s) and subscriber(s) to the <code>/cmd_vel</code> topic.  In addition, this also tells us the topic type, or the type of message that is being published on this topic.</p> <p>Overall, the output above tells us the following:</p> <ol> <li>The <code>/turtlebot3_teleop_keyboard</code> node is currently publishing (i.e. writing data) to the <code>/cmd_vel</code> topic, confirming what we saw from the <code>rqt_graph</code> node before.</li> <li>The <code>/gazebo</code> node is subscribing to the topic. This node is the Gazebo application that's running the simulation of the robot. The node therefore monitors (i.e. subscribes to) the <code>/cmd_vel</code> topic and makes the robot move in the simulator whenever a velocity command is published.</li> <li> <p>The type of message used by the <code>/cmd_vel</code> topic is called: <code>geometry_msgs/Twist</code>. </p> <p>The message type has two parts: <code>geometry_msgs</code> and <code>Twist</code>. <code>geometry_msgs</code> is the name of the ROS package that this message belongs to and <code>Twist</code> is the actual message type. </p> <p>We have just learnt then, that if we want to make the robot move we need to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic.</p> </li> </ol> </li> <li> <p>We can use the <code>rosmsg</code> ROS command to provide further information about this message, or any other message that we may be interested in:</p> <p>TERMINAL 4: <pre><code>rosmsg info geometry_msgs/Twist\n</code></pre></p> <p>From this, we should obtain the following:</p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x\n  float64 y\n  float64 z\ngeometry_msgs/Vector3 angular\n  float64 x\n  float64 y\n  float64 z\n</code></pre> <p>We'll learn more about what this means in Part 2.</p> </li> <li> <p>To finish, enter Ctrl+C in each of the three terminals that should currently have ROS processes running (Terminals 1, 2 and 3). The associated Gazebo and RQT Graph windows should close as a result of this too.</p> </li> </ol> <p>Tip</p> <p>Whenever you need to stop any ROS process use Ctrl+C in the terminal it's running in.</p>"},{"location":"com2009/assignment1/part1/#creating-your-first-ros-applications","title":"Creating Your First ROS Applications","text":""},{"location":"com2009/assignment1/part1/#ex4","title":"Exercise 4: Creating your own ROS Package","text":"<p>In a minute or two you will create some simple publisher and subscriber nodes in Python and send messages between them. As we learnt earlier though, ROS applications must be created within packages, and so we need to create a package first in order to start creating our own ROS nodes. </p> <p>ROS provides a tool to create a new ROS package and ensure that all the essential elements are present: <code>catkin_create_pkg</code>.</p> <p>It's important to work in a specific filesystem location when we create and work on our own ROS packages, so that ROS can access and build everything appropriately. These spaces are called \"Catkin Workspaces\" and one has already been created in the WSL-ROS environment, called <code>catkin_ws</code><sup>1</sup>:</p> <p><pre><code>/home/student/catkin_ws/\n</code></pre> Or: <pre><code>~/catkin_ws/\n</code></pre></p> <p>Note</p> <p><code>~</code> is an alias for your home directory. So <code>cd ~/catkin_ws/</code> is the same as typing <code>cd /home/student/catkin_ws/</code> (in WSL-ROS, the default user is <code>student</code>).</p> <ol> <li> <p>Navigate to the <code>catkin_ws</code> folder by using the Linux <code>cd</code> command (change directory). In TERMINAL 1 enter the following:</p> <p>TERMINAL 1: <pre><code>cd ~/catkin_ws/\n</code></pre></p> </li> <li> <p>Inside the catkin workspace there is a directory called <code>src</code>. All new packages need to be located in the <code>src</code> folder, so we need to be here when we use the <code>catkin_create_pkg</code> tool to create a new package. So, use the <code>cd</code> command again to navigate into the <code>catkin_ws/src</code> folder:</p> <p>TERMINAL 1: <pre><code>cd src\n</code></pre></p> </li> <li> <p>Now, use the <code>catkin_create_pkg</code> script to create a new package called <code>part1_pubsub</code>, and define <code>rospy</code> and <code>std_msgs</code> as dependencies:</p> <p>TERMINAL 1: <pre><code>catkin_create_pkg part1_pubsub rospy std_msgs\n</code></pre></p> <p>Question</p> <p>What did the <code>catkin_create_pkg</code> tool just do? (Hint: there were four things, and it will have told you about them!)</p> </li> <li> <p>Navigate into this new package directory.</p> </li> <li> <p><code>ls</code> is a Linux command which lists the contents of the current directory. Use <code>ls</code> to list the content of the <code>part1_pubsub</code> directory, as created by the <code>catkin_create_pkg</code> tool.</p> <p>Catkin packages are typically organised in the following way, and have a few essential features that must be present in order for the package to be valid:</p> <pre><code>package_folder/    -- All packages must be self-contained within their \n  |                   own root folder [essential]\n  |-launch/        -- A folder for launch files (optional)\n  |-src/           -- A folder for source files (python scripts etc)\n  |-CMakeLists.txt -- Rules for compiling the package [essential]\n  `-package.xml    -- Information about the package [essential]\n</code></pre> <p>You will have noticed that the <code>catkin_create_pkg</code> tool made sure that the essential features of a Catkin Package were created when we asked it to build the <code>part1_pubsub</code> package above.</p> </li> <li> <p>Before we do anything else, it's good practice to now run <code>CMake</code> on the package (using <code>catkin build</code>) to register it on our ROS system and make sure there are no errors with its definition so far:</p> <p>TERMINAL 1: <pre><code>catkin build part1_pubsub\n</code></pre></p> </li> <li> <p>Finally, \"re-source\" your environment<sup>2</sup> using the following command:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>... and you're good to go.</p> <p>Warning</p> <p>You will need run <code>source ~/.bashrc</code> in any other terminals that you have open too, in order for the changes to propagate through to these as well!</p> </li> </ol>"},{"location":"com2009/assignment1/part1/#ex5","title":"Exercise 5: Creating a publisher node","text":"<ol> <li>Within your <code>part1_pubsub</code> package directory, navigate to the <code>src</code> folder using the <code>cd</code> command.</li> <li> <p><code>touch</code> is a Linux command that we can use to create an empty file. Use this to create an empty file called <code>publisher.py</code>, which we will add content to shortly:</p> <p>TERMINAL 1: <pre><code>touch publisher.py\n</code></pre></p> </li> <li> <p>Use <code>ls</code> to verify that the file has been created, but use the <code>-l</code> option with this, so that the command provides its output in \"a long listing format\":</p> <p>TERMINAL 1: <pre><code>ls -l\n</code></pre></p> <p>This should output something similar to the following:</p> <pre><code>total 0\n-rw-r--r-- 1 student student 0 MMM DD HH:MM publisher.py\n</code></pre> <p>This confirms that the file exists, and the <code>0</code> in the middle of the bottom line there indicates that the file is empty (i.e. its current size is 0 bytes), which is what we'd expect.</p> </li> <li> <p>We therefore now need to open the file and add content to it. We'll be using Visual Studio Code (VS Code) as our IDE for this course. Launch VS Code now using the following command in TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>code ~\n</code></pre></p> <p>WSL Users...</p> <p>It's important to launch VS Code within your ROS environment using the \"WSL\" extension. Always remember to check for this.</p> </li> <li> <p>Using the VS Code File Explorer, navigate to your <code>part1_pubsub</code> package directory (<code>~/catkin_ws/src/part1_pubsub/</code>), locate the <code>publisher.py</code> file that you have just created in the <code>/part1_pubsub/src/</code> folder and click on the file to open it. </p> </li> <li> <p>Once opened, copy the code provided here into the empty file and save it. </p> <p>Note</p> <p>It's important that you understand how this code works, so make sure that you read the annotations!</p> </li> <li> <p>Make sure that you've saved the <code>publisher.py</code> file (in VS Code) before trying to run it!</p> <p>Do this by using the Ctrl+S keyboard shortcut, or going to <code>File &gt; Save</code> from the menu at the top of the VS Code screen.</p> </li> <li> <p>We can now run this node using the <code>rosrun</code> ROS command. However, because we closed everything down earlier on, the ROS Master is no longer active. First then, we need to re-launch it manually using <code>roscore</code>:</p> <p>TERMINAL 1: <pre><code>roscore\n</code></pre></p> </li> <li> <p>Then, in TERMINAL 2, use <code>rosrun</code> to execute the <code>publisher.py</code> script that you have just created by providing the relevant information to the <code>rosrun</code> command. Remember: <code>rosrun {package name} {script name}</code>, so:</p> <p>TERMINAL 2: <pre><code>rosrun part1_pubsub publisher.py\n</code></pre></p> <p>... Hmmm, something not quite right? If you typed the command exactly as above and then tried to run it, you probably just received the following error:</p> <pre><code>[rosrun] Couldn't find executable named publisher.py below /home/student/catkin_ws/src/part1_pubsub\n[rosrun] Found the following, but they're either not files,\n[rosrun] or not executable:\n[rosrun]   /home/student/catkin_ws/src/part1_pubsub/src/publisher.py\n</code></pre> <p>The clue there is the word \"executable\". When we create a file, using <code>touch</code> it is given certain permissions. Run <code>ls -l</code> again (making sure that your terminal is in the right location: <code>~/catkin_ws/src/part1_pubsub/src/</code>).</p> <p>The first bit tells us about the permissions that are currently set: <code>-rw-r--r--</code>. This tells us who has permission to do what with this file and (currently) the first bit: <code>-rw-</code>, tells us that we (as the user <code>student</code>) have permission to Read or Write to it. There is a third option we can set too though, which is the execute permission, and we can set this using the <code>chmod</code> Linux command...</p> </li> <li> <p>Run the <code>chmod</code> command as follows:</p> <p>TERMINAL 2: <pre><code>chmod +x publisher.py\n</code></pre></p> </li> <li> <p>Now, run <code>ls -l</code> again to see what has changed:</p> <p>TERMINAL 2: <pre><code>ls -l\n</code></pre></p> <p>We have now granted permission for the file to be eXecuted too:</p> <pre><code>-rwxr-xr-x 1 student student 1557 MMM DD HH:MM publisher.py\n</code></pre> </li> <li> <p>OK, now use <code>rosrun</code> again to (hopefully!) run the <code>publisher.py</code> node (remember: <code>rosrun {package name} {script name}</code>).</p> <p>If you see a message in the terminal similar to the following then the node has been launched successfully:</p> <pre><code>[INFO] [#####]: The 'simple_publisher' node is active...\n</code></pre> <p>Phew!</p> </li> <li> <p>We can further verify that our publisher node is running using a number of different tools. Try running the following commands in TERMINAL 3:</p> <ol> <li><code>rosnode list</code>: This will provide a list of all the nodes that are currently active on the system. Verify that the name of our publisher node is visible in this list.</li> <li><code>rostopic list</code>: This will provide a list of the topics that are currently being used by nodes on the system. Verify that the name of the topic that our publisher is publishing messages to is present within this list.</li> </ol> </li> </ol> <p>Using WSL-ROS on the University Managed Desktops?</p> <p>Remember: any work that you do in the WSL-ROS environment on campus machines will not be preserved automatically. You should therefore backup your work to your University <code>U:\\</code> drive regularly to avoid losing anything. To do this, run the following command in any WSL-ROS terminal instance:</p> <pre><code>wsl_ros backup\n</code></pre>"},{"location":"com2009/assignment1/part1/#rostopic","title":"Using the <code>rostopic</code> command","text":"<p>So far we have used the <code>rostopic</code> ROS command with two additional arguments:</p> <ul> <li><code>list</code>: to provide us with a list of all the topics that are active on our ROS system, and</li> <li><code>info</code>: to provide us with information on a particular topic of interest.</li> </ul> <p>We can use the autocomplete functionality of the Linux terminal to provide us with a list of all the available options that we can use with the <code>rostopic</code> command.  To do this you can type <code>rostopic</code> followed by a Space and then press the Tab key twice:</p> <pre><code>rostopic[SPACE][TAB][TAB]\n</code></pre> <p>You should then be presented with a list of the available arguments for the <code>rostopic</code> command:</p> <p></p> <ul> <li> <p><code>rostopic hz {topic name}</code> provides information on the frequency (in Hz) at which messages are being published to a topic:</p> <pre><code>rostopic hz /chatter\n</code></pre> <p>This should tell us that our publisher node is publishing messages to the <code>/chatter</code> topic at (or close to) 10 Hz, which is exactly what we ask for in the <code>publisher.py</code> file (in the <code>__init__</code> part of our <code>Publisher</code> class). Enter Ctrl+C to stop this command.</p> </li> <li> <p><code>rostopic echo {topic name}</code> shows the messages being published to a topic:</p> <pre><code>rostopic echo /chatter\n</code></pre> <p>This will provide a live stream of the messages that our <code>publisher.py</code> node is publishing to the <code>/chatter</code> topic. Enter Ctrl+C to stop this.</p> </li> <li> <p>We can see some additional options for this command by viewing the help documentation:</p> <pre><code>rostopic echo -h\n</code></pre> <p>From here, for instance, we can learn that if we just wanted the echo command to display a set number of messages from the <code>/chatter</code> topic we could use the <code>-n</code> option. To display the most recent two messages only, for example:</p> <pre><code>rostopic echo /chatter -n2\n</code></pre> </li> </ul>"},{"location":"com2009/assignment1/part1/#ex6","title":"Exercise 6: Creating a subscriber node","text":"<p>You will now create another node to subscribe to the topic that our publisher node is broadcasting messages to, to illustrate how information can be passed from one node to another, via topic messages.</p> <ol> <li>In TERMINAL 3 use the filesystem commands that were introduced earlier (<code>cd</code>, <code>ls</code> and <code>roscd</code>) to navigate to the <code>src</code> folder of your <code>part1_pubsub</code> package.</li> <li>Use the same procedure as before to create a new empty Python file called <code>subscriber.py</code> and remember to make it executable! </li> <li> <p>Then, open the newly created <code>subscriber.py</code> file in VS Code, paste in the code here and save it. Once again, it's important that you understand how this code works, so make sure you read the code annotations! </p> </li> <li> <p>Use <code>rosrun</code> to execute your newly created <code>subscriber.py</code> node (remember: <code>rosrun {package name} {script name}</code>). If your publisher and subscriber nodes are working correctly you should see an output like this:</p> <p> </p> </li> <li> <p>As before, we can find out what nodes are running on our system by using the <code>rosnode list</code> command. Run this in TERMINAL 4 and see if you can identify the nodes that you have just launched.</p> </li> <li> <p>Finally, close down your publisher and subscriber nodes and the ROS Master by entering Ctrl+C in Terminals 1, 2 and 3.</p> </li> </ol> <p>Advanced:  </p> <p>You've now created a publisher and subscriber, both of which were able to communicate with one another over the <code>/chatter</code> topic, using the <code>String</code> standard ROS message type. This message is provided, by ROS, as part of the <code>std_msgs</code> package, but there are other simple message types within this package that we can use too to pass data around a ROS network too, one of which is <code>Float64</code>.</p> <ul> <li>How could you adapt your publisher and subscriber nodes to use the <code>Float64</code> message type, instead of <code>String</code>?</li> </ul>"},{"location":"com2009/assignment1/part1/#launch-files","title":"Launch Files","text":"<p>At the beginning of this session we launched the Gazebo Simulation of our robot using a launch file and the <code>roslaunch</code> command. This provides a means to launch multiple ROS nodes simultaneously, and we will demonstrate this by building a launch file for the publisher and subscriber nodes that we created in the previous exercises. Launch files must be located within a package, so first let's have a look at a package that already exists, and explore the launch file within it.</p>"},{"location":"com2009/assignment1/part1/#ex7","title":"Exercise 7: Exploring a ROS Package and Launch File","text":"<p>We launched the <code>turtlebot3_teleop_key</code> node earlier using <code>rosrun</code>, but there's also a way to launch this using a launch file. To investigate this, we need to look inside the package within which the <code>turtlebot3_teleop_key</code> node exists.</p> <ol> <li> <p>We launched the <code>turtlebot3_teleop_key</code> node with <code>rosrun</code> as follows: <code>rosrun turtlebot3_teleop turtlebot3_teleop_key</code>, which means that the node is located within a package called <code>turtlebot3_teleop</code>. <code>roscd</code> is a ROS command that allows us to navigate to the directory of any ROS package installed on our system, without us needing to know the path to the package beforehand.</p> </li> <li> <p>In TERMINAL 3 use the <code>roscd</code> command to navigate to the <code>turtlebot3_teleop</code> package directory on the Linux filesystem:</p> <p>TERMINAL 3: <pre><code>roscd turtlebot3_teleop\n</code></pre></p> <p>The terminal prompt should have changed to illustrate where on the filesystem the <code>roscd</code> command has just taken you:</p> <p> </p> </li> <li> <p><code>pwd</code> is a Linux command which tells us the current filesystem location of our terminal. Enter this command to confirm what the terminal prompt has told us.</p> <p>So, now we know where the <code>turtlebot3_teleop</code> package is located on our machine, and we can then use more Linux commands to explore this further:</p> </li> <li> <p>Use <code>ls</code> to list the contents of the <code>turtlebot3_teleop</code> package directory. <code>ls</code> on its own will simply list the items in the current directory, try this first.</p> </li> <li> <p>Then, use the <code>-F</code> option to find out a little more:</p> <p>TERMINAL 3: <pre><code>ls -F\n</code></pre></p> <p>You will notice that the output has now changed slightly: items followed by a <code>/</code> are folders (aka \"directories\") and items without the <code>/</code> are files (files will often have a file extension too).</p> <p>Questions</p> <ol> <li>How many items were there in the <code>turtlebot3_teleop</code> package directory?</li> <li>How many of these were directories and how many were files? </li> </ol> <p>Launch files for a package should be located in a launch folder within the package directory. Did you notice a <code>launch</code> folder in the output of the <code>ls</code> command above?</p> </li> <li> <p>Use the <code>cd</code> command to navigate into the <code>turtlebot3_teleop</code> package <code>launch</code> folder and then use <code>ls</code> again to see what's in there. </p> <p>In this folder you should see a <code>turtlebot3_teleop_key.launch</code> file. We'll now have a look at the contents of this file...</p> </li> <li> <p><code>cat</code> is a Linux command that we can use to display the contents of a file in the terminal. Use this to display the contents of the <code>turtlebot3_teleop_key.launch</code> file.</p> <p>TERMINAL 3: <pre><code>cat turtlebot3_teleop_key.launch\n</code></pre></p> <p>Question</p> <p>Knowing what you do now, how would you launch the <code>turtlebot_teleop_key</code> node using <code>roslaunch</code> (instead of <code>rosrun</code>)?</p> </li> </ol>"},{"location":"com2009/assignment1/part1/#launch_attributes","title":"Summary","text":"<p>From the output of <code>cat</code> in the step above you should have noticed that the contents of a launch file are contained within a <code>&lt;launch&gt;</code> tag:</p> <pre><code>&lt;launch&gt;\n  ... \n&lt;/launch&gt;\n</code></pre> <p>Within that, we also have (amongst other things) a <code>&lt;node&gt;</code> tag which tells ROS exactly what scripts (\"executables\") to launch and how to launch them:</p> <pre><code>&lt;node pkg=\"turtlebot3_teleop\" type=\"turtlebot3_teleop_key\" name=\"turtlebot3_teleop_keyboard\" output=\"screen\"&gt;\n&lt;/node&gt;\n</code></pre> <p>The attributes here have the following meaning:</p> <ul> <li><code>pkg</code>: The name of the ROS package containing the functionality that we want to launch.</li> <li><code>type</code>: The full name of the script (i.e. ROS Node) that we want to execute within that package (including the file extension, if it has one).</li> <li><code>name</code>: A descriptive name that we want to give to the ROS node, which will be used to register it on the ROS Network.</li> <li><code>output</code>: The place where any output from the node will be printed (either screen where the output will be printed to our terminal window, or log where the output will be printed to a log file).</li> </ul>"},{"location":"com2009/assignment1/part1/#ex8","title":"Exercise 8: Creating a launch file","text":"<ol> <li>In TERMINAL 1, use <code>roscd</code> to navigate to the root of your <code>part1_pubsub</code> package directory.</li> <li> <p>Use the Linux <code>mkdir</code> command to make a new directory in the package root folder called <code>launch</code>:</p> <p>TERMINAL 1: <pre><code>mkdir launch\n</code></pre></p> </li> <li> <p>Use the <code>cd</code> command to enter the <code>launch</code> folder that you just created, then use the <code>touch</code> command (as before) to create a new empty file called <code>pubsub.launch</code>.</p> </li> <li> <p>Open this launch file in VS Code and enter the following text:</p> <pre><code>&lt;launch&gt;\n  &lt;node pkg={BLANK} type={BLANK} name=\"pub_node\" output=\"screen\"&gt;\n  &lt;/node&gt;\n&lt;/launch&gt;\n</code></pre> <p></p> <p>Fill in the Blanks!</p> <p>Referring to what we learned about the format of launch files above, replace each <code>{BLANK}</code> above with the correct text to launch the publisher node that you created in Exercise 5.</p> </li> <li> <p>Use <code>roslaunch</code> to launch this file and test it out as it is (remember: <code>roslaunch {package name} {launch file}</code>). If everything looks OK then carry on to the next step.</p> </li> <li>The code that we've given you above will launch the <code>publisher.py</code> node, but not the <code>subscriber.py</code> node.  Add another <code>&lt;node&gt;</code> tag to your <code>pubsub.launch</code> file to launch the subscriber node as well.</li> <li>The publisher and subscriber nodes and the ROS Master can now all be launched with the <code>roslaunch</code> command and the <code>pubsub.launch</code> file that you have now created.  </li> <li>Launch this in TERMINAL 1 and then use <code>rosnode list</code> in TERMINAL 2 to check that it all works correctly.</li> </ol>"},{"location":"com2009/assignment1/part1/#roslaunch","title":"Summary","text":"<ul> <li><code>roslaunch</code> can be used to launch multiple nodes on a robot from one single command.</li> <li>It will also automatically launch the ROS Master (equivalent to running the <code>roscore</code> command manually) if it isn't already running (did you notice that we didn't have to do this manually in Exercise 8, but we did when we launched our nodes individually, using <code>rosrun</code>, in Exercises 6 &amp; 7?)</li> <li>In the <code>rospy.init(...)</code> functions of our <code>publisher.py</code> and <code>subscriber.py</code> Python scripts, we defined a node name and set <code>anonymous=True</code>. As a result, when we launched our nodes manually using <code>rosrun</code>, the names we defined were honoured, but were appended with a unique combination of numbers.</li> <li>When we launched our nodes using <code>roslaunch</code> however, the node names were set according to what we had defined in the <code>name</code> field of the <code>&lt;node&gt;</code> tag within the launch file, and anything specified within the <code>rospy.init(...)</code> functions of our Python scripts were overwritten as a result.</li> </ul>"},{"location":"com2009/assignment1/part1/#wrapping-up","title":"Wrapping Up","text":"<p>In this session we've learnt about some key concepts in ROS, such as Packages; Launch files; Nodes and the Publisher-Subscriber Communication Method using Topics and Messages.</p> <p>We've learnt how to use some key ROS commands:  </p> <ul> <li><code>roslaunch</code>: to launch multiple ROS Nodes via launch files.</li> <li><code>roscd</code>: to navigate to installed ROS packages using a package name alone.</li> <li><code>rosnode</code>: to display information about active ROS Nodes.</li> <li><code>rosrun</code>: to run executables within a ROS package.</li> <li><code>rostopic</code>: to display information about active ROS topics.</li> <li><code>rosmsg</code>: to display information about all ROS messages that are available to use in a ROS application.</li> <li><code>roscore</code>: to launch the ROS Master: The baseline nodes and programs that are required for ROS to function.</li> </ul> <p>In addition to this we've also learnt how to use <code>catkin_create_pkg</code>, which is a helper script for creating ROS package templates.</p> <p>We have also learnt how to work in the Linux Terminal and navigate a Linux filesystem using key commands such as:</p> <ul> <li><code>pwd</code>: prints the path of the current working directory to show you which directory you're currently located in.</li> <li><code>ls</code>: lists the files in the current directory.</li> <li><code>cd</code>: change directory to move around the file system.</li> <li><code>mkdir</code>: make a new directory (<code>mkdir {new_folder}</code>).</li> <li><code>cat</code>: show the contents of a file.</li> <li><code>chmod</code>: modify file permissions (i.e. to add execute permissions to a file for all users: <code>chmod +x {file}</code>).</li> <li><code>touch</code>: create a file without any content.</li> </ul> <p>Finally, we have learnt how to create basic ROS nodes in Python to both publish and subscribe to ROS topics using standard ROS messages.</p>"},{"location":"com2009/assignment1/part1/#backup","title":"WSL-ROS Managed Desktop Users: Save your work!","text":"<p>Remember, the work you have done in the WSL-ROS environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University <code>U:\\</code> Drive, allowing you to restore it on another managed desktop machine the next time you fire up WSL-ROS.  </p> <ol> <li> <p>\"The name catkin comes from the tail-shaped flower cluster found on willow trees -- a reference to Willow Garage where catkin was created.\" (According to ROS.org)\u00a0\u21a9</p> </li> <li> <p>What does <code>source ~/.bashrc</code> do? See here for an explanation.\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/assignment1/part2/","title":"Part 2: Odometry & Navigation","text":""},{"location":"com2009/assignment1/part2/#introduction","title":"Introduction","text":"<p> Exercises: 5 Estimated Completion Time: 3 hours</p>"},{"location":"com2009/assignment1/part2/#aims","title":"Aims","text":"<p>In Part 2 you will learn how to control a ROS robot's position and velocity from both the command line and through ROS Nodes. You will also learn how to interpret the data that allows us to monitor a robot's position in its physical environment (odometry).  The things you will learn here form the basis for all robot navigation in ROS, from simple open-loop methods to more advanced closed-loop control (both of which you will explore).</p>"},{"location":"com2009/assignment1/part2/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Interpret the Odometry data published by a ROS Robot and identify the parts of these messages that are relevant to a 2-wheeled differential drive robot (such as the TurtleBot3).</li> <li>Develop Python nodes to obtain Odometry messages from an active ROS network and translate them to provide useful information about a robot's pose in a convenient, human-readable way.</li> <li>Implement open-loop velocity control of a robot using ROS command-line tools.</li> <li>Develop Python nodes that use open-loop velocity control methods to make a robot follow a pre-defined motion path.</li> <li>Combine both publisher &amp; subscriber communication methods into a single Python node to implement closed-loop (odometry-based) velocity control of a robot.</li> <li>Explain the limitations of Odometry-based motion control methods. </li> </ol>"},{"location":"com2009/assignment1/part2/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Exploring Odometry Data</li> <li>Exercise 2: Creating a Python node to process Odometry data</li> <li>Exercise 3: Moving a Robot with <code>rostopic</code> in the Terminal</li> <li>Exercise 4: Creating a Python node to make the robot move in a circle</li> <li>Exercise 5: Making your robot follow a Square motion path</li> </ul>"},{"location":"com2009/assignment1/part2/#additional-resources","title":"Additional Resources","text":"<ul> <li>Working with Twist Messages in Python</li> <li>The <code>move_square</code> Template (for Exercise 5)</li> </ul>"},{"location":"com2009/assignment1/part2/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>If you haven't done so already, launch your ROS environment now:</p> <ol> <li>Using WSL-ROS on a university managed desktop machine: follow the instructions here to launch it.</li> <li>Running WSL-ROS on your own machine: launch the Windows Terminal to access a WSL-ROS terminal instance.</li> <li>Other Users: Launch a terminal instance with access to your local ROS installation.</li> </ol> <p>You should now have access to a Linux terminal instance, and we'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 2: Restore your work (WSL-ROS Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS Environment will not be preserved between sessions or across different University computers. At the end of Part 1 you should have run the <code>wsl_ros</code> tool to back up your home directory to your University <code>U:\\</code> Drive. Once WSL-ROS is up and running, you should be prompted to restore this:</p> <p></p> <p>Enter <code>Y</code> to restore your work from last time. You can also restore your work at any time using the following command:</p> <pre><code>wsl_ros restore\n</code></pre> <p>Step 3: Launch VS Code </p> <p>It's also worth launching VS Code now, so that it's ready to go for when you need it later on. </p> <p>WSL Users...</p> <p>It's important to launch VS Code within your ROS environment using the \"WSL\" extension. Always remember to check for this.</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p></p> <p>In Part 1 you should have downloaded and installed The Course Repo into your ROS environment. If you haven't done this yet then go back and do it now. If you have already done it, then it's worth just making sure it's all up-to-date, so run the following command now to do so:</p> <p>TERMINAL 1: <pre><code>roscd &amp;&amp; cd ../src/tuos_ros/ &amp;&amp; git pull\n</code></pre></p> <p>Then run <code>catkin build</code> </p> <pre><code>roscd &amp;&amp; cd .. &amp;&amp; catkin build\n</code></pre> <p>And finally, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Remember</p> <p>If you have any other terminal instances open, then you'll need run <code>source ~/.bashrc</code> in these too, in order for the changes made by <code>catkin build</code> to propagate through to these as well!</p> <p>Step 5: Launch the Robot Simulation</p> <p>In TERMINAL 1 enter the following command to launch a simulation of a TurtleBot3 Waffle in an empty world:  </p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre></p> <p>A Gazebo simulation window should open and within this you should see a TurtleBot3 Waffle in empty space:</p> <p></p> <p>You're all set up and ready to go!</p>"},{"location":"com2009/assignment1/part2/#position-and-velocity","title":"Position and Velocity","text":"<p>Two types of Velocity Command can be issued to any ROS Robot to make it move (and thus change its position):</p> <ul> <li>Linear Velocity: The velocity at which the robot moves forwards or backwards in one of its principal axes.</li> <li>Angular Velocity: The velocity at which the robot rotates about one of its principal axes.</li> </ul>"},{"location":"com2009/assignment1/part2/#principal-axes","title":"Principal Axes","text":"<p>The motion (i.e. the velocity) of any mobile robot can be defined in terms of three principal axes: <code>X</code>, <code>Y</code> and <code>Z</code>. In the context of our TurtleBot3 Waffle, these axes (and the motion about them) are as follows:</p> <p></p> <p>In theory then, a robot can move linearly or angularly about any of these three axes, as shown by the arrows in the figure. That's six Degrees of Freedom (DOFs) in total, achieved based on a robot's design and the actuators it is equipped with. </p> <p>You should hopefully recall from the \"Introducing the Robots\" page that our TurtleBot3 Waffles only have two motors though, so they don't actually have six DOFs! These two motors can be controlled independently, which is known as a \"differential drive\" configuration, and ultimately provides it with a total of two degrees of freedom in total, as illustrated below.</p> <p></p> <p>It can therefore only move linearly in the x-axis (Forwards/Backwards) and angularly in the z-axis (Yaw).</p> <p>It's also worth noting (while we're on the subject of motion) that our TurtleBot3 Waffles have maximum velocity limits, which were also defined on the \"Robots\" page.</p> <p>Question</p> <p>What are the maximum velocity limits of our robots?</p>"},{"location":"com2009/assignment1/part2/#ros-velocity-commands","title":"ROS Velocity Commands","text":"<p>In Part 1 you learnt how to list all the topics that are currently active on a ROS system. Open up a new terminal instance now (TERMINAL 2) and use what you learnt previously to list all the topics that are active on the ROS network now, as a result of launching the Gazebo simulation earlier.</p> <p>Questions</p> <ol> <li>Which topic in the list do you think could be used to control the velocity of the robot?</li> <li>Use the <code>rostopic info</code> command on the topic to find out more about it.</li> </ol> <p>The topic you identified<sup>1</sup> should use a message of the <code>geometry_msgs/Twist</code> type. You'll have to send messages of this type to this topic in order to make the robot move. Use the <code>rosmsg</code> command (as you did in Part 1) to find out more about the format of this message<sup>2</sup>.</p> <p>You should now be looking at a message format that looks like this: </p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x\n  float64 y\n  float64 z\ngeometry_msgs/Vector3 angular\n  float64 x\n  float64 y\n  float64 z\n</code></pre> <p>There are six parameters that we can assign values to here: </p> <ol> <li><code>linear.x</code></li> <li><code>linear.y</code></li> <li><code>linear.z</code></li> <li><code>angular.x</code></li> <li><code>angular.y</code></li> <li><code>angular.z</code></li> </ol> <p>These relate to a robot's six degrees of freedom (about its three principal axes), as we discussed above. These topic messages are therefore formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x  &lt;-- Forwards (or Backwards)\n  float64 y  &lt;-- Left (or Right)\n  float64 z  &lt;-- Up (or Down)\ngeometry_msgs/Vector3 angular\n  float64 x  &lt;-- Roll\n  float64 y  &lt;-- Pitch\n  float64 z  &lt;-- Yaw\n</code></pre> <p>As we also learnt above though, our TurtleBots can only actually move with linear velocity in the x-axis and angular velocity in the z-axis. As a result then, only velocity commands issued to the <code>linear.x</code> (Forwards/Backwards) or <code>angular.z</code> (Yaw) parts of this message will have any effect.</p>"},{"location":"com2009/assignment1/part2/#robot-odometry","title":"Robot Odometry","text":"<p>Another topic that should have appeared when you ran <code>rostopic list</code> earlier is <code>/odom</code>. This topic contains Odometry data, which is also essential for robot navigation and is a basic feedback signal, allowing a robot to approximate its location.</p>"},{"location":"com2009/assignment1/part2/#ex1","title":"Exercise 1: Exploring Odometry Data","text":"<ol> <li> <p>In TERMINAL 2 use the <code>rostopic echo</code> command to display the odometry data currently being published by our simulated robot:</p> <p>TERMINAL 2: <pre><code>rostopic echo -c /odom\n</code></pre></p> <p>Expand the terminal window as necessary so that you can see the whole topic message (it starts with <code>header</code> and ends with <code>---</code>).</p> <p>Question</p> <p>What does the <code>-c</code> option in the command above actually do?</p> </li> <li> <p>Now, you need to launch a new Windows Terminal instance so that you can view it side-by-side with TERMINAL 2. To do this, press the \"New Tab\" button whilst pressing the Shift key. We'll call this one TERMINAL 3. Arrange both windows side-by-side, so you can see what's happening in both, simultaneously.</p> </li> <li> <p>In TERMINAL 3 launch the <code>turtlebot3_teleop_keyboard</code> node as you did last time: </p> <p>TERMINAL 3: <pre><code>rosrun turtlebot3_teleop turtlebot3_teleop_key\n</code></pre></p> </li> <li> <p>In TERMINAL 3 enter A a couple of times to make the robot rotate on the spot.  Observe how the odometry data changes in TERMINAL 2.  Is there anything in the <code>twist</code> part of the <code>/odom</code> message that corresponds to the \"angular vel\" that you are setting in TERMINAL 3? </p> </li> <li>Now press the S key to halt the robot, then press W a couple of times to make the robot drive forwards.  How does the <code>twist</code> part of the message now correspond to the \"linear vel\" setting in TERMINAL 3?</li> <li> <p>Now press D a couple of times and your robot should start to move in a circle.  What linear and angular velocities are you requesting in TERMINAL 3, and how are these represented in the <code>twist</code> part of the <code>/odom</code> message?  What about the <code>pose</code> part of the message?  How is this data changing as your robot moves in a circular path.</p> <p>Question</p> <p>What do you think <code>twist</code> and <code>pose</code> are actually telling us?</p> </li> <li> <p>Press S in TERMINAL 3 to stop the robot (but leave the <code>turtlebot3_teleop_key</code> node running).  Then, press Ctrl+C in TERMINAL 2 to shut down the <code>rostopic echo</code> process. </p> </li> <li> <p>Let's look at the <code>pose</code> part of the <code>Odometry</code> message in more detail now. With the robot stationary, use <code>rosrun</code> to run a Python node that we have created to help illustrate how this relates to the robot's position and orientation in its environment: </p> <p>TERMINAL 2: <pre><code>rosrun tuos_examples robot_pose.py\n</code></pre></p> </li> <li> <p>Now (using the <code>turtlebot3_teleop_key</code> node in TERMINAL 3), drive your robot around again, keeping an eye on the outputs that are being printed by the <code>robot_pose.py</code> node in TERMINAL 2 as you do so.</p> <p>The output of the <code>robot_pose.py</code> node shows you how the robot's position and orientation (i.e. \"pose\") are changing in real-time as you move the robot around. The <code>\"initial\"</code> column tells us the robot's pose when the node was first launched, and the <code>\"current\"</code> column show us what its pose currently is. The <code>\"delta\"</code> column then shows the difference between the two.</p> <p>Question</p> <p>Which pose parameters haven't changed, and is this what you would expect (considering the robot's principal axes, as illustrated above)?</p> </li> <li> <p>Press Ctrl+C in TERMINAL 2 and TERMINAL 3, to stop the <code>robot_pose.py</code> and <code>turtlebot3_teleop_key</code> nodes.  Then, close down TERMINAL 3 so that only one Windows Terminal application remains open with 2 active tabs: TERMINAL 1 and TERMINAL 2.</p> </li> </ol>"},{"location":"com2009/assignment1/part2/#odometry","title":"What is Odometry?","text":"<p>We can learn more about Odometry data by using the <code>rostopic info</code> command:</p> <p>TERMINAL 2: <pre><code>rostopic info /odom\n</code></pre></p> <p>This provides information about the type of message used by this topic:</p> <pre><code>Type: nav_msgs/Odometry  \n</code></pre> <p>We can find out more about this using the <code>rosmsg info</code> command:</p> <p>TERMINAL 2: <pre><code>rosmsg info nav_msgs/Odometry\n</code></pre></p> <p>Which tells us that the <code>nav_msgs/Odometry</code> message contains four base elements:</p> <ol> <li>header</li> <li>child_frame_id</li> <li>pose</li> <li>twist</li> </ol>"},{"location":"com2009/assignment1/part2/#pose","title":"Pose","text":"<p>Pose tells us the position and orientation of the robot relative to an arbitrary reference point (typically where the robot was when it was turned on). The pose is determined from:</p> <ul> <li>Data from the Inertial Measurement Unit (IMU) on the OpenCR board,</li> <li>Data from both the left and right wheel encoders,</li> <li>An estimation of the distance travelled by the robot from its pre-defined reference point (using dead-reckoning).</li> </ul> <p>Position data is important for determining the movement of our robot, and from this we can estimate its location in 3-dimensional space.</p> <p>Orientation is expressed in units of Quaternions, and needs to be converted into Euler angles (in radians) about the principal axes. Fortunately, there are functions within the ROS <code>tf</code> library to do that for us, which we can use in any Python node as follows:</p> <pre><code>from tf.transformations import euler_from_quaternion\n\n(roll, pitch, yaw) = euler_from_quaternion([orientation.x, \n                     orientation.y, orientation.z, orientation.w], \n                     'sxyz')\n</code></pre> <p>Our TurtleBot3 can only move in a 2D plane and so, actually, its pose can be fully represented by 3 parameters: <code>(x,y,\u03b8<sub>z</sub>)</code>, where <code>x</code> and <code>y</code> are the 2D coordinates of the robot in the <code>X-Y</code> plane, and <code>\u03b8<sub>z</sub></code> is the angle of the robot about the <code>z</code> (yaw) axis.</p> <p>Question</p> <p>In the previous exercise, did you notice how the <code>linear_z</code>, <code>theta_x</code> and <code>theta_y</code> values in the <code>delta</code> column all remained at <code>0.000</code>, even when the robot was moving around?</p>"},{"location":"com2009/assignment1/part2/#twist","title":"Twist","text":"<p>Twist tells us the current linear and angular velocities of the robot, and this data comes directly from the wheel encoders.</p> <p>Once again, all of this data is defined in terms of the principal axes, as illustrated in the figure above.</p>"},{"location":"com2009/assignment1/part2/#ex2","title":"Exercise 2: Creating a Python node to process Odometry data","text":"<p>In Part 1 you learnt how to create a package and build simple nodes in Python to publish and subscribe to messages on a topic. In this exercise you will build a new subscriber node, much like you did in the previous session, but this one will subscribe to the <code>/odom</code> topic that we've been talking about above. You'll also create a new package called <code>part2_navigation</code> for this node to live in!</p> <ol> <li> <p>Create a package in the same way as you did in Part 1, this time called <code>part2_navigation</code>, which depends on the <code>rospy</code>, <code>nav_msgs</code> and <code>geometry_msgs</code> libraries. Use the <code>catkin_create_pkg</code> tool as you did in Part 1. Remember to ensure that you are located in the <code>~/catkin_ws/src/</code> directory before you do this though:</p> <p>TERMINAL 2: <pre><code>cd ~/catkin_ws/src/\n</code></pre> Then: <pre><code>catkin_create_pkg part2_navigation {BLANK}\n</code></pre></p> <p></p> <p>Fill in the Blank!</p> <p>Recall how we used the <code>catkin_create_pkg</code> tool in Part 1, but adapt this now for the <code>part2_navigation</code> package, as detailed above.</p> </li> <li> <p>Run <code>catkin build</code> on this:</p> <p>TERMINAL 2: <pre><code>catkin build part2_navigation\n</code></pre> and then re-source your environment: <pre><code>source ~/.bashrc\n</code></pre></p> </li> <li> <p>The subscriber that we will build here will be structured in much the same way as the subscriber that we built in Part 1. The difference now though is that this one will subscribe to the <code>/odom</code> topic (instead of <code>/chatter</code>), and its callback function will therefore receive <code>Odometry</code> type messages (instead of <code>String</code>), so we'll have to deal with those a bit differently. We've created a template for this to help you to get started. Download this into the <code>src</code> directory of your new <code>part2_navigation</code> package now:</p> <p>TERMINAL 2:</p> <ol> <li>Step 1: navigate to the <code>src</code> directory of your <code>part2_navigation</code> package:     <pre><code>cd ~/catkin_ws/src/part2_navigation/src/\n</code></pre></li> <li>Then download the template code from GitHub:     <pre><code>wget -O odom_subscriber.py \\\nhttps://raw.githubusercontent.com/tom-howard/tuos_ros/main/tuos_examples/src/odom_subscriber_template.py\n</code></pre></li> <li>Finally, make this executable using <code>chmod</code>:     <pre><code>chmod +x odom_subscriber.py\n</code></pre></li> </ol> </li> <li> <p>Run this as it is to see what happens to begin with:</p> <p>TERMINAL 2: <pre><code>rosrun part2_navigation odom_subscriber.py\n</code></pre></p> <p>... Hmmm, something's wrong here isn't it!? You may have seen the following error:</p> <pre><code>/usr/bin/env: \u2018python3\\r\u2019: Permission denied\n</code></pre> <p>The clue here is the <code>python3\\r</code> (specifically the <code>\\r</code> bit). This is a Windows line ending... </p> <p>Text files (including things like Python scripts) created on Windows use different line endings (i.e. the characters that signify the end of each line of text) to those created on Linux. Windows uses a \"carriage return\" and a \"line feed\" (<code>\\r\\n</code>) at the end of each line, but Linux uses just a \"line feed\" (<code>\\n</code>)<sup>3</sup>. Because we're working within a Linux environment here (Ubuntu), we must make sure we're using Linux line endings at all times! We can change this easily from inside VS Code... </p> <ol> <li>In the VS Code File Explorer navigate to the <code>~/catkin_ws/src/part2_navigation/src</code> folder and open the <code>odom_subscriber.py</code> file.</li> <li>In the blue bar along the bottom of the VS Code screen (towards the bottom right-hand corner) you should see the text <code>CRLF</code>. Click on this and a menu should then appear at the top of the screen with the text <code>\"Select End of Line Sequence\"</code>.</li> <li>Select the <code>LF</code> option in this menu, then save the file.</li> </ol> <p> </p> </li> <li> <p>OK, the file should run now, so launch it (using <code>rosrun</code> again) and see what it does.</p> </li> <li> <p>Have a think about what's different between this and the subscriber from last time...</p> <p>In the Subscriber from Part 1 we were working with a <code>String</code> type message from the <code>std_msgs</code> package, whereas this time we're using an <code>Odometry</code> message from the <code>nav_msgs</code> package instead - notice how the imports and the callback function have changed as a result of this.</p> </li> <li> <p>You need to add some additional code to the callback function now: </p> <ol> <li>The node needs to print the robot's real-time odometry data to the terminal in the form: <code>(x,y,\u03b8<sub>z</sub>)</code>.</li> <li>The format of the message has already been structured for you, but you need to add in the relevant variables that represent the correct elements of the robot's real-time pose.</li> <li>You'll need to use the <code>euler_from_quaternion</code> function from the <code>tf.transformations</code> library to convert the raw orientation values from Quaternions into Radians. If you need a hint, why not have a look back at this bit from earlier, or at the source code for the <code>robot_pose.py</code> node that we launched from the <code>tuos_examples</code> package in the previous exercise. </li> </ol> </li> <li> <p>Launch your node using <code>rosrun</code> and observe how the output (the formatted odometry data) changes whilst you move the robot around again using the <code>turtlebot3_teleop_key</code> node in a new terminal instance (TERMINAL 3).</p> </li> <li>Stop your <code>odom_subscriber.py</code> node in TERMINAL 2 and the <code>turtlebot3_teleop</code> node in TERMINAL 3 by entering Ctrl+C in each of the terminals.</li> </ol> <p>Using WSL-ROS on the University Managed Desktops?</p> <p>Remember: any work that you do in the WSL-ROS environment on campus machines will not be preserved automatically. You should therefore backup your work to your University <code>U:\\</code> drive regularly to avoid losing anything. To do this, run the following command in any WSL-ROS terminal instance (now!):</p> <pre><code>wsl_ros backup\n</code></pre>"},{"location":"com2009/assignment1/part2/#basic-navigation-open-loop-velocity-control","title":"Basic Navigation: Open-loop Velocity Control","text":""},{"location":"com2009/assignment1/part2/#ex3","title":"Exercise 3: Moving a Robot with <code>rostopic</code> in the Terminal","text":"<p>Warning</p> <p>Make sure that you've stopped the <code>turtlebot3_teleop_key</code> node running in TERMINAL 3 (by entering Ctrl+C ) before starting this exercise.</p> <p>We can use the <code>rostopic pub</code> command to publish data to a topic from a terminal by using the command in the following way:</p> <pre><code>rostopic pub {topic_name} {message_type} {data}\n</code></pre> <p>As we discovered earlier, the <code>/cmd_vel</code> topic is expecting linear and angular data, each with an <code>x</code>, <code>y</code> and <code>z</code> component. We can get further help with formatting this message by using the autocomplete functionality within the terminal. Type the following into TERMINAL 3 hitting the Space and Tab keys on your keyboard as indicated below:</p> <p>TERMINAL 3: <pre><code>rostopic pub /cmd_vel geometry_msgs/Twist[SPACE][TAB]\n</code></pre></p> <p>The full message should then be presented to us:</p> <pre><code>rostopic pub /cmd_vel geometry_msgs/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre> <ol> <li>Scroll back through the message using the \u2190 key on your keyboard and then edit the values of the various parameters, as appropriate. First, define some values that would make the robot rotate on the spot.  Make a note of the command that you used.</li> <li>Enter Ctrl+C in TERMINAL 3 to stop the message from being published.</li> <li>Next, enter a command in TERMINAL 3 to make the robot move in a circle.  Again, make a note of the command that you used.</li> <li>Enter Ctrl+C in TERMINAL 3 to again stop the message from being published.</li> <li>Finally, enter a command to stop the TurtleBot3 and make a note of this too.</li> <li>Enter Ctrl+C in TERMINAL 3 to stop this final message from being published.</li> </ol>"},{"location":"com2009/assignment1/part2/#ex4","title":"Exercise 4: Creating a Python node to make the robot move in a circle","text":"<p>You will now create another node to control the motion of your TurtleBot3 by publishing messages to the <code>/cmd_vel</code> topic. You created a publisher node in Part 1, and you can use this as a starting point.</p> <ol> <li> <p>In TERMINAL 2, ensure that you are still located within the <code>src</code> folder of your <code>part2_navigation</code> package. You could use <code>pwd</code> to check your current working directory, where the output should look like this:</p> <pre><code>/home/student/catkin_ws/src/part2_navigation/src\n</code></pre> <p>If you aren't located here then navigate to this directory using <code>cd</code>.</p> </li> <li> <p>Create a new file called <code>move_circle.py</code>:</p> <p>TERMINAL 2: <pre><code>touch move_circle.py\n</code></pre> ... and make this file executable using the <code>chmod</code> command.</p> </li> <li> <p>Open up this file in VS Code. This node should make the TurtleBot3 move in a circle with a path radius of approximately 0.5 meters: </p> <ul> <li>Your Python node needs to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic in order to make the TurtleBot3 move. See here for some tips on this.</li> <li>Remember that our robots have a maximum linear velocity (<code>linear.x</code>) of 0.26 m/s, and a maximum angular velocity (<code>angular.z</code>) of 1.82 rad/s. </li> <li>Make sure that you code your <code>shutdownhook()</code> correctly so that the robot stops moving when the node is shutdown (via Ctrl+C in the terminal that launched it).</li> </ul> <p>Use this code template to get you started:</p> A template for the move_circle.py node<pre><code>#!/usr/bin/env python3\n\nimport rospy # (1)!\n# (2)!\n\nclass Circle():\n\n    def __init__(self):\n        self.node_name = # (3)!\n\n        self.pub = rospy.Publisher() # (4)!\n        rospy.init_node(self.node_name, anonymous=True)\n        self.rate = # (5)!\n\n        self.ctrl_c = False \n        rospy.on_shutdown(self.shutdownhook) \n\n        rospy.loginfo(f\"The '{self.node_name}' node is active...\") \n\n    def shutdownhook(self):\n        # (6)!\n\n        self.ctrl_c = True\n\n    def main(self):\n        while not self.ctrl_c:\n            # (7)!\n\n\nif __name__ == '__main__':\n    # (8)!\n</code></pre> <ol> <li>This is important, we always need to import <code>rospy</code></li> <li>What other imports might we need here in order to create and publish a message to make the robot move?</li> <li>Give your node a descriptive name - this is the name that it will be given when it is registered on the ROS network, and the one that you would see if you used the <code>rosnode list</code> command.</li> <li>What do you need to add here in order to set up an appropriate publisher to the <code>/cmd_vel</code> topic?</li> <li>Define an appropriate rate for your <code>main()</code> loop to run at.</li> <li>Anything in here will run when the node receives a shutdown request (i.e., we enter Ctrl+C in the terminal). What actions would be important to take here to make sure the node shuts down safely and the robot actually stops moving?</li> <li>You're going to need to create a message here containing appropriate velocities for the robot to move at. Then you'll need to actually publish that message to <code>/cmd_vel</code> (via your <code>self.pub</code> object). Finally, how would you use the <code>self.rate</code> object that you created above to control the execution rate of the while loop?</li> <li>What will you need to do here to instantiate your <code>Circle()</code> class and execute its main functionality?</li> </ol> <p>Refer back to the publisher node from Part 1 to help you as you're working on this. </p> </li> </ol> <p>Advanced feature:</p> <ol> <li>Create a launch file to launch this and your <code>odom_subscriber.py</code> node simultaneously with a single <code>roslaunch</code> command. Refer to the launch file that you created in Part 1 for a reminder on how to do this.</li> </ol> <p>Assignment #2 Checkpoint</p> <p>Having completed Assignment #1 up to this point, you'll now be ready to tackle Assignment #2 Task 1.</p>"},{"location":"com2009/assignment1/part2/#odometry-based-navigation","title":"Odometry-based Navigation","text":"<p>In the previous exercise you created a Python node to make your robot move using open-loop control. To achieve this you published velocity commands to the <code>/cmd_vel</code> topic to make the robot follow a circular motion path.</p> <p>Questions</p> <ol> <li>How do you know if your robot actually achieved the motion path that you were hoping for?</li> <li>In a real-world environment, what external factors might result in your robot not achieving its desired trajectory?</li> </ol> <p>Earlier on you also learnt about Robot Odometry, which is used by the robot to keep track of its position and orientation (aka Pose) in the environment.  This is determined by a process called \"dead-reckoning,\" which is only really an approximation, but it's a fairly good one in any case, and we can use this as a feedback signal to understand if our robot is moving in the way that we expect it to.  We can therefore build on the techniques that we used in the <code>move_circle.py</code> exercise, and now also build in the ability to subscribe to a topic too. In this case, we'll be subscribing to the <code>/odom</code> topic that we worked with a bit (in isolation) in Exercise 2, and use this to provide us with a feedback signal to allow us to implement some basic closed-loop control.</p>"},{"location":"com2009/assignment1/part2/#ex5","title":"Exercise 5: Making your robot follow a Square motion path","text":"<ol> <li>Make sure your <code>move_circle.py</code> node is no longer running in TERMINAL 2, stopping it with Ctrl+C if necessary.</li> <li> <p>Make sure TERMINAL 2 is still located inside your <code>part2_navigation</code> package<sup>4</sup>.</p> </li> <li> <p>Navigate to the package <code>src</code> directory and use the Linux <code>touch</code> command to create a new file called <code>move_square.py</code>:</p> <p>TERMINAL 2: <pre><code>touch move_square.py\n</code></pre></p> </li> <li> <p>Then make this file executable using <code>chmod</code>:</p> <p>TERMINAL 2: <pre><code>chmod +x move_square.py\n</code></pre></p> </li> <li> <p>Use the VS Code File Explorer to navigate to this <code>move_square.py</code> file and open it up, ready for editing.</p> </li> <li>There's a template here to help you with this exercise. Copy and paste the template code into your new <code>move_square.py</code> file to get you started. </li> <li> <p>Run the code as it is to see what happens... </p> <p>Fill in the Blank!</p> <p>Something not quite working as expected? We may have missed out something very crucial on the very first line of the code template, can you work out what it is?!</p> </li> <li> <p>Fill in the blank as required and then adapt the code to make your robot follow a square motion path of 1 x 1 meter dimensions:</p> <ul> <li>The robot's odometry will tell you how much the robot has moved and/or rotated, and so you should use this information to achieve the desired motion path. </li> <li>Your Python node will therefore need to subscribe to the <code>/odom</code> topic as well as publish to <code>/cmd_vel</code>.</li> </ul> </li> </ol> <p>Advanced features:</p> <ol> <li>Adapt the node further to make the robot automatically stop once it has performed two complete loops.</li> <li>Create a launch file to launch this and the <code>odom_subscriber.py</code> node from Exercise 2 simultaneously!</li> </ol> <p>After following a square motion path a few times, your robot should return to the same location that it started from.</p>"},{"location":"com2009/assignment1/part2/#wrapping-up","title":"Wrapping Up","text":"<p>In this session you have learnt how to control the velocity and position of a robot from both the command-line (using ROS command-line tools) and from ROS Nodes by publishing correctly formatted messages to the <code>/cmd_vel</code> topic.  </p> <p>You have also learnt about Odometry, which is published by our robot to the <code>/odom</code> topic.  The odometry data tells us the current linear and angular velocities of our robot in relation to its 3 principal axes.  In addition to this though, it also tells us where in physical space our robot is located and oriented, which is determined based on dead-reckoning. </p> <p>Questions</p> <ol> <li>If odometry is derived from dead-reckoning, what information (sensor/actuator data) is used to do this?</li> <li>Do you see any potential limitations of this?</li> <li>Can a control method that uses odometry as a feedback signal be considered closed-loop control? </li> </ol> <p>Consider reading Chapter 11.1.3 (\"Pose of Robot\") in the ROS Robot Programming eBook that we mentioned here.</p> <p>In the final exercise you also learnt how to develop an odometry-based controller to make your robot follow a square motion path. You will likely have observed some degree of error in this which, as you already know, could be due to the fact that Odometry data is determined by dead-reckoning and is therefore subject to drift and error.  Consider how other factors may impact the accuracy of control too:</p> <p>Questions</p> <ol> <li>How might the rate at which the odometry data is sampled play a role?</li> <li>How quickly can your robot receive new velocity commands, and how quickly can it respond?</li> </ol> <p>Be aware that we did all this in simulation here too. In fact, in a real world environment, this type of navigation might be less effective, since things such as measurement noise and calibration errors can also have considerable impact. You will have the opportunity to experience this first hand later in this course.</p> <p>Ultimately then, we have seen a requirement here for additional information to provide more confidence of a robot's location in its environment, in order to enhance its ability to navigate effectively and avoid crashing into things! We'll explore this further in the next part of this course.</p>"},{"location":"com2009/assignment1/part2/#backup","title":"WSL-ROS Managed Desktop Users: Save your work!","text":"<p>Remember, the work you have done in the WSL-ROS environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University <code>U:\\</code> Drive, allowing you to restore it on another managed desktop machine the next time you fire up WSL-ROS.  </p> <ol> <li> <p>The topic is <code>/cmd_vel</code> (i.e. command velocity).\u00a0\u21a9</p> </li> <li> <p>Answer: <code>rosmsg info geometry_msgs/Twist</code>.\u00a0\u21a9</p> </li> <li> <p>Adapted from: https://www.cs.toronto.edu/~krueger/csc209h/tut/line-endings.html\u00a0\u21a9</p> </li> <li> <p>Remember, you can use the <code>roscd</code> command for this!\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/assignment1/part3/","title":"Part 3: SLAM & Autonomous Navigation","text":""},{"location":"com2009/assignment1/part3/#introduction","title":"Introduction","text":"<p> Exercises: 3 Estimated Completion Time: 1 hour 30 minutes</p>"},{"location":"com2009/assignment1/part3/#aims","title":"Aims","text":"<p>From the work you did in Part 2 you may have started to appreciate the limitations associated with using odometry data alone as a feedback signal when trying to control a robot's position in its environment. In this next part you will explore an alternative data-stream that could be used to aid navigation further. You will leverage some existing ROS libraries and TurtleBot3 packages to explore some really powerful mapping and autonomous navigation methods that are available within ROS.</p>"},{"location":"com2009/assignment1/part3/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Interpret the data that is published to the <code>/scan</code> topic and use existing ROS tools to visualise this.</li> <li>Use existing ROS tools to implement SLAM and build a map of an environment. </li> <li>Leverage existing ROS libraries to make a robot navigate an environment autonomously, using the map that you have generated.</li> <li>Explain how these SLAM and Navigation tools are implemented and what information is required in order to make them work.</li> </ol>"},{"location":"com2009/assignment1/part3/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Using RViz to Visualise Robot Data</li> <li>Exercise 2: Building a map of an environment with SLAM</li> <li>Exercise 3: Navigating an Environment Autonomously</li> </ul>"},{"location":"com2009/assignment1/part3/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>If you haven't done so already, launch your ROS environment now:</p> <ol> <li>Using WSL-ROS on a university managed desktop machine: follow the instructions here to launch it.</li> <li>Running WSL-ROS on your own machine: launch the Windows Terminal to access a WSL-ROS terminal instance.</li> <li>Other Users: Launch a terminal instance with access to your local ROS installation.</li> </ol> <p>You should now have access to a Linux terminal instance, and we'll refer to this terminal instance as TERMINAL 1.</p> <p>Step 2: Restore your work (WSL-ROS Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS Environment will not be preserved between sessions or across different University computers. At the end of Part 2 you should have run the <code>wsl_ros</code> tool to back up your home directory to your University <code>U:\\</code> Drive. Once WSL-ROS is up and running, you should be prompted to restore this:</p> <p></p> <p>Enter <code>Y</code> to restore your work from last time. You can also restore your work at any time using the following command:</p> <pre><code>wsl_ros restore\n</code></pre> <p>Step 3: Launch VS Code </p> <p>It's also worth launching VS Code now, so that it's ready to go for when you need it later on. </p> <p>WSL Users...</p> <p>It's important to launch VS Code within your ROS environment using the \"WSL\" extension. Always remember to check for this.</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>In Part 1 you should have downloaded and installed The Course Repo into your ROS environment. Hopefully you've done this by now, but if you haven't then go back and do it now (you'll need it for some exercises here). If you have already done it, then (once again) it's worth just making sure it's all up-to-date, so run the following command now to do so:</p> <p>TERMINAL 1: <pre><code>roscd &amp;&amp; cd ../src/tuos_ros/ &amp;&amp; git pull\n</code></pre></p> <p>Then run <code>catkin build</code> </p> <pre><code>roscd &amp;&amp; cd .. &amp;&amp; catkin build\n</code></pre> <p>And finally, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Remember</p> <p>If you have any other terminal instances open, then you'll need run <code>source ~/.bashrc</code> in these too, in order for the changes made by <code>catkin build</code> to propagate through to these as well!</p>"},{"location":"com2009/assignment1/part3/#lidar","title":"Laser Displacement Data and The LiDAR Sensor","text":"<p>As you'll know from Part 2, odometry is really important for robot navigation, but it can be subject to drift and accumulated error over time. You may have observed this in simulation during Part 2 Exercise 5, and you would most certainly notice it if you were to do the same on a real robot. Fortunately, our robots have another sensor on-board which provides even richer information about the environment, and we can use this to supplement the odometry information and enhance the robot's navigation capabilities.</p>"},{"location":"com2009/assignment1/part3/#ex1","title":"Exercise 1: Using RViz to Visualise Robot Data","text":"<p>We're now going to place the robot in a more interesting environment than the \"empty world\" we've used in the previous parts of this course so far...</p> <ol> <li> <p>In TERMINAL 1 enter the following command to launch this:</p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_world.launch\n</code></pre></p> <p>A Gazebo simulation should now be launched with a TurtleBot3 Waffle in a new environment:</p> <p> </p> </li> <li> <p>Open a new terminal instance (TERMINAL 2) and enter the following:</p> <p>TERMINAL 2: <pre><code>roslaunch tuos_simulations rviz.launch\n</code></pre></p> <p>On running the command a new window should open:</p> <p> </p> <p>This is RViz, which is a ROS tool that allows us to visualise the data being measured by a robot in real-time. The red dots scattered around the robot represent laser displacement data which is measured by the LiDAR sensor located on the top of the robot.  This data allows the robot to measure the distance to any obstacles in its immediate surroundings. The LiDAR sensor spins continuously, sending out laser pulses as it does so. These laser pulses then bounce off any objects and are reflected back to the sensor. Distance can then be determined based on the time it takes for the pulses to complete the full journey (from the sensor, to the object, and back again), by a process called \"time of flight\". Because the LiDAR sensor spins and performs this process continuously, a full 360\u00b0 scan of the environment can be generated.  In this case (because we are working in simulation here) the data represents the objects surrounding the robot in its simulated environment, so you should notice that the red dots produce an outline that resembles the objects in the world that is being simulated in Gazebo (or partially at least).</p> </li> <li> <p>Next, open up a new terminal instance (TERMINAL 3). Laser displacement data from the LiDAR sensor is published by the robot to the <code>/scan</code> topic. We can use the <code>rostopic info</code> command to find out more about the nodes that are publishing and subscribing to this topic, as well as the message type:</p> <p>TERMINAL 3: <pre><code>rostopic info /scan\n</code></pre> <pre><code>Type: sensor_msgs/LaserScan\n\nPublishers:\n    * /gazebo (http://localhost:#####/)\n\nSubscribers:\n    * /rviz_#### (http://localhost:#####/) \n</code></pre></p> </li> <li> <p>As we can see from above, <code>/scan</code> messages are of the <code>sensor_msgs/LaserScan</code> type, and we can find out more about this message type using the <code>rosmsg info</code> command:</p> <p>TERMINAL 3: <pre><code>rosmsg info sensor_msgs/LaserScan\n</code></pre> <pre><code>std_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\nfloat32 angle_min\nfloat32 angle_max\nfloat32 angle_increment\nfloat32 time_increment\nfloat32 scan_time\nfloat32 range_min\nfloat32 range_max\nfloat32[] ranges\nfloat32[] intensities \n</code></pre></p> </li> </ol>"},{"location":"com2009/assignment1/part3/#interpreting-laserscan-data","title":"Interpreting <code>/LaserScan</code> Data","text":"<p>The <code>LaserScan</code> message is a standardised ROS message (from the <code>sensor_msgs</code> package) that any ROS Robot can use to publish data that it obtains from a Laser Displacement Sensor such as the LiDAR on the TurtleBot3.  You can find the full definition of the message here. Have a look at this to find out more.</p> <p><code>ranges</code> is an array of <code>float32</code> values (we know it's an array of values because of the <code>[]</code> after the data-type). This is the part of the message containing all the actual distance measurements that are being obtained by the LiDAR sensor (in meters).</p> <p>Consider a simplified example here, taken from a TurtleBot3 robot in a much smaller, fully enclosed environment.  In this case, the displacement data from the <code>ranges</code> array is represented by green squares:</p> <p></p> <p>As illustrated in the figure, we can associate each data-point within the <code>ranges</code> array to an angular position by using the <code>angle_min</code>, <code>angle_max</code> and <code>angle_increment</code> values that are also provided within the <code>LaserScan</code> message.  We can use the <code>rostopic echo</code> command to drill down into these elements of the message specifically and find out what their values are:</p> <p><pre><code>$ rostopic echo /scan/angle_min -n1\n0.0\n</code></pre> <pre><code>$ rostopic echo /scan/angle_max -n1\n6.28318977356\n</code></pre> <pre><code>$ rostopic echo /scan/angle_increment -n1\n0.0175019223243\n</code></pre></p> <p>Notice how we were able to access specific variables within the <code>/scan</code> message using <code>rostopic echo</code> here, rather than simply printing the whole thing?</p> <p>Questions</p> <ul> <li>What does the <code>-n1</code> option do, and why is it appropriate to use this here?</li> <li>What do these values represent? (Compare them with the figure above)</li> </ul> <p>The <code>ranges</code> array contains 360 values in total, i.e. a distance measurement at every 1\u00b0 (an <code>angle_increment</code> of 0.0175 radians) around the robot. The first value in the <code>ranges</code> array (<code>ranges[0]</code>) is the distance to the nearest object directly in front of the robot (i.e. at \u03b8 = 0 radians, or <code>angle_min</code>). The last value in the <code>ranges</code> array (<code>ranges[359]</code>) is the distance to the nearest object at 359\u00b0 (i.e. \u03b8 = 6.283 radians, or <code>angle_max</code>) from the front of the robot. <code>ranges[65]</code>, for example, would represent the distance to the closest object at an angle of 65\u00b0 (1.138 radians) from the front of the robot (anti-clockwise), as shown in the figure.</p> <p>The <code>LaserScan</code> message also contains the parameters <code>range_min</code> and <code>range_max</code>, which represent the minimum and maximum distance (in meters) that the LiDAR sensor can detect, respectively. You can use the <code>rostopic echo</code> command to report these directly too.  </p> <p>Question</p> <p>What is the maximum and minimum range of the LiDAR sensor? Use the same technique as we used above to find out.</p> <p>Finally, use the <code>rostopic echo</code> command again to display the <code>ranges</code> portion of the <code>LaserScan</code> topic message. There's a lot of data here (360 data points in fact, as you know from above!) so let's just focus on the data within a 0-65\u00b0 angular range (again, as illustrated in the figure). You can therefore use the <code>rostopic echo</code> command as follows:</p> <pre><code>rostopic echo /scan/ranges[0:65] -c\n</code></pre> <p>We're dropping the <code>-n1</code> option now, so that we can see the data points updating in real-time, but we're introducing the <code>-c</code> option to clear the screen after every message to make things a bit clearer.  You might need to expand the terminal window so that you can see all the data points; data will be bound by square brackets <code>[]</code>, and there should be a <code>---</code> at the end of each message too, to help you confirm that you are viewing the whole thing.</p> <p>The main thing you'll notice here is that there's lots of information, and it changes rapidly! As you have already seen though, it is the numbers that are flying by here that are represented by red dots in RViz.  Head back to the RViz screen to have another look at this now. As you'll no doubt agree, this is a much more useful way to visualise the <code>ranges</code> data, and illustrates how useful RViz can be for interpreting what your robot can see in real-time.</p> <p>What you may also notice is several <code>inf</code> values scattered around the array.  These represent sensor readings that are outside the sensor's measurement range (i.e. greater than <code>range_max</code> or less than <code>range_min</code>), so the sensor can't report a distance measurement in such cases.</p> <p>Note</p> <p>This behaviour is different on the real robots! See Fact-Finding Mission 4 for further info, and be aware of this when developing code for real robots!!</p> <p>Stop the <code>rostopic echo</code> command from running in the terminal window by entering Ctrl+C.</p>"},{"location":"com2009/assignment1/part3/#slam","title":"Simultaneous Localisation and Mapping (SLAM)","text":"<p>In combination, the data from the LiDAR sensor and the robot's odometry (the robot pose specifically) are really powerful, and allow some very useful conclusions to be made about the environment a robot is operating within.  One of the key applications of this data is \"Simultaneous Localisation and Mapping\", or SLAM.  This is a tool that is built into ROS, and allows a robot to build up a map of its environment and locate itself within that map at the same time!  You will now learn how easy it is to leverage this in ROS.</p>"},{"location":"com2009/assignment1/part3/#ex2","title":"Exercise 2: Building a map of an environment with SLAM","text":"<ol> <li> <p>Close down all ROS processes that are running now by entering Ctrl+C in each terminal:</p> <ol> <li>The Gazebo processes in TERMINAL 1.</li> <li>The RViz processes running in TERMINAL 2.</li> </ol> </li> <li> <p>We're going to launch our robot into another new simulated environment now, which we'll be creating a map of using SLAM! To launch the simulation enter the following command in TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_simulations nav_world.launch\n</code></pre></p> <p>The environment that launches should look like this:</p> <p> </p> </li> <li> <p>Now we will launch SLAM to start building a map of this environment. In TERMINAL 2, launch SLAM as follows:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch\n</code></pre></p> <p>This will launch RViz again, and you should be able to see a model of your TurtleBot3 from a top-down view, this time with green dots representing the real-time LiDAR data. The SLAM tools will already have begun processing this data to start building a map of the boundaries that are currently visible to your robot based on its position in the environment.</p> </li> <li> <p>In TERMINAL 3 launch the <code>turtlebot3_teleop_key</code> node (you should know how to do this by now).  Re-arrange and re-size your windows so that you can see Gazebo, RViz and the <code>turtlebot3_teleop_key</code> terminal instances all at the same time:</p> <p> </p> </li> <li> <p>Drive the robot around the arena slowly, using the <code>turtlebot3_teleop_key</code> node, and observe the map being updated in the RViz window as you do so. Drive the robot around until a full map of the environment has been generated.</p> <p> </p> </li> <li> <p>As you're doing this you need to also determine the centre coordinates of the four circles (A, B, C &amp; D) that are printed on the arena floor. Drive your robot into each of these circular zones and stop the robot inside them. As you should remember from Part 2, we can determine the position (and orientation) of a robot in its environment from its odometery, as published to the <code>/odom</code> topic. In Part 2 Exercise 2 you built an odometry subscriber node, so you could launch this now (in a new terminal: TERMINAL 4), and use this to inform you of your robot's <code>x</code> and <code>y</code> position in the environment when located within each of the zone markers:</p> <p>TERMINAL 4: <pre><code>rosrun part2_navigation odom_subscriber.py\n</code></pre></p> <p>Record the zone marker coordinates in a table such as the one below (you'll need this information for the next exercise).</p> <p> Zone X Position (m) Y Position (m) START 0.5 -0.04 A B C D <p></p> <li> <p>Once you've obtained all this data, and you're happy that your robot has built a complete map of the environment, you then need to save this map for later use. We do this using a ROS <code>map_server</code> package.  First, stop the robot by pressing S in TERMINAL 3 and then enter Ctrl+C to shut down the <code>turtlebot3_teleop_key</code> node.</p> </li> <li> <p>Then, remaining in TERMINAL 3, navigate to the root of your <code>part2_navigation</code> package directory and create a new folder in it called <code>maps</code>:</p> <p>TERMINAL 3: <pre><code>roscd part2_navigation\n</code></pre> <pre><code>mkdir maps\n</code></pre></p> </li> <li> <p>Navigate into this new directory:</p> <p>TERMINAL 3: <pre><code>cd maps/\n</code></pre></p> </li> <li> <p>Then, run the <code>map_saver</code> node from the <code>map_server</code> package to save a copy of your map:</p> <p>TERMINAL 3: <pre><code>rosrun map_server map_saver -f {map name}\n</code></pre> Replacing <code>{map name}</code> with a name of your choosing. </p> <p>This will create two files: a <code>{map name}.pgm</code> and a <code>{map name}.yaml</code> file, both of which contain data related to the map that you have just created.  The <code>.pgm</code> file contains an Occupancy Grid Map (OGM), which is used for autonomous navigation in ROS.  Have a look at the map by launching it in an Image Viewer Application called <code>eog</code>:</p> <p>TERMINAL 3: <pre><code>eog {map name}.pgm\n</code></pre></p> <p>A new window should launch containing the map you have just created with SLAM and the <code>map_saver</code> node: </p> <p> </p> <p>White regions represent the area that your robot has determined is open space and that it can freely move within.  Black regions, on the other hand, represent boundaries or objects that have been detected.  Any grey area on the map represents regions that remain unexplored, or that were inaccessible to the robot.</p> </li> <li> <p>Compare the map generated by SLAM to the real simulated environment. In a simulated environment this process should be pretty accurate, and the map should represent the simulated environment very well (unless you didn't allow your robot to travel around and see the whole thing!)  In a real environment this is often not the case.  </p> <p>Questions</p> <ul> <li>How accurately did your robot map the environment?</li> <li>What might impact this when working in a real-world environment?</li> </ul> </li> <li> <p>Close the image using the  button on the right-hand-side of the eog window.</p> </li>"},{"location":"com2009/assignment1/part3/#summary-of-slam","title":"Summary of SLAM","text":"<p>See how easy it was to map an environment in the previous exercise? This works just as well on a real robot in a real environment too (as you will observe in one of the Real Waffle \"Getting Started Exercises\" for Assignment #2). </p> <p>This illustrates the power of ROS: having access to tools such as SLAM, which are built into the ROS framework, makes it really quick and easy for a robotics engineer to start developing robotic applications on top of this. Our job was made even easier here since we used some packages that had been pre-made by the manufacturers of our TurtleBot3 Robots to help us launch SLAM with the right configurations for our exact robot.  If you were developing a robot yourself, or working with a different type of robot, then you might need to do a bit more work in setting up and tuning the SLAM tools to make them work for your own application.</p>"},{"location":"com2009/assignment1/part3/#advanced-navigation-methods","title":"Advanced Navigation Methods","text":"<p>As mentioned above, the map that you created in the previous exercise can now be used by ROS to autonomously navigate the mapped area.  We'll explore this now.</p>"},{"location":"com2009/assignment1/part3/#ex3","title":"Exercise 3: Navigating an Environment Autonomously","text":"<ol> <li>Close down all ROS processes now so that nothing is running (but leave all the terminal windows open).</li> <li>In order to perform autonomous navigation we now need to activate a number of ROS libraries, our simulated environment and also specify some custom parameters, such as the location of our map file. The easiest way to do all of this in one go is to create a launch file. </li> <li> <p>You may have already created a launch directory in your <code>part2_navigation</code> package, but if you haven't then do this now:</p> <p>TERMINAL 1: <pre><code>roscd part2_navigation\n</code></pre> <pre><code>mkdir launch\n</code></pre></p> </li> <li> <p>Next, navigate into this directory and create a new file called <code>navigation.launch</code>:</p> <p>TERMINAL 1: <pre><code>cd launch/\n</code></pre> <pre><code>touch navigation.launch\n</code></pre></p> </li> <li> <p>Open up this file in VS Code and copy and paste the following content: </p> <pre><code>&lt;!-- Adapted from the Robotis \"turtlebot3_navigation\" package: \nhttps://github.com/ROBOTIS-GIT/turtlebot3/blob/master/turtlebot3_navigation/launch/turtlebot3_navigation.launch\n--&gt;\n&lt;launch&gt;\n  &lt;include file=\"$(find tuos_simulations)/launch/nav_world.launch\" /&gt;\n\n  &lt;!-- To be modified --&gt;\n  &lt;arg name=\"map_file\" default=\" $(find part2_navigation)/maps/{map name}.yaml\"/&gt;\n  &lt;arg name=\"initial_pose_x\" default=\"0.0\"/&gt;\n  &lt;arg name=\"initial_pose_y\" default=\"0.0\"/&gt;\n\n  &lt;!-- Other arguments --&gt;\n  &lt;arg name=\"initial_pose_a\" default=\"0.0\"/&gt;\n  &lt;arg name=\"model\" default=\"$(env TURTLEBOT3_MODEL)\" doc=\"model type [burger, waffle, waffle_pi]\"/&gt;\n  &lt;arg name=\"move_forward_only\" default=\"false\"/&gt;\n\n  &lt;!-- Turtlebot3 Bringup --&gt;\n  &lt;include file=\"$(find turtlebot3_bringup)/launch/turtlebot3_remote.launch\"&gt;\n    &lt;arg name=\"model\" value=\"$(arg model)\" /&gt;\n  &lt;/include&gt;\n\n  &lt;!-- Map server --&gt;\n  &lt;node pkg=\"map_server\" name=\"map_server\" type=\"map_server\" args=\"$(arg map_file)\"/&gt;\n\n  &lt;!-- AMCL --&gt;\n  &lt;include file=\"$(find turtlebot3_navigation)/launch/amcl.launch\"&gt;\n    &lt;arg name=\"initial_pose_x\" value=\"$(arg initial_pose_x)\"/&gt;\n    &lt;arg name=\"initial_pose_y\" value=\"$(arg initial_pose_y)\"/&gt;\n    &lt;arg name=\"initial_pose_a\" value=\"$(arg initial_pose_a)\"/&gt;\n  &lt;/include&gt;\n\n  &lt;!-- move_base --&gt;\n  &lt;include file=\"$(find turtlebot3_navigation)/launch/move_base.launch\"&gt;\n    &lt;arg name=\"model\" value=\"$(arg model)\" /&gt;\n    &lt;arg name=\"move_forward_only\" value=\"$(arg move_forward_only)\"/&gt;\n  &lt;/include&gt;\n\n  &lt;!-- rviz --&gt;\n  &lt;node pkg=\"rviz\" type=\"rviz\" name=\"rviz\" required=\"true\"\n    args=\"-d $(find turtlebot3_navigation)/rviz/turtlebot3_navigation.rviz\"/&gt;\n\n&lt;/launch&gt;\n</code></pre> </li> <li> <p>Edit the default values in the three lines below the <code>&lt;!-- To be modified --&gt;</code> line:</p> <ol> <li>Change <code>{map name}</code> to the name of your map file as created in the previous exercise (remove the <code>{}</code>s!).</li> <li>Change the <code>initial_pose_x</code> and <code>initial_pose_y</code> default values. Current these are both set to <code>\"0.0\"</code>, but they need to be set to match the coordinates of the start zone of the <code>tuos_simulations/nav_world</code> environment (we may have given you a clue about these in the table earlier!) </li> </ol> </li> <li> <p>Once you've made these changes, save the file and then launch it:</p> <p></p> <p>TERMINAL 1: <pre><code>{BLANK} part2_navigation navigation.launch\n</code></pre></p> <p>Fill in the Blank!</p> <p>Which ROS command do we use to execute launch files?</p> </li> <li> <p>RViz and Gazebo should be launched, both windows looking something like this:</p> <p> </p> <p>Question</p> <p>How many nodes were actually launched on our ROS Network by executing this launch file?</p> <p>As shown in the figure, in RViz you should see the map that you generated with SLAM earlier.</p> <ul> <li>There should be a \"heatmap\" surrounding your robot and a lot of green arrows scattered all over the place. </li> <li> <p>The green arrows represent the localisation particle cloud, and the fact that these are all scattered across quite a wide area at the moment indicates that there is currently a great deal of uncertainty about the robot's actual pose within the environment. Once we start moving around this will improve and the arrows will start to converge more closely around the robot. </p> <p>This is actually called a \"costmap\", and it illustrates what the robot perceives of its environment: blue regions representing safe space that it can move around in; red regions representing areas where it could collide with an obstacle.</p> </li> <li> <p>Finally, the green dots illustrate the real-time <code>LaserScan</code> data coming from the LiDAR sensor, as we saw earlier. This should be nicely overlaid on top of the boundaries in our map.</p> </li> </ul> </li> <li> <p>To send a navigation goal to our robot we need to issue a request to the move_base action server. We will cover ROS Actions later in this course, but for now, all you really need to know is that we can send a navigation goal by publishing a message to a topic on the ROS network. In TERMINAL 2 run <code>rostopic list</code> and filter this to show only topics related to <code>/move_base</code>:</p> <p>TERMINAL 2: <pre><code>rostopic list | grep /move_base\n</code></pre></p> <p>This will provide quite a long list, but right at the bottom you should see the item <code>/move_base_simple/goal</code>. We'll use this to publish navigation goals to our robot to make it move autonomously using the ROS Navigation Stack.</p> </li> <li> <p>Running <code>rostopic info</code> on this topic will allow us to find out more about it:</p> <p>TERMINAL 2: <pre><code>rostopic info move_base_simple/goal\n</code></pre> <pre><code>Type: geometry_msgs/PoseStamped\n\nPublishers:\n  * /rviz (http://localhost:#####/)\n\nSubscribers:\n  * /move_base (http://localhost:#####/)\n  * /rviz (http://localhost:#####/)\n</code></pre></p> <p>Question</p> <p>What type of message does this topic use, and which ROS package does it live within?</p> </li> <li> <p>Run another command now to find out what the structure of this message is (you did this earlier for the <code>LaserScan</code> messages published to the <code>/scan</code> topic).</p> </li> <li> <p>Knowing all this information now, we can use the <code>rostopic pub</code> command to issue a navigation goal to our robot, via the <code>/move_base_simple/goal</code> topic. This command works exactly the same way as it did when we published messages to the <code>/cmd_vel</code> topic in Part 2 (when we made the robot move at a velocity of our choosing).</p> <p>Remember that the <code>rostopic pub</code> command takes the following format:</p> <pre><code>rostopic pub {topic_name} {message_type} {data}\n</code></pre> <p>...but to make life easier, we can use the autocomplete functionality in our terminal to help us format the message correctly:</p> <pre><code>rostopic pub {topic_name} {message_type}[SPACE][TAB]\n</code></pre> <p>Do this now, (replacing <code>{topic_name}</code> and <code>{message_type}</code> accordingly) to generate the full message structure that we will use to send the navigation goal to the robot, from the terminal. Don't press Enter yet though, as we will need to edit the message data in order to provide a valid navigation goal.</p> </li> <li> <p>There are four things in this message that need to be changed before we can publish it:</p> <ol> <li><code>frame_id: ''</code> should be changed to <code>frame_id: 'map'</code></li> <li> <p>The <code>pose.orientation.w</code> value needs to be changed to <code>1.0</code>:</p> <pre><code>pose:\n  orientation:\n    w: 1.0\n</code></pre> </li> <li> <p>The <code>pose.position.x</code> and <code>pose.position.y</code> parameters define the location, in the environment, that we want the robot to move to, as determined in the previous exercise:</p> <pre><code>pose:\n  position:\n    x: {desired location in x}\n    y: {desired location in y}\n</code></pre> <p>Scroll back through the message using the left arrow key on your keyboard (\u2190), and modify the four parameters of the message accordingly, setting your <code>x</code> and <code>y</code> coordinates to make the robot move to any of the four marker zones in the environment.</p> </li> </ol> </li> <li> <p>Once you're happy, hit Enter and watch the robot move on its own to the location that you specified!</p> <p> </p> <p>Notice how the green particle cloud arrows very quickly converge around the robot as it moves around? This is because the robot is becoming more certain of it's pose (its position and orientation) within the environment as it compares the boundaries its LiDAR sensor can actually see with the boundaries marked out in the map that you supplied to it.</p> </li> <li> <p>Have a go at requesting more goals by issuing further commands in the terminal (using <code>rostopic pub</code>) to make the robot move between each of the four zone markers.</p> </li> </ol>"},{"location":"com2009/assignment1/part3/#summary","title":"Summary","text":"<p>We have just made a robot move by issuing navigation goals to an Action Server on our ROS Network. You will learn more about ROS Actions later on in this course, where you will start to understand how this communication method actually works. You will also learn how to create Action Client Nodes in Python, so that - in theory - everything that you have been doing on the command-line in this exercise could be done programmatically instead.</p> <p>As you have observed in this exercise, in order to use ROS navigation tools to make a robot move autonomously around an environment there are a few important things that we need to provide to the Navigation Stack:</p> <ol> <li>A map of the environment that we want to navigate around.     This means that our robot needs to have already explored the environment once beforehand to know what the environment actually looks like. We drove our robot around manually in this case but, often, some sort of basic exploratory behaviour would be required in the first instance so that the robot can safely move around and create a map (using SLAM) without crashing into things! You will learn more about robotic search/exploration strategies in your lectures.</li> <li>The robot's initial location within the environment.     ...so that it could compare the map file that we supplied to it with what it actually observes in the environment. If we didn't know where the robot was to begin with, then some further exploration would be required to start with, in order for the robot to build confidence in its actual pose in the environment, prior to navigating it.</li> <li>The coordinates of the places we want to navigate to.     This may seem obvious, but it's an extra thing that we need to establish before we are able to navigate autonomously.</li> </ol>"},{"location":"com2009/assignment1/part3/#further-reading","title":"Further Reading","text":"<p>The ROS Robot Programming eBook that we have mentioned previously goes into more detail on how SLAM and the autonomous navigation tools that you have just implemented actually work.  There is information in here on how these tools have been configured to work with the TurtleBot3 robots specifically.  We therefore highly recommend that you download this book and have a read of it.  You should read through Chapters 11.3 (\"SLAM Application\") and 11.4 (\"SLAM Theory\") in particular, and pay particular attention to the following:  </p> <ul> <li>What information is required for SLAM? One of these bits of information may be new to you: how does this relate to Odometry, which you do know about? (See Section 11.3.4)</li> <li>Which nodes are active in the SLAM process and what do they do?  What topics are published and what type of messages do they use?  How does the information flow between the node network?</li> <li>Which SLAM method did we use? What parameters had to be configured for our TurtleBot3 Waffle specifically, and what do all these parameters actually do?</li> <li>What are the 5 steps in the iterative process of pose estimation? </li> </ul> <p>We would also recommend you read Chapter 11.7 (\"Navigation Theory\") too, which should allow you to then answer the following:</p> <ul> <li>What is the algorithm that is used to perform pose estimation?</li> <li>What process is used for trajectory planning? </li> <li>How many nodes do we need to launch to activate the full navigation functionality on our ROS Network? (We asked you this earlier, and the best way to determine it might be to do it experimentally, i.e.: using the <code>rosnode</code> command-line tool perhaps?)!</li> </ul>"},{"location":"com2009/assignment1/part3/#wrapping-up","title":"Wrapping Up","text":"<p>Odometry data is determined by dead-reckoning and control algorithms based on this alone will be subject to drift and accumulated error. </p> <p>Ultimately then, a robot needs additional information to pinpoint its precise location within an environment, and thus enhance its ability to navigate effectively and avoid crashing into things!</p> <p>This additional information can come from a LiDAR sensor, which you learnt about in this session. We explored where this data is published, how we access it, and what it tells us about a robot's immediate environment.  We then looked at some ways odometry and laser displacement data can be combined to perform advanced robotic functions such as the mapping of an environment and the subsequent navigation around it. This is all complicated stuff but, using ROS, we can leverage these tools with relative ease, which illustrates just how powerful ROS can be for developing robotic applications quickly and effectively without having to re-invent the wheel!</p>"},{"location":"com2009/assignment1/part3/#backup","title":"WSL-ROS Managed Desktop Users: Save your work!","text":"<p>Remember, the work you have done in the WSL-ROS environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University <code>U:\\</code> Drive, allowing you to restore it on another managed desktop machine the next time you fire up WSL-ROS.  </p>"},{"location":"com2009/assignment1/part4/","title":"Part 4: ROS Services","text":""},{"location":"com2009/assignment1/part4/#introduction","title":"Introduction","text":"<p> Exercises: 4 Estimated Completion Time: 2 hours</p>"},{"location":"com2009/assignment1/part4/#aims","title":"Aims","text":"<p>In this part you will learn about another communication method that can be used to transmit data/information and invoke actions across a ROS Network: ROS Services. You will learn how ROS Services can be used in combination with the standard publisher/subscriber principles that you already know about, to control a robot more effectively for certain operations.</p>"},{"location":"com2009/assignment1/part4/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Recognise how ROS Services differ from the standard topic-based publisher-subscriber approach, and identify appropriate use-cases for this type of messaging system.</li> <li>Implement Python node pairs to observe services in action, and understand how they work.</li> <li>Invoke different services using a range of service message types.</li> <li>Develop Python Service nodes of your own to perform specific robotic tasks.</li> <li>Harness Services, in combination with LiDAR data, to implement a basic obstacle avoidance behaviour.</li> <li>Demonstrate your understanding of ROS so far by developing a Python node which incorporates elements from this and previous parts of this course.</li> </ol>"},{"location":"com2009/assignment1/part4/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Creating a Service Server in Python and calling it from the command-line</li> <li>Exercise 2: Creating a Python Service Client Node</li> <li>Exercise 3: Making and calling your own Service</li> <li>Exercise 4: Approaching an object using a Service and closed-loop control</li> </ul>"},{"location":"com2009/assignment1/part4/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Service Server Code (for Exercise 1)</li> <li>The Service Client Code (for Exercise 2)</li> <li>Creating a <code>/scan</code> Callback Function</li> </ul>"},{"location":"com2009/assignment1/part4/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>If you haven't done so already, launch your ROS environment now. Having done this, you should now have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS Environment will not be preserved between sessions or across different University computers, and so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS in TERMINAL 1) enter <code>Y</code> to restore this<sup>1</sup>.</p> <p>Step 3: Launch VS Code </p> <p>It's also worth launching VS Code now. WSL users remember to check for this.</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Once again, it's worth quickly checking that the Course Repo is up-to-date before you start on the Part 4 exercises. Go back to Part 1 if you haven't installed it yet (really?!). For the rest of us, see here for how to update.</p> <p>Step 5: Launch the Robot Simulation</p> <p>From TERMINAL 1, launch the TurtleBot3 Waffle \"Empty World\" simulation:</p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre> ...and then wait for the Gazebo window to open:</p> <p></p> Tip <p>You can also use the <code>tb3_empty_world</code> command-line alias to launch the simulation, rather than using that long <code>roslaunch</code> command!</p>"},{"location":"com2009/assignment1/part4/#an-introduction-to-services","title":"An Introduction to Services","text":"<p>So far, we've learnt about ROS topics and messages, and how individual nodes can access data on a robot by simply subscribing to topics that are being published by any other node on the system.  In addition to this, we also learnt how any node can publish messages to any topic: this essentially broadcasts the data contained in the message across the ROS Network, making it available to any other node on the network that may wish to access it.</p> <p>Another way to pass data between ROS Nodes is by using Services.  These are different to messages in that \"Service calls\" (that is, the process of requesting a service) occur only between one node and another:</p> <ul> <li>One node (a Service Client) sends a Request to another node.</li> <li>Another node (a Service Server) processes that request, performs an action and then sends back a Response.</li> </ul> <p></p> <p>Services are Synchronous (or sequential): When a ROS node sends a request to a service (as a Service Client) it can't do anything else until the service has been completed and the Service Server has sent a response back. This can be useful for a few reasons:</p> <ol> <li> <p>Discrete, short-duration actions: A robot might need to do something before it can move on to something else, e.g.:</p> <ul> <li>A robot needs to see something before it can move towards it.</li> <li>High definition cameras generate large amounts of data and consume battery power, so you may wish to turn a camera on for a specific amount of time (e.g. until an image has been captured) and then turn it off again.</li> </ul> </li> <li> <p>Computations: Remember that ROS is network-based, so you might want to offload some computations to a remote computer or a different device on a robot, e.g.:</p> <ul> <li>A client might send some data and then wait for another process (the server) to process it and send back the result.</li> </ul> </li> </ol> <p>It's also worth noting that any number of ROS Client nodes can call a service, but you can only have a single Server providing that particular service at any one time.</p> <p></p> <p>Question</p> <p>Can you think of any other scenarios where this type of communication protocol might be useful?</p> <p>You'll explore how all this works in the next two exercises, where you will create service Server and Client nodes in Python, launch them from the command-line and observe the outcomes.</p>"},{"location":"com2009/assignment1/part4/#ex1","title":"Exercise 1: Creating a Service Server in Python and calling it from the command-line","text":"<p>To start with, let's set up a service and learn how to make a call to it from the command-line to give you an idea of how this all works and why it might be useful.</p> <ol> <li> <p>First open up a new terminal instance (TERMINAL 2) and create a package called <code>part4_services</code> using the <code>catkin_create_pkg</code> tool as you have done previously:</p> <ol> <li> <p>Navigate to the <code>catkin_ws/src</code> directory:</p> <p>TERMINAL 2: <pre><code>cd ~/catkin_ws/src\n</code></pre></p> </li> <li> <p>Create the <code>part4_services</code> package and define <code>rospy</code>, <code>geometry_msgs</code> and <code>tuos_msgs</code> as dependencies:</p> <p>TERMINAL 2: <pre><code>catkin_create_pkg part4_services rospy geometry_msgs tuos_msgs\n</code></pre></p> </li> <li> <p>Run <code>catkin build</code> on the package:</p> <p>TERMINAL 2: <pre><code>catkin build part4_services\n</code></pre></p> </li> <li> <p>And then re-source your environment:</p> <p>TERMINAL 2: <pre><code>source ~/.bashrc\n</code></pre></p> Tip <p>We're having to do this <code>source ~/.bashrc</code> thing a lot aren't we?! We've created a handy alias for it... use <code>src</code> instead!</p> </li> </ol> </li> <li> <p>Then, navigate to your package <code>src</code> folder that should have been created by <code>catkin_create_pkg</code>:</p> <p>TERMINAL 2: <pre><code>roscd part4_services/src\n</code></pre></p> </li> <li> <p>Create a file called <code>move_server.py</code> (using <code>touch</code>) and set this to be executable (using <code>chmod</code>).        </p> </li> <li> <p>Then, open the file in VS Code, copy and paste this code and then save it. </p> <p>Note</p> <p>It's really important that you understand how the code above works, so that you know how to build your own service Servers in Python. Make sure you read the code annotations thoroughly.</p> </li> <li> <p>Return to the terminal window and launch the node using <code>rosrun</code>:</p> <p>TERMINAL 2: <pre><code>rosrun part4_services move_server.py\n</code></pre></p> <p>You should see the message:</p> <pre><code>[INFO] [#####]: the 'move_service' Server is ready to be called...\n</code></pre> </li> <li> <p>Then open up a new terminal window (TERMINAL 3)</p> </li> <li> <p>We can use the <code>rosservice</code> command to view all the services that are currently active on our system:</p> <p>TERMINAL 3: <pre><code>rosservice list\n</code></pre></p> <p>You should see the <code>/move_service</code> service that we defined in the Python code:</p> <pre><code>service_name = \"move_service\"\n</code></pre> </li> <li> <p>We can find out more about this using the <code>rosservice info</code> command:</p> <p>TERMINAL 3: <pre><code>rosservice info /move_service\n</code></pre></p> <p>Which should provide the following output:</p> <pre><code>Node: /move_service_server\nURI: #####\nType: tuos_msgs/SetBool\nArgs: request_signal\n</code></pre> <p>You may notice that the node name is <code>/move_service_server</code>, as set in our Python code when we initialised the node:</p> <pre><code>rospy.init_node(f\"{service_name}_server\")\n</code></pre> <p>Type tells us the type of message this service uses, and we'll look at this in more detail later. Args tells us what input arguments we need to supply to the service in order to make a valid service call (or Request).</p> </li> <li> <p>We can now call this service from the command-line using the <code>rosservice</code> command again.  The autocomplete functionality in the terminal can help us format this message correctly.  Type the following text followed by a space and two tabs as illustrated:</p> <p>TERMINAL 3: <pre><code>rosservice call /move_service[SPACE][TAB]\n</code></pre></p> <p>which should autocomplete the rest of the command for us:</p> <pre><code>rosservice call /move_service \"request_signal: false\"\n</code></pre> </li> <li> <p>Press Enter to issue this command and make a call to the service.  You should see the following response:</p> <pre><code>response_signal: False\nresponse_message: \"Nothing happened, set request_signal to 'true' next time.\"\n</code></pre> </li> <li> <p>Arrange your windows so that you can see both the Gazebo simulation with your robot in, and the terminal that you just issued the <code>rosservice call</code> command (TERMINAL 3).</p> </li> <li> <p>In TERMINAL 3 enter the <code>rosservice call</code> command again, but this time setting the input argument to <code>true</code>.  Observe the response to the simulated robot in Gazebo.  Switch back to TERMINAL 2 and observe the terminal output here too.</p> </li> </ol> <p>Summary:</p> <p>You have just created a node in Python to launch a service. This node acted as a Server: sitting idle and waiting, indefinitely, for its service to be called. We then issued the call to the service via the command-line, which then prompted our Service Server to carry out the tasks that we had defined within the Python code, namely:</p> <ol> <li>Start a timer.</li> <li>Issue a velocity command to the robot to make it move forwards.</li> <li>Wait for 5 seconds.</li> <li>Issue a velocity command to make the robot stop.</li> <li>Prepare a Service Response and issue this to the terminal in which we called the service (TERMINAL 3).</li> </ol>"},{"location":"com2009/assignment1/part4/#rossrv","title":"Using <code>rossrv</code>","text":"<p>In the previous exercise we used <code>rosservice list</code> to identify all the services that were currently active on the ROS system.  We then used <code>rosservice info</code> to find out a bit more about the service that we had launched with our Python node (which we called <code>/move_service</code>).</p> <pre><code>rosservice info /move_service:\n\nNode: /move_service_server\nURI: #####\nType: tuos_msgs/SetBool\nArgs: request_signal\n</code></pre> <p>Type tells us the type of message this service uses. Just like a topic message there are two parts to this definition:</p> <pre><code>tuos_msgs/SetBool\n</code></pre> <ol> <li>The service message is part of a package called <code>tuos_msgs</code></li> <li>The message itself is called <code>SetBool</code></li> </ol> <p>We can find out more about this using the <code>rossrv</code> command, which has the same usage as the <code>rosmsg</code> command that you have already used previously (for interrogating topic messages). <code>rossrv</code> gives us information about all the service messages that are installed on our system and that are available for us to use in any ROS applications that we create:</p> <p>TERMINAL 3: <pre><code>rossrv info tuos_msgs/SetBool\n</code></pre> ... which gives: </p> <pre><code>bool request_signal\n---\nbool response_signal\nstring response_message\n</code></pre>"},{"location":"com2009/assignment1/part4/#the-format-of-a-service-message","title":"The Format of a Service Message","text":"<p>As you can see from above, service messages have two parts to them, separated by three hyphens (<code>---</code>). Above the separator is the Service Request, and below it is the Service Response:</p> <pre><code>bool request_signal     &lt;-- Request\n---\nbool response_signal    &lt;-- Response (Parameter 1 of 2)\nstring response_message &lt;-- Response (Parameter 2 of 2)\n</code></pre> <p>In order to Call a service, we need to provide data to it in the format specified in the Request section of the message. A service Server (like the Python node we created above) will then send data back to the caller in the format specified in the Response section of the message.</p> <p>The <code>tuos_msgs/SetBool</code> service message that we're working with here has a one request parameter:</p> <ol> <li>A boolean input called <code>request_signal</code>     ...which is the only thing we need to send to the Service Server in order to call the service.</li> </ol> <p>There are then two response parameters:</p> <ol> <li>A boolean flag called <code>response_signal</code></li> <li>A text string called <code>response_message</code>     ...both of these will be returned to the client, by the server, once the Service has completed.</li> </ol>"},{"location":"com2009/assignment1/part4/#ex2","title":"Exercise 2: Creating a Python Service Client Node","text":"<p>As well as calling a service from the command-line we can also build Python nodes to do the same thing (i.e. we can build Python Service Client Nodes). In this exercise you will learn how this is done.</p> <ol> <li> <p>TERMINAL 3 should be idle, so from here navigate to the <code>src</code> folder within the <code>part4_services</code> package that we created earlier:</p> <p>TERMINAL 3: <pre><code>roscd part4_services/src\n</code></pre></p> </li> <li> <p>Create a new file called <code>move_client.py</code> and make sure that this is executable.</p> </li> <li> <p>Launch the file in VS Code, copy and paste this code and then save the file. </p> <p>Note</p> <p>Once again, be sure to read the code annotations, and make sure that you understand how this Python Service Client Node works too!</p> </li> <li> <p>Return to TERMINAL 3 and launch the node using <code>rosrun</code>:</p> <p>TERMINAL 3: <pre><code>rosrun part4_services move_client.py\n</code></pre></p> <p>The response should be exactly the same as observed in Exercise 1.</p> </li> </ol>"},{"location":"com2009/assignment1/part4/#ex3","title":"Exercise 3: Making and calling your own Service","text":"<p>In this exercise you will create your own service Server to make the Waffle perform a specific movement for a given amount of time and then stop.</p> <p>A service message called <code>tuos_msgs/TimedMovement</code> has already been set up to help you do this. Interrogate this using the <code>rossrv</code> command (as described above) to work out how to use this message in your Python Server node.</p> <p>The service should respond to four different movement commands to invoke four different actions:</p> <ol> <li><code>\"fwd\"</code>: Move forwards.</li> <li><code>\"back\"</code>: Move backwards.</li> <li><code>\"left\"</code>: Turn left.</li> <li><code>\"right\"</code>: Turn right.</li> </ol> <p>The Server should make the robot perform the desired action for a duration that is also specified within the service message (in seconds).</p> <p>Procedure:</p> <ol> <li>Close down the Service Server that is currently running in TERMINAL 2.</li> <li> <p>Create a new node in your <code>part4_services</code> package:</p> <ol> <li> <p>Navigate to the <code>part4_services/src</code> folder using <code>roscd</code>:</p> <p>TERMINAL 2: <pre><code>roscd part4_services/src\n</code></pre></p> </li> <li> <p>You can use the <code>move_server.py</code> node that you created earlier as a starting point if you want to. Copy the file and rename it <code>timed_move_server.py</code> using the <code>cp</code> command:</p> <p>TERMINAL 2: <pre><code>cp move_server.py timed_move_server.py\n</code></pre></p> </li> </ol> </li> <li> <p>Open the new <code>timed_move_server.py</code> file in VS Code and modify it as follows:</p> <ol> <li>Change the imports to utilise the correct service message type (<code>tuos_msgs/TimedMovement</code>).</li> <li>Modify the <code>rospy.Service</code> call to use the <code>TimedMovement</code> service message type.</li> <li> <p>Develop the <code>callback_function()</code> to:</p> <ol> <li> <p>Process the two parameters that will be provided to the server via the <code>service_request</code> input argument.  </p> <p>Remember</p> <p>You can use <code>rossrv info ...</code> to find out what these two parameters are called, and their data types.</p> </li> <li> <p>Make the robot perform the correct action.</p> </li> <li>Return a correctly formatted service response message to the service caller.</li> <li>Launch your server node using <code>rosrun</code> from TERMINAL 2 and call the service from the command-line using the <code>rosservice call</code> command in TERMINAL 3, as you did earlier.</li> </ol> </li> </ol> </li> </ol>"},{"location":"com2009/assignment1/part4/#a-recap-on-everything-youve-learnt-so-far","title":"A recap on everything you've learnt so far...","text":"<p>You should now hopefully understand how to use the ROS Service architecture and understand why, and in what context, it might be useful to use this type of communication method in a robot application.</p> <p>Remember</p> <p>Services are synchronous and are useful for one-off, quick actions; or for offloading jobs or computations that might need to be done before something else can happen.  (Think of it as a transaction that you might make in a shop: You hand over some money, and in return you get a chocolate bar, for example!)</p> <p>Throughout this course so far we've learnt how to use a range of key ROS tools, and hopefully you're starting to understand how ROS works and how you might approach a robot programming task using this framework. In the final exercise now you'll consolidate some of the things that you've done so far:</p> <ul> <li>Publishing and subscribing to topics.</li> <li>Making a robot move.</li> <li>Interpreting Laser Displacement Data from the LiDAR sensor.</li> <li>Invoking a behaviour using a ROS Service.</li> <li>Develop ROS Nodes in Python, using the Python Class Structure.</li> </ul>"},{"location":"com2009/assignment1/part4/#sim-env-mods","title":"Manipulating the Environment in Gazebo","text":"<p>In order to carry out the last exercise you'll also need to be able to manipulate the robot's simulated environment using some basic tools in Gazebo. First, make sure that there are no active processes running in TERMINALS 2 or 3, but leave the Gazebo simulation in TERMINAL 1 running.</p> <p>In the Gazebo simulation window, use the \"Box\" tool in the top toolbar to place a box in front of the robot:</p> <p></p> <p>Use the \"Scale Mode\" button to resize the box and use the \"Translation Mode\" button to reposition it.</p> <p></p> <p>Once you are happy with this, right-click on the object and select \"Delete\" to remove it from the world. </p> <p></p>"},{"location":"com2009/assignment1/part4/#ex4","title":"Exercise 4: Approaching an object using a Service and closed-loop control","text":"<p>For this exercise you need to build another Python Server node which must perform the following tasks:</p> <ol> <li>Make the robot move forwards, towards an object placed in front of it. As you know, you'll do this by publishing velocity commands to the <code>/cmd_vel</code> topic.</li> <li>The server node must then stop the robot before it hits the obstacle that you have placed in front of it by subscribing to the <code>/scan</code> topic and monitoring distance information from the LiDAR sensor telling us how far away the object is. </li> <li>The server must do this by considering two inputs received from a Service Request:<ol> <li>The speed (in m/s) at which to approach the object.</li> <li>The distance (in meters) at which the robot must stop in front of it.</li> </ol> </li> <li>A service message called <code>tuos_msgs/Approach</code> is available for you to use for this exercise. Use this to build your service server. Remember, you can find out more about this message using <code>rossrv info</code>.</li> <li> <p>We haven't really done much work with the LiDAR data published to the <code>/scan</code> topic yet, so you might want to consider this suggested approach for building a <code>/scan</code> callback function. </p> <p>Tip</p> <p>You should use a class structure in your Python code here. Start off with the Server code from Exercise 1 and add to this to build the functionality required for this exercise.</p> </li> </ol>"},{"location":"com2009/assignment1/part4/#wrapping-up","title":"Wrapping Up","text":"<p>In Part 4 you have learnt about ROS Services and why they might be useful for robot applications:</p> <ul> <li>Services differ from standard topic-based communication methods in ROS in that they are a direct form of communication between one node and another.  </li> <li>The communication between the two nodes is sequential or synchronous: once a service Caller has called a service, it must wait until it has received a response.</li> <li>This is useful for controlling quick, short-duration tasks or for offloading computations (which could perhaps also be considered decision-making).</li> </ul> <p>Having completed all the exercises above, you should now be able to:</p> <ul> <li>Create and execute Python Service Servers.</li> <li>Create and execute Python Service Callers, as well as call services from the command-line.</li> <li>Implement these principles with a range of different service message types to perform a number of different robot tasks.</li> <li>Use LiDAR data effectively for basic closed-loop robot control.</li> <li>Develop Python nodes which also incorporate principles from Parts 1, 2 &amp; 3 of this course:<ul> <li>Publishing and subscribing to topics.</li> <li>Controlling the velocity and position of a robot.</li> <li>Using the Python Class architecture.</li> <li>Harnessing ROS and Linux command-line tools.</li> </ul> </li> </ul>"},{"location":"com2009/assignment1/part4/#backup","title":"WSL-ROS Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS environment next time you fire one up (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/assignment1/part5/","title":"Part 5: ROS Actions","text":""},{"location":"com2009/assignment1/part5/#introduction","title":"Introduction","text":"<p> Exercises: 4 essential (plus 2 advanced exercises) Estimated Completion Time: 3 hours (for the essential exercises only)</p>"},{"location":"com2009/assignment1/part5/#aims","title":"Aims","text":"<p>In this part of the course you will learn about a third (and final) communication method available within ROS: Actions.  Actions are essentially an advanced version of ROS Services, and you will learn about exactly how these two differ and why you might choose to employ an action over a service for certain robotic tasks. </p>"},{"location":"com2009/assignment1/part5/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Recognise how ROS Actions differ from ROS Services and explain where this method might be useful in robot applications.</li> <li>Explain the structure of Action messages and identify the relevant information within them, enabling you to build Action Servers and Clients.</li> <li>Implement Python Action Client nodes that utilise concurrency and preemption.</li> <li>Develop Action Server &amp; Client nodes that could be used as the basis for a robotic search strategy.</li> </ol>"},{"location":"com2009/assignment1/part5/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Launching an Action Server and calling it from the command-line</li> <li>Exercise 2: Building a Python Action Client Node with concurrency</li> <li>Exercise 3: Building a Preemptive Python Action Client Node</li> <li>Exercise 4: Developing an \"Obstacle Avoidance\" behaviour using an Action Server</li> <li>Advanced (optional) exercises:<ul> <li>Advanced Exercise 1: Implementing a Search strategy</li> <li>Advanced Exercise 2: Autonomous Navigation using waypoint markers</li> </ul> </li> </ul>"},{"location":"com2009/assignment1/part5/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Action Client Code (for Exercise 2)</li> <li>The Preemptive Action Client Code (for Exercise 3)</li> </ul>"},{"location":"com2009/assignment1/part5/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>Launch your ROS environment now so that you have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS Environment will not be preserved between sessions or across different University computers, and so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS in TERMINAL 1) enter <code>Y</code> to restore this<sup>1</sup>.</p> <p>Step 3: Launch VS Code </p> <p>WSL users remember to check for this.</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Check that the Course Repo is up-to-date before you start on these exercises. See here for how to install and/or update.</p>"},{"location":"com2009/assignment1/part5/#calling-an-action-server","title":"Calling an Action Server","text":"<p>Before we talk about what actions actually are, we're going to dive straight in and see one in action (excuse the pun). As you may remember from Part 3, you actually used a ROS Action to make your robot navigate autonomously in Exercise 3, by calling an action server from the command-line. We will do a similar thing now, in a different context, and this time we'll also look at what's going on in a bit more detail.</p>"},{"location":"com2009/assignment1/part5/#ex1","title":"Exercise 1: Launching an Action Server and calling it from the command-line","text":"<p>We'll play a little game here. We're going to launch our TurtleBot3 Waffle in a mystery environment now, and we're going to do this by launching Gazebo headless i.e. Gazebo will be running behind the scenes, but there'll be no Graphical User Interface (GUI) to show us what the environment actually looks like.  Then, we'll use an action server to make our robot scan the environment and take pictures for us, to reveal its surroundings!</p> <ol> <li> <p>To launch our TurtleBot3 Waffle in the mystery environment, use the following <code>roslaunch</code> command:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_simulations mystery_world.launch\n</code></pre></p> <p>Messages in the terminal should indicate that something has happened, but that's about all you will see!</p> </li> <li> <p>Next, open up a new terminal window (i.e. not a new tab in the same window). Windows Terminal users can do this by pressing the \"New Tab\" () button whilst pressing the Shift key (we'll call this WT(B)).</p> </li> <li> <p>In WT(B) have a look at all the topics that are currently active on the ROS network (you should know exactly how to do this by now!)</p> </li> <li> <p>Return to the original Terminal instance (the one with the Gazebo processes running, and which we'll now refer to as WT(A)), open up a new tab (WT(A) TERMINAL 2) and launch an action server that we have already prepared for you for this exercise:</p> <p>WT(A) TERMINAL 2: <pre><code>roslaunch tuos_examples camera_sweep.launch\n</code></pre></p> </li> <li> <p>Now, return to WT(B) and take a look again at all the topics that are active on the ROS network.</p> <p>Questions</p> <ul> <li>What do you notice?</li> <li>Anything new there now compared to when you ran the same command before?</li> </ul> <p>You should in fact notice 5 new items in that list:</p> <pre><code>/camera_sweep_action_server/cancel\n/camera_sweep_action_server/feedback\n/camera_sweep_action_server/goal\n/camera_sweep_action_server/result\n/camera_sweep_action_server/status\n</code></pre> <p>A ROS action therefore has five messages associated with it. We'll talk about these in a bit more detail later on, but for now, all we need to know is that in order to call an action, we need to send the action server a Goal (which you may remember doing in Part 3).</p> Comparing with ROS Services <p>This is a bit like sending a Request to a ROS Service Server, like we did in the previous session.</p> </li> <li> <p>ROS Actions use topic messages (unlike ROS Services, which use dedicated service messages). We can therefore tap into the ROS network and observe the messages being published to these in exactly the same way as we have done in previous parts of this course using <code>rostopic echo</code>. In order to monitor some of these messages now, we'll launch a couple more instances of the Windows Terminal, so that we can view a few things simultaneously:</p> <ol> <li>Once again, launch an additional Terminal instance. Again, Windows Terminal users can press the \"New Tab\" button whilst pressing the Shift key (this one will be called WT(C))</li> <li>Do this again to launch another Terminal instance, which we'll call WT(D)</li> <li>You should now have four Terminal instances open! Arrange these so that they are all visible, e.g.: </li> </ol> <p> </p> </li> <li> <p>In WT(C) run a <code>rostopic echo</code> command to echo the messages being published to the <code>/camera_sweep_action_server/feedback</code> topic:</p> <p>WT(C): <pre><code>rostopic echo /camera_sweep_action_server/feedback\n</code></pre></p> <p>To begin with, you'll see the message:</p> <pre><code>WARNING: no messages received and simulated time is active.\nIs /clock being published?\n</code></pre> <p>Don't worry about this.</p> </li> <li> <p>Do the same in WT(D), but this time to echo the messages being published to the <code>/result</code> part of the action server message.</p> </li> <li> <p>Now, going back to WT(B), run the <code>rostopic pub</code> command on the <code>/camera_sweep_action_server/goal</code> topic, using the autocomplete functionality in the terminal to help you format the message correctly:</p> <p>WT(B): <pre><code>rostopic pub /camera_sweep_action_server/goal[SPACE][TAB][TAB]\n</code></pre></p> <p>Which should provide you with the following:</p> <pre><code>rostopic pub /camera_sweep_action_server/goal tuos_msgs/CameraSweepActionGoal \"header:\n  seq: 0\n  stamp:\n  secs: 0\n  nsecs: 0\n  frame_id: ''\ngoal_id:\n  stamp:\n  secs: 0\n  nsecs: 0\n  id: ''\ngoal:\n  sweep_angle: 0.0\n  image_count: 0\"\n</code></pre> </li> <li> <p>Edit the <code>goal</code> portion of the message by using the left arrow button on your keyboard to scroll back through the message. Modify the <code>sweep_angle</code> and <code>image_count</code> parameters:</p> <ul> <li><code>sweep_angle</code> is the angle (in degrees) that the robot will rotate on the spot</li> <li><code>image_count</code> is the number of images it will capture from its front-facing camera while it is rotating</li> </ul> </li> <li>Once you have decided on some values, hit Enter to actually publish the message and call the action server.     Keep an eye on all four terminal instances. What do you notice happening in each of them?</li> <li> <p>Now, in WT(B):</p> <ol> <li>Cancel the <code>rostopic pub</code> command by entering Ctrl+C</li> <li> <p>Once the action had completed, a message should have been published in WT(D) (a \"result\"), informing you of the filesystem location where the action server has stored the images that have just been captured by the robot:</p> <pre><code>result:\n  image_path: \"~/myrosdata/action_examples/YYYYMMDD_hhmmss\"\n---\n</code></pre> </li> <li> <p>Navigate to this directory in WT(B) (using <code>cd</code>) and have a look at the content using <code>ll</code> (a handy alias for the <code>ls</code> command):</p> <p>You should see the same number of image files in there as you requested with the <code>image_count</code> parameter.</p> </li> <li> <p>Launch <code>eog</code> in this directory and click through all the images to reveal your robot's mystery environment:</p> <p>WT(B): <pre><code>eog .\n</code></pre></p> </li> </ol> </li> <li> <p>Finally, open another tab in the WT(A) terminal instance (WT(A) TERMINAL 3) and launch the Gazebo client to view the simulation that has, until now, been running headless:</p> <p>WT(A) TERMINAL 3: <pre><code>gzclient\n</code></pre></p> </li> <li> <p>The actual simulated environment should now be revealed!! To finish off, close down some active ROS processes and Windows Terminal instances that we've just been working with:</p> <ol> <li>Close down the <code>eog</code> window and WT(B).</li> <li>Stop the <code>rostopic echo</code> commands that are running in WT(C) and WT(D) by entering Ctrl+C in each of them and then close each of these Terminal instances too.</li> <li>Enter Ctrl+C in WT(A) TERMINAL 3 to stop the Gazebo GUI, but keep the terminal tab open. </li> <li>Leave the processes running in WT(A) TERMINAL 2 and 1 for now (the Action Server and the headless Gazebo processes).</li> </ol> </li> </ol>"},{"location":"com2009/assignment1/part5/#summary","title":"Summary","text":"<p>Phew, that was a long one! Essentially, what we did here is launched an action server and then called it from the command-line using <code>rostopic pub</code>. Hopefully, while the action server was performing the task that we had requested, you also noticed that it was providing us with some real-time feedback on how it was getting on (in WT(C)). In the same way as a ROS Service, it should also have provided us with a result (in WT(D)), once the action had been completed.  Feedback is one of the key features that differentiates a ROS Action from a ROS Service, but there are other interesting features too, and we'll explore these in more detail now.</p>"},{"location":"com2009/assignment1/part5/#what-is-a-ros-action","title":"What is a ROS Action?","text":"<p>As you will have observed from the above exercise, a ROS Action actually seems to work a lot like a ROS Service.  We've seen that we have a feedback message associated with an Action though, which is indeed different, but this isn't the main differentiating feature. The key difference is that when a node calls a ROS Action (i.e. an action \"Caller\" or \"Client\"), it doesn't need to wait until the action is complete before it can move on to something else: it can continue to do other tasks at the same time. Unlike ROS Services then, ROS Actions are Asynchronous, which makes them useful when implementing robotic behaviours that take a longer time to execute, and which an Action Client might need to be updated on throughout the process.</p> <p>Recall the five messages associated with the action server from the exercise above, the messages had the following names:</p> <pre><code>/cancel\n/feedback\n/goal\n/result\n/status\n</code></pre> <p>The top item there hints at the most important feature of ROS Actions: they can be cancelled (or \"preempted\"), which we'll learn more about later.  </p> <p>The other thing to note is that - where we used the <code>rosservice</code> command to interrogate the ROS Services that were active on our ROS network previously - Actions use ROS Topics, so we use <code>rostopic</code> commands to interrogate action servers:</p> <ol> <li><code>rostopic list</code>: to identify the action servers that are available on the network.</li> <li><code>rostopic echo</code>: to view the messages being published by a given action server.</li> <li><code>rostopic pub</code>: to call an action from the command-line. </li> </ol>"},{"location":"com2009/assignment1/part5/#the-format-of-action-messages","title":"The Format of Action Messages","text":"<p>Like Services, Action Messages have multiple parts to them, and we need to know what format these action messages take in order to be able to call them. We don't have a tool like <code>rossrv</code> to do this for Actions though, instead we have to use <code>rosmsg</code>, or look for the message definition inside the Action Message Package.</p> <p>We ran <code>rostopic list</code> to identify our action server in the previous exercise, which told us that there was an action server running called <code>/camera_sweep_action_server</code>:</p> <pre><code>rostopic list\n[some topics...]\n/camera_sweep_action_server/cancel\n/camera_sweep_action_server/feedback\n/camera_sweep_action_server/goal\n/camera_sweep_action_server/result\n/camera_sweep_action_server/status\n[some more topics...]\n</code></pre>"},{"location":"com2009/assignment1/part5/#cancel-and-status","title":"\"Cancel\" and \"Status\"","text":"<p>Every ROS Action has both a cancel and status message associated with them. These are standardised, so the format of these two messages will always be the same, regardless of the type of Action Server we use. We won't worry about these too much for now, but we'll make use of them in some ROS Nodes that we'll build in a short while.</p> <p>The feedback, goal and result messages will be different for any given action server though, and so we need to know about the format of all of these before we attempt to make a call to the action server.</p> <p>We can run <code>rostopic info</code> on any of these to find out more about them...</p>"},{"location":"com2009/assignment1/part5/#goal","title":"\"Goal\"","text":"<p>Let's look at the goal to start with:</p> <p>TERMINAL 1: <pre><code>rostopic info /camera_sweep_action_server/goal\n</code></pre> From which we obtain the usual information:</p> <pre><code>Type: tuos_msgs/CameraSweepActionGoal\n\nPublishers: None\n\nSubscribers:\n  * /camera_sweep_action_server (http://localhost:#####/)\n</code></pre> <p>The <code>Type</code> field tells us that the action message belongs to the <code>tuos_msgs</code> package, and we can find out more about the <code>goal</code> message by using <code>rosmsg info</code>. You'll be familiar with how this works by now:</p> <pre><code>rosmsg info {messageType}\n</code></pre> <p>Where <code>{messageType}</code> is established from the output of the <code>rostopic info</code> command above: </p> <pre><code>Type: tuos_msgs/CameraSweepActionGoal\n</code></pre> <p>When working with ROS Actions and the <code>rosmsg</code> command though, we can actually drop the word \"<code>Action</code>\" in the message Type, so our <code>rosmsg</code> command becomes:</p> <p>TERMINAL 1: <pre><code>rosmsg info tuos_msgs/CameraSweepGoal\n</code></pre> Which will output:</p> <pre><code>float32 sweep_angle\nint32 image_count\n</code></pre> Further Info <p><code>rosmsg info tuos_msgs/CameraSweepActionGoal</code> will work as well, but we get a lot of other information in the output that we're not all that interested in. Give it a go and see the difference, if you want to!</p> <p>In order to call this action server, we need to send a goal, and <code>rosmsg info</code> has just told us that there are two goal parameters that we must provide:</p> <ol> <li><code>sweep_angle</code>: a 32-bit floating-point value</li> <li><code>image_count</code>: a 32-bit integer</li> </ol> <p>So we know more about our Action Server's Goal now, but there are two other parameters we still know nothing about: Result and Feedback. It's important to know about all three things in order to be able to work with the Action Server effectively, and we can use an alternative approach to interrogate all three at the same time...</p>"},{"location":"com2009/assignment1/part5/#goal-feedback-and-result","title":"\"Goal,\" \"Feedback\" and \"Result\"","text":"<p>We know, from above, that the <code>/camera_sweep_action_server</code> messages are part of the <code>tuos_msgs</code> package, so we can navigate to the package directory (using <code>roscd</code>) and look at the actual message definition. </p> <p>TERMINAL 1: <pre><code>roscd tuos_msgs/\n</code></pre></p> <p>Actions are always contained within an <code>action</code> folder inside the package directory, so we can then navigate into this folder using <code>cd</code>:</p> <p>TERMINAL 1: <pre><code>cd action/\n</code></pre></p> <p>Use the <code>ll</code> command again here to view all the action messages within the package. Here you should see the <code>CameraSweep.action</code> message listed. Run <code>cat</code> on this file to view the full message definition:</p> <p>TERMINAL 1: <pre><code>cat CameraSweep.action\n</code></pre> <pre><code>#goal\nfloat32 sweep_angle    # the angular sweep over which to capture images (degrees)\nint32 image_count      # the number of images to capture during the sweep\n---\n#result\nstring image_path      # The filesystem location of the captured images\n---\n#feedback\nint32 current_image    # the number of images taken\nfloat32 current_angle  # the current angular position of the robot (degrees)\n</code></pre></p> <p>Questions</p> <ul> <li>What are the names of the result and feedback message parameters? (There are three parameters in total.)</li> <li>What datatypes do these parameters use?</li> </ul> <p>You'll learn how we use this information to develop Python Action Server &amp; Client nodes in the following exercises.</p>"},{"location":"com2009/assignment1/part5/#concurrent-activity","title":"Concurrent Activity","text":"<p>An Action Server provides feedback messages at regular intervals whilst performing an action and working towards its goal.  This is one way that an Action Client can monitor the progress of the action that it has requested.  Another way it can do this is by monitoring the status of an action.  Both of these features enable concurrency, allowing an action client to work on other things whilst waiting for the requested behaviour to be completed by the action server.</p> <p></p>"},{"location":"com2009/assignment1/part5/#ex2","title":"Exercise 2: Building a Python Action Client Node with Concurrency","text":"<ol> <li> <p>You should only have one Terminal application instance open now, with three terminal tabs in it. TERMINAL 3 should already be idle (i.e. not running any commands), and (if you haven't done so already) enter Ctrl+C in TERMINAL 1 and TERMINAL 2 to stop the headless Gazebo simulation processes and the Camera Sweep Action Server respectively. </p> </li> <li> <p>In TERMINAL 1 create a new package called <code>part5_actions</code> using the <code>catkin_create_pkg</code> tool as you have done previously. This time, define <code>rospy</code>, <code>actionlib</code> and <code>tuos_msgs</code> as dependencies.</p> <p>Remember</p> <p>Make sure you're in your <code>~/catkin_ws/src/</code> folder when you run the <code>catkin_create_pkg</code> command!</p> </li> <li> <p>Once again, run <code>catkin build</code> on this and then re-source your environment:</p> <p>TERMINAL 1: First: <pre><code>catkin build part5_actions\n</code></pre> Then: <pre><code>source ~/.bashrc\n</code></pre></p> </li> <li> <p>Navigate to the <code>src</code> folder of this package, create a file called <code>action_client.py</code> (using <code>touch</code>) and set this to be executable (using <code>chmod</code>).        </p> </li> <li> <p>Review the code provided here, and the annotations, then copy and paste the code into your newly created <code>action_client.py</code> file. </p> </li> <li> <p>Then, in TERMINAL 2, execute the same launch file as before but this time with a couple of additional arguments:</p> <p>TERMINAL 2: <pre><code>roslaunch tuos_simulations mystery_world.launch gui:=true camera_search:=true\n</code></pre></p> <p>... which will launch the Gazebo simulation in GUI mode this time, as well as the <code>/camera_sweep_action_server</code> too.</p> </li> <li> <p>In TERMINAL 1, use <code>rosrun</code> to call the action server with the <code>action_client.py</code> node that you have just created...</p> <p>Something not right?</p> <p>You may need to change the values that have been assigned to the goal parameters, in order for the client to successfully make a call to the server!</p> <p>The node we have just created, in its current form, uses a feedback callback function to perform some operations while the action server is working. In this case, it simply prints the feedback data that is coming from the Action Server.  That's it though, and the <code>client.wait_for_result()</code> line still essentially just makes the client node wait until the action server has finished doing its job before it can do anything else. This still therefore looks a lot like a service, so let's modify this now to really build concurrency into the client node.</p> </li> <li> <p>First, create a copy of your <code>action_client.py</code> node and call it <code>concurrent_action_client.py</code> (you will need to make sure you are still in the <code>src</code> directory of your <code>part5_actions</code> package before you run this command):</p> <p>TERMINAL 1: <pre><code>cp action_client.py concurrent_action_client.py\n</code></pre></p> </li> <li> <p>We want to use the status message from the action server now, and we can find out a bit more about this as follows:</p> <ol> <li>Use <code>rostopic info camera_sweep_action_server/status</code> to find the message type.</li> <li>Then, use <code>rosmsg info</code> (using the message type you have just identified) to tell you all the status codes that could be returned by the action server.</li> </ol> <p>You should have identified the following states, listed in the <code>status_list</code> portion of the message:</p> <pre><code>PENDING=0\nACTIVE=1\nPREEMPTED=2\nSUCCEEDED=3\nABORTED=4\nREJECTED=5\n...\n</code></pre> <p>We can set up our action client to monitor these status codes in a <code>while</code> loop, and then perform other operations inside this loop until the action has completed (or has been stopped for another reason).</p> </li> <li> <p>To do this, replace the <code>client.wait_for_result()</code> line in the <code>concurrent_action_client.py</code> file with the following code:</p> <pre><code>rate = rospy.Rate(1)\ni = 1\nprint(\"While we're waiting, let's do our seven-times tables...\")\nwhile client.get_state() &lt; 2:\n    print(f\"STATE: Current state code is {client.get_state()}\")\n    print(f\"TIMES TABLES: {i} times 7 is {i*7}\")\n    i += 1\n    rate.sleep()\n</code></pre> </li> <li> <p>Run the <code>concurrent_action_client.py</code> node and see what happens this time.  Essentially, we know that we can carry on doing other things as long as the status code is less than 2 (either <code>PENDING</code> or <code>ACTIVE</code>), otherwise either our goal has been achieved, or something else has happened...</p> </li> </ol>"},{"location":"com2009/assignment1/part5/#preemptive_client","title":"Cancelling (or Preempting) an Action","text":"<p>Actions are extremely useful for controlling robotic tasks or processes that might take a while to complete, but what if something goes wrong, or if we just change our mind and want to stop an action before the goal has been reached? The ability to preempt an action is one of the things that makes them so useful.</p>"},{"location":"com2009/assignment1/part5/#ex3","title":"Exercise 3: Building a Preemptive Python Action Client Node","text":"<ol> <li>In TERMINAL 1 you should still be located within the <code>src</code> folder of your <code>part5_actions</code> package. If not, then go back there now! Create a new file called <code>preemptive_action_client.py</code> and make this executable.</li> <li> <p>Have a look at the code here, then copy and paste it into the <code>preemptive_action_client.py</code> node that you have just created.</p> <p>Here, we've built an action client that will cancel the call to the action server if we enter Ctrl+C into the terminal.  This is useful, because otherwise the action server would continue to run, even when we terminate the client.  A lot of the code is similar to the Action Client from the previous exercise, but we've built a class structure around this now for more flexibility.  Have a look at the code annotations and make sure that you understand how it all works.</p> </li> <li> <p>Run this using <code>rosrun</code>, let the server take a couple of images and then enter Ctrl+C to observe the goal cancelling in action.</p> <p>Warning</p> <p>You'll need to set some values for the goal parameters again!</p> </li> <li> <p>We can also cancel a goal conditionally, which may also be useful if, say, too much time has elapsed since the call was made, or the caller has been made aware of something else that has happened in the meantime (perhaps we're running out of storage space on the robot and can't save any more images!) This is all achieved using the <code>cancel_goal()</code> method.</p> <ul> <li>Have a go now at introducing a conditional call to the <code>cancel_goal()</code> method once a total of 5 images have been captured.</li> <li>You could use the <code>captured_images</code> attribute from the <code>CameraSweepFeedback</code> message to trigger this.</li> </ul> </li> </ol>"},{"location":"com2009/assignment1/part5/#a-summary-of-ros-actions","title":"A Summary of ROS Actions","text":"<p>ROS Actions work a lot like ROS Services, but they have the following key differences:</p> <ol> <li>They are asynchronous: a client can do other things while it waits for an action to complete.</li> <li>They can be cancelled (or preempted): If something is taking too long, or if something else has happened, then an Action Client can cancel an Action whenever it needs to.</li> <li>They provide feedback: so that a client can monitor what is happening and act accordingly (i.e. preempt an action, if necessary).</li> </ol> <p></p> <p>This mechanism is therefore useful for robotic operations that may take a long time to execute, or where intervention might be necessary.</p>"},{"location":"com2009/assignment1/part5/#cam_swp_act_srv","title":"Creating Action Servers in Python","text":"<p>Important</p> <p>Cancel all active processes that you may have running before moving on.</p> <p>So far we have looked at how to call an action server, but what about if we actually want to set up our own? We've been working with a pre-made action server in the previous exercises, but so far we haven't really considered how it actually works. First, let's do some detective work... We launched the Action Server using <code>roslaunch</code> in Exercise 1:</p> <pre><code>roslaunch tuos_examples camera_sweep.launch\n</code></pre> <p>Questions</p> <ul> <li>What does this tell us about the package that the action server node belongs to?</li> <li>Where, in the package directory, is this node likely to be located?</li> <li>How might we find out the name of the Python node from the <code>camera_sweep.launch</code> file?</li> </ul> <p>Once you've identified the name and the location of the source code, open it up in VS Code and have a look through it to see how it all works.</p> <p>Don't worry too much about all the content associated with obtaining and manipulating camera images in there, we'll learn more about this in the next session. Instead, focus on the general overall structure of the code and the way that the action server is implemented.</p> <ol> <li> <p>As a starting point, consider the way in which the action server is initialised and the way a callback function is defined to encapsulate all the code that will be executed when the action is called:</p> <pre><code>self.actionserver = actionlib.SimpleActionServer(self.server_name, \n    CameraSweepAction, self.action_server_launcher, auto_start=False)\nself.actionserver.start()\n</code></pre> </li> <li> <p>Look at how a <code>/cmd_vel</code> publisher and an <code>/odom</code> subscriber are defined in external classes:</p> <pre><code>self.robot_controller = Tb3Move()\nself.robot_odom = Tb3Odometry()\n</code></pre> <p>These are imported (at the start of the code) from an external <code>tb3.py</code> module that also lives in the same directory as the action server itself:</p> <pre><code>from tb3 import Tb3Move, Tb3Odometry\n</code></pre> <p>We do this to simplify the process of obtaining odometry data and controlling the robot, whilst keeping the actual action server code itself more concise. Have a look at the <code>tb3.py</code> module to discover exactly how these Python classes work.</p> </li> <li> <p>Look inside the action server callback function to see how the camera sweep operation is performed once the action has been called:</p> <pre><code>def action_server_launcher(self, goal):\n    ...\n</code></pre> <ol> <li> <p>Consider the error checking that is performed on the <code>goal</code> input variables, and how the call to the action server is aborted should any of these goal requests be invalid:</p> <pre><code>success = True\nif goal.sweep_angle &lt;= 0 or goal.sweep_angle &gt; 180:\n    print(\"Invalid sweep_angle! Select a value between 1 and 180 degrees.\")\n    success = False\n    ...\n\nif not success:\n    self.result.image_path = \"None [ABORTED]\"\n    self.actionserver.set_aborted(self.result)\n    return\n</code></pre> </li> <li> <p>Consider how preemption is implemented in the server, and how the Action is stopped on receipt of a preempt request:</p> <pre><code>if self.actionserver.is_preempt_requested():\n    ...\n</code></pre> </li> <li> <p>Also have a look at the way a <code>feedback</code> message is constructed and published by the server:</p> <pre><code>self.feedback.current_image = i\nself.feedback.current_angle = abs(self.robot_odom.yaw)\nself.actionserver.publish_feedback(self.feedback)\n</code></pre> </li> <li> <p>Finally, consider how we tell the server that the action has been completed successfully, how the <code>result</code> message is published to the caller, and how we make the robot stop moving:</p> <pre><code>if success:\n    rospy.loginfo(\"Camera sweep completed successfully.\")\n    self.actionserver.set_succeeded(self.result)\n    self.robot_controller.stop()\n</code></pre> </li> </ol> </li> </ol>"},{"location":"com2009/assignment1/part5/#ex4","title":"Exercise 4: Developing an \"Obstacle Avoidance\" behaviour using an Action Server","text":"<p>Knowing what you now do about ROS Actions, do you think the Service Server/Client systems that we developed in Part 4 were actually appropriate use cases for ROS Services?  Probably not!  In fact, Action Server/Client methods would have probably been more appropriate! </p> <p>You are now going to construct your own Action Server and Client nodes to implement a more effective obstacle avoidance behaviour that could form the basis of an effective search strategy. For this, you're going to need to build your own Search Server and Client.</p> <p>Step 1: Launch a simulation</p> <p>There's a simulation environment that you can use as you're developing your action server/client nodes for this exercise. Launch the simulation in TERMINAL 1, with the following <code>roslaunch</code> command: </p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_stage_4.launch\n</code></pre></p> <p>Step 2: Build the Action Server</p> <ol> <li> <p>In TERMINAL 2 navigate to the <code>src</code> folder of your <code>part5_actions</code> package, create a Python script called <code>search_server.py</code>, and make it executable.</p> </li> <li> <p>The job of the Action Server node is as follows:</p> <ul> <li>The action server should make the robot move forwards until it detects an obstacle up ahead.</li> <li>Similarly to the Service Server that you created last part, your Action Server here should be configured to accept two goal parameters:<ol> <li>The speed (in m/s) at which the robot should move forwards when the action server is called. Consider doing some error checking on this to make sure a velocity request is less than the maximum speed that the robot can actually achieve (0.26 m/s)!</li> <li>The distance (in meters) at which the robot should stop ahead of any objects or boundary walls that are in front of it. To do this you'll need to subscribe to the <code>/scan</code> topic. Be aware that an object won't necessarily be directly in front of the robot, so you may need to monitor a range of <code>LaserScan</code> data points (within the <code>ranges</code> array) to make the collision avoidance effective (recall the LaserScan callback example and also have a look at the <code>Tb3LaserScan</code> class within the <code>tuos_examples/tb3.py</code> module that might help you with this).</li> </ol> </li> <li> <p>Whilst your server performs its task it should provide the following feedback to the Action Caller:</p> <ol> <li> <p>The distance travelled (in meters) since the current action was initiated.</p> <p>To do this you'll need to subscribe to the <code>/odom</code> topic. Remember that there's a <code>Tb3Odometry</code> class within the <code>tuos_examples/tb3.py</code> module that might help you with obtaining this data.</p> <p>Remember also that your robot's orientation shouldn't change over the course of a single action call, only its <code>linear.x</code> and <code>linear.y</code> positions should vary.  Bear in mind however that the robot won't necessarily be moving along the <code>X</code> or <code>Y</code> axis, so you will need to consider the total distance travelled in the <code>X-Y</code> plane.  You should have done this in the Part 2 <code>move_square</code> exercise, so refer to this if you need a reminder.</p> </li> </ol> </li> <li> <p>Finally, on completion of the action, your server should provide the following three result parameters:</p> <ol> <li>The total distance travelled (in meters) over the course of the action.</li> <li>The distance to the obstacle that made the robot stop (this should match, or very close to, the distance that was provided by the Action Client in the goal).</li> <li>The angle (in degrees) at which this obstacle is located in front of the robot (<code>Tb3LaserScan</code> class within the <code>tuos_examples/tb3.py</code> module, which may already provide this).</li> </ol> </li> </ul> </li> <li> <p>An action message has been created for you to use for this exercise: <code>tuos_msgs/Search.action</code>.  Navigate to the <code>action</code> folder of the <code>tuos_msgs</code> package directory (or use <code>rosmsg info ...</code> in the terminal) to find out everything you need to know about this action message in order to develop your Action Server (and Client) nodes appropriately.</p> </li> <li> <p>We've put together some template code to help you with this. For further guidance though, you should also refer to the code for <code>/camera_sweep_action_server</code> node, which we talked about earlier: a lot of the techniques used by <code>/camera_sweep_action_server</code> node will be similar to what you'll need to do in this exercise. </p> </li> <li> <p>Whenever you're ready you can launch your action server from TERMINAL 2, using <code>rosrun</code>, as below:</p> <p>TERMINAL 2: <pre><code>rosrun part5_actions search_server.py\n</code></pre></p> </li> </ol> <p>Step 3: Build the Action Client</p> <ol> <li> <p>In TERMINAL 3 navigate to the <code>src</code> folder of your <code>part5_actions</code> package, create a Python script called <code>search_client.py</code>, and make it executable.</p> </li> <li> <p>The job of the Action Client node is as follows:</p> <ul> <li>The client needs to issue a correctly formatted goal to the server.</li> <li>The client should be programmed to monitor the feedback data from the Server.  If it detects (from the feedback) that the robot has travelled a distance greater than 2 meters without detecting an obstacle, then it should cancel the current action call using the <code>cancel_goal()</code> <code>actionlib</code> method.</li> </ul> </li> <li> <p>Use the techniques that we used in the Client node from Exercise 3 as a guide to help you with this. There's also a code template here to help you get started. </p> </li> <li> <p>Once you have everything in place launch the action client with <code>rosrun</code> as below:</p> <p>TERMINAL 3: <pre><code>rosrun part5_actions search_client.py\n</code></pre></p> <p>If all is good, then this client node should call the action server, which will - in turn - make the robot move forwards until it reaches a certain distance from an obstacle up ahead, at which point the robot will stop, and your client node will stop too. Once this happens, reorient your robot (using the <code>turtlebot3_teleop</code> node) and launch the client node again to make sure that it is robustly stopping in front of obstacles repeatedly, and when approaching them from a range of different angles. </p> <p>Important</p> <p>Make sure that your preemption functionality works correctly too, so that the robot never moves any further than 2 meters during a given action call!</p> </li> </ol>"},{"location":"com2009/assignment1/part5/#advanced","title":"Some advanced exercises (if you're feeling adventurous!)","text":"<p>Want to do more with the ROS skills that you have now developed?! Consider the following advanced exercises that you could try out now that you know how to use ROS Actions!</p> <p>Note</p> <p>We've covered a lot already in this session, and the next exercises are really just suggestions for more advanced things that you may want to explore to push your knowledge further (it may also help with the further work that you will do in Assignment #2...)</p>"},{"location":"com2009/assignment1/part5/#adv_ex1","title":"Advanced Exercise 1: Implementing a Search strategy","text":"<p>What you developed in the previous exercise could be used as the basis for an effective robot search strategy.  Up to now, your Action Client node should have the capability to call your <code>Search.action</code> server to make the robot move forwards by 2 meters, or until it reaches an obstacle (whichever occurs first), but you could enhance this further:</p> <ul> <li>Between action calls, your client node could make the robot turn on the spot to face a different direction and then issue a further action call to make the robot move forwards once again.</li> <li>The turning process could be done at random, or it could be informed by the result of the last action call, i.e.: if (on completion) the server has informed the client that it detected an object at an angle of, say, 10\u00b0 anti-clockwise from the front of the robot, then the client might then decide to turn the robot clockwise in an attempt to turn away from the object before issuing its next action call to make the robot move forwards again.</li> <li> <p>By programming your client node to repeat this process over and over again, the robot would (somewhat randomly) travel around its environment safely, stopping before it crashes into any obstacles and reorienting itself every time it stops moving forwards. This is effectively an implementation of a basic robotic search strategy! </p> <p>Enhancing this further...</p> <p>Imagine SLAM was running at the same time too... your robot could be building up a map of its environment in the background as it slowly explored every part of it!</p> </li> </ul> <p>Assignment #2 Checkpoint</p> <p>Having completed Assignment #1 up to this point, you should have everything you need to tackle Assignment #2 Task 2.</p>"},{"location":"com2009/assignment1/part5/#adv_ex2","title":"Advanced Exercise 2: Autonomous Navigation using waypoint markers","text":"<p>In Part 3 you used SLAM to construct a map of an environment (Exercise 2) and then issued navigation requests to the <code>move_base</code> action server, via the command-line, (Exercise 3) to make your robot move to a zone marker, based on coordinates that you had established beforehand. Now that you know how to build Action Client Nodes in Python you could return to your <code>part2_navigation</code> package and build a new node that makes the robot move sequentially between each zone marker programmatically.</p> <ul> <li>Your node could cycle through the coordinates of all four of the zone markers (or \"waypoints\") that you established whilst using SLAM to build a map of the environment (as per Exercise 2).</li> <li>Your node could monitor the status of the <code>move_base_simple</code> action call to know when the robot has reached a zone marker, so that it knows when to issue a further action call to move on to the next one.</li> <li>You could refer to the launch file that you created in Part 3 to launch all the navigation processes that need to be running in order to enable and configure the ROS Navigation Stack appropriately for the TurtleBot3 robot.</li> </ul>"},{"location":"com2009/assignment1/part5/#wrapping-up","title":"Wrapping Up","text":"<p>In Part 5 of this course you've learnt:</p> <ul> <li>How ROS Actions work and why they might be useful.</li> <li>How to develop Action Client Nodes in Python which can perform other tasks concurrently to the action they have requested, and which can also cancel the requested action, if required.</li> <li>How to use standard ROS tools to interrogate the topic messages used by an action server, allowing you to build clients to call them, and to also allow you to build standalone action servers yourself using bespoke Action messages.</li> <li>How to harness this communication method to implement a behaviour that could be used as the basis for a genuine robotic search strategy. </li> </ul>"},{"location":"com2009/assignment1/part5/#topics-services-or-actions-which-to-choose","title":"Topics, Services or Actions: Which to Choose?","text":"<p>You should now have developed a good understanding of the three communication methods that are available within ROS to facilitate communication between ROS Nodes:</p> <ol> <li>Topic-based messaging.</li> <li>ROS Services.</li> <li>ROS Actions.</li> </ol> <p>Through this course you've gained some practical experience using all three of these, but you may still be wondering how to select the appropriate one for a certain robot task... </p> <p>This ROS.org webpage summarises all of this very nicely (and briefly), so you should have a read through this to make sure you know what's what. In summary though:</p> <ul> <li>Topics: Are most appropriate for broadcasting continuous data-streams such as sensor data and robot state information, and for publishing data that is likely to be required by a range of Nodes across a ROS network.</li> <li>Services: Are most appropriate for very short procedures like quick calculations (inverse kinematics etc.) and performing short discrete actions that are unlikely to go wrong or will not need intervention (e.g. turning on a warning LED when a battery is low).</li> <li>Actions: Are most appropriate for longer running tasks (like moving a robot), for longer processing calculations (processing the data from a camera stream) or for operations where we might need to change our mind and do something different or cancel an invoked behaviour part way through.</li> </ul>"},{"location":"com2009/assignment1/part5/#backup","title":"WSL-ROS Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS environment next time you fire one up (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/assignment1/part6/","title":"Part 6: Cameras, Machine Vision & OpenCV","text":""},{"location":"com2009/assignment1/part6/#introduction","title":"Introduction","text":"<p> Exercises: 4 Estimated Completion Time: 2 hours</p>"},{"location":"com2009/assignment1/part6/#aims","title":"Aims","text":"<p>In this final part of the course you will finally make use of the TurtleBot3's camera, and look at how to work with images in ROS! Here we'll look at how to build ROS nodes that capture images and process them. We'll explore some ways in which this data can be used to inform decision-making in robotic applications.  </p>"},{"location":"com2009/assignment1/part6/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Use a range of ROS tools to interrogate camera image topics on a ROS Network and view the images being streamed to them.</li> <li>Use the computer vision library OpenCV with ROS, to obtain camera images and process them in real-time.  </li> <li>Apply filtering processes to isolate objects of interest within an image.</li> <li>Develop object detection nodes and harness the information generated by these processes to control a robot's position.</li> <li>Use camera data as a feedback signal to implement a line following behaviour using proportional control.</li> </ol>"},{"location":"com2009/assignment1/part6/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Using the <code>rqt_image_view</code> node whilst changing the robot's viewpoint</li> <li>Exercise 2: Object Detection</li> <li>Exercise 3: Locating image features using Image Moments</li> <li>Exercise 4: Line following</li> </ul>"},{"location":"com2009/assignment1/part6/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Initial Object Detection Code (for Exercise 2)</li> <li>A Complete Worked Example of the <code>object_detection.py</code> Node</li> <li>A <code>line_follower</code> Template (for Exercise 4)</li> </ul>"},{"location":"com2009/assignment1/part6/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch your ROS Environment</p> <p>Launch your ROS environment now so that you have access to a Linux terminal instance (aka TERMINAL 1).</p> <p>Step 2: Restore your work (WSL-ROS Managed Desktop Users ONLY)</p> <p>Remember that any work that you do within the WSL-ROS Environment will not be preserved between sessions or across different University computers, and so you should be backing up your work to your <code>U:\\</code> drive regularly. When prompted (on first launch of WSL-ROS in TERMINAL 1) enter <code>Y</code> to restore this<sup>1</sup>.</p> <p>Step 3: Launch VS Code </p> <p>WSL users remember to check for this.</p> <p>Step 4: Make Sure The Course Repo is Up-To-Date</p> <p>Check that the Course Repo is up-to-date before you start on these exercises. See here for how to install and/or update.</p> <p>Step 5: Launch the Robot Simulation</p> <p>In this session we'll start by working with the same mystery world environment from Part 5. In TERMINAL 1, use the following <code>roslaunch</code> command to load it:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_simulations coloured_pillars.launch\n</code></pre> ...and then wait for the Gazebo window to open:</p> <p></p>"},{"location":"com2009/assignment1/part6/#working-with-cameras-and-images-in-ros","title":"Working with Cameras and Images in ROS","text":""},{"location":"com2009/assignment1/part6/#camera-topics-and-data","title":"Camera Topics and Data","text":"<p>There are a number of tools that we can use to view the live images that are being captured by a robot's camera in ROS. As with all robot data, these streams are published to topics, so we firstly need to identify those topics.</p> <p>In a new terminal instance (TERMINAL 2), run <code>rostopic list</code> to see the full list of topics that are currently active on our system. Conveniently, all the topics related to our robot's camera are prefixed with <code>camera</code>! Filter the <code>rostopic list</code> output using <code>grep</code> (a Linux command), to filter the list and only show topics prefixed with <code>/camera</code>:</p> <p>TERMINAL 2: <pre><code>rostopic list | grep /camera\n</code></pre></p> <p>This should provide the following filtered list:</p> <pre><code>/camera/depth/camera_info\n/camera/depth/image_raw\n/camera/depth/points\n/camera/parameter_descriptions\n/camera/parameter_updates\n/camera/rgb/camera_info\n/camera/rgb/image_raw\n/camera/rgb/image_raw/compressed\n/camera/rgb/image_raw/compressed/parameter_descriptions\n/camera/rgb/image_raw/compressed/parameter_updates\n/camera/rgb/image_raw/compressedDepth\n/camera/rgb/image_raw/compressedDepth/parameter_descriptions\n/camera/rgb/image_raw/compressedDepth/parameter_updates\n/camera/rgb/image_raw/theora\n/camera/rgb/image_raw/theora/parameter_descriptions\n/camera/rgb/image_raw/theora/parameter_updates\n</code></pre> <p>Our real TurtleBot3 Waffles in the Diamond have a slightly different camera module to that used by the simulated robots that we are working with here.  Despite this though, the camera data on our real robots is published to topics using the same ROS message formats as used in simulation, making it fairly straight-forward to transfer nodes that we develop in simulation here onto the real robots<sup>2</sup>.</p> <p>The first items in the list of camera topics above tell us that depth information is available here. Much like the real robots, the simulated versions that we are working with here also have a camera module capable of determining depth information as well as simply capturing images.  Remember from Part 3 though, that we also have a very capable LiDAR sensor to give us this type of information too, and so we won't really be using the depth capabilities of our camera in this session.</p> <p>The main thing we are actually interested in here is the RGB images that are captured by the camera, and the key topic that we'll therefore be using here is:</p> <pre><code>/camera/rgb/image_raw\n</code></pre> <p>Run <code>rostopic info</code> on this to identify the message type.</p> <p>Now, run <code>rosmsg info</code> on this message type to find out exactly what information is published to the topic.  You should end up with a list that looks like this:</p> <pre><code>std_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\nuint32 height\nuint32 width\nstring encoding\nuint8 is_bigendian\nuint32 step\nuint8[] data\n</code></pre> <p></p> <p>Questions</p> <ol> <li>What type of message is used on this topic, and which package is this message derived from?</li> <li>Using <code>rostopic echo</code> and the information about the topic message (as shown above) determine the size of the images that our robot's camera will capture (i.e. its dimensions, in pixels).  It will be quite important to know this when we start manipulating these camera images later on. </li> <li>Finally, considering the list above again, which part of the message do you think contains the actual image data?</li> </ol>"},{"location":"com2009/assignment1/part6/#viz","title":"Visualising Camera Streams","text":"<p>We can view the images being streamed to the above camera topic (in real-time) in a variety of different ways, and we'll explore a couple of these now.</p> <p>One way is to use RViz, which can be launched using the following <code>roslaunch</code> command:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch\n</code></pre></p> <p>Once RViz launches, find the camera item in the left-hand panel and tick the checkbox next to it. This should open up a camera panel with a live stream of the images being obtained from the robot's camera!  The nice thing about this is that the real-time LiDAR data will also be overlaid on top of the images too!</p> <p></p> <p>Close down RViz by entering Ctrl+C in TERMINAL 2.  </p>"},{"location":"com2009/assignment1/part6/#ex1","title":"Exercise 1: Using the RQT Image View node whilst changing the robot's viewpoint","text":"<p>Another tool we can use to view camera data-streams is the <code>rqt_image_view</code> node.</p> <ol> <li> <p>To launch this, use <code>rosrun</code> as follows:</p> <p>TERMINAL 2: <pre><code>rosrun rqt_image_view rqt_image_view\n</code></pre> </p> <p>This is a nice tool that allows us to easily view images that are being published to any camera topic on the ROS network. Another useful feature is the ability to save these images (as <code>.jpg</code> files) to the filesystem: See the \"Save as image\" button highlighted in the figure above. This might be useful later on...</p> </li> <li> <p>Click the drop-down box in the top left of the window to select an image topic to display.  Select <code>/camera/rgb/image_raw</code> (if it's not already selected).</p> </li> <li> <p>Keep this window open now, and launch a new terminal instance (TERMINAL 3).</p> </li> <li> <p>Launch the <code>turtlebot3_teleop</code> node, either using the full command or a handy alias: <code>tb3_teleop</code>! Rotate your robot on the spot, keeping an eye on the <code>rqt_image_view</code> window as you do this.  Stop the robot once one of the coloured pillars in the arena is roughly in the centre of the robot's field of vision, then close the <code>turtlebot3_teleop</code> node and the <code>rqt_image_view</code> node by entering Ctrl+C in TERMINAL 3 and TERMINAL 2 respectively.</p> </li> </ol>"},{"location":"com2009/assignment1/part6/#opencv","title":"OpenCV and ROS","text":"<p>OpenCV is a mature and powerful computer vision library designed for performing real-time image analysis, and it is therefore extremely useful for robotic applications.  The library is cross-platform and there is a Python API (<code>cv2</code>), which we'll be using to do some computer vision tasks of our own during this lab session. While we can work with OpenCV using Python straight away (via the API), the library can't directly interpret the native image format used by the ROS, so there is an interface that we need to use.  The interface is called CvBridge, which is a ROS package that handles the conversion between ROS and OpenCV image formats.  We'll therefore need to use these two libraries (OpenCV and CvBridge) hand-in-hand when developing ROS nodes to perform computer vision related tasks.</p>"},{"location":"com2009/assignment1/part6/#object-detection","title":"Object Detection","text":"<p>One common job that we often want a robot to perform is object detection, and we will illustrate how this can be achieved using OpenCV tools for colour filtering, to detect the coloured pillar that your robot should now be looking at.  </p>"},{"location":"com2009/assignment1/part6/#ex2","title":"Exercise 2: Object Detection","text":"<p>In this exercise you will learn how to use OpenCV to capture images, filter them and perform other analysis to confirm the presence and location of features that we might be interested in.</p> <p>Step 1</p> <ol> <li>First create a new package in your <code>catkin_ws/src</code> directory called <code>part6_vision</code> with <code>rospy</code>, <code>cv_bridge</code>, <code>sensor_msgs</code> and <code>geometry_msgs</code> as dependencies.</li> <li>Then, run <code>catkin build</code> on the package and then re-source your environment (as you've done so many times by now!)</li> <li>In the <code>src</code> folder of the package you have just created, create a new Python file called <code>object_detection.py</code>. What else do we need to do to this file before we can run it? Do it now!</li> <li>Copy the code here, save the file, then read the annotations so that you understand how this node works and what should happen when you run it. </li> <li> <p>Run the node using <code>rosrun</code>.</p> <p>Warning</p> <p>This node will capture an image and display it in a pop-up window. Once you've viewed the image in this pop-up window MAKE SURE YOU CLOSE THE POP-UP WINDOW DOWN so that the node can complete its execution!</p> </li> <li> <p>As you should know from reading the explainer, the node has just obtained an image and saved it to a location on the filesystem.  Navigate to this filesystem location and view the image using <code>eog</code>.</p> </li> </ol> <p>What you may have noticed from the terminal output when you ran the <code>object_detection.py</code> node is that the robot's camera captures images at a native size of 1080x1920 pixels (you should already know this from interrogating the <code>/camera/rgb/image_raw/width</code> and <code>/height</code> messages using <code>rostopic echo</code> earlier, right?!).  That's over 2 million pixels in total in a single image (2,073,600 pixels per image, to be exact), each pixel having a blue, green and red value associated with it - so that's a lot of data in a single image file! </p> <p>Question</p> <p>The size of the image file (in bytes) was actually printed to the terminal when you ran the <code>object_detection.py</code> node. Did you notice how big it was exactly?</p> <p>Processing an image of this size is therefore hard work for a robot: any analysis we do will be slow and any raw images that we capture will occupy a considerable amount of storage space. The next step then is to reduce this down by cropping the image to a more manageable size.</p> <p>Step 2</p> <p>We're going to modify the <code>object_detection.py</code> node now to:</p> <ul> <li>Capture a new image in its native size</li> <li>Crop it down to focus in on a particular area of interest</li> <li> <p>Save both of the images (the cropped one should be much smaller than the original).</p> </li> <li> <p>In your <code>object_detection.py</code> node locate the line:</p> <pre><code>show_and_save_image(cv_img, img_name = \"step1_original\")\n</code></pre> </li> <li> <p>Underneath this, add the following additional lines of code:</p> <pre><code>crop_width = width - 400\ncrop_height = 400\ncrop_y0 = int((width / 2) - (crop_width / 2))\ncrop_z0 = int((height / 2) - (crop_height / 2))\ncropped_img = cv_img[crop_z0:crop_z0+crop_height, crop_y0:crop_y0+crop_width]\n\nshow_and_save_image(cropped_img, img_name = \"step2_cropping\")\n</code></pre> </li> <li> <p>Run the node again.  </p> <p>Remember</p> <p>Make sure you close all of these pop-up windows down after viewing them to ensure that all your images are saved to the filesystem and the node completes all of its tasks successfully.</p> <p>The code that you have just added here has created a new image object called <code>cropped_img</code>, from a subset of the original by specifying a desired <code>crop_height</code> and <code>crop_width</code> relative to the original image dimensions.  Additionally, we have also specified where in the original image (in terms of pixel coordinates) we want this subset to start, using <code>crop_y0</code> and <code>crop_z0</code>. This process is illustrated in the figure below:</p> <p> </p> <p>The original image (<code>cv_img</code>) is cropped using a process called \"slicing\":</p> <p><pre><code>cropped_img = cv_img[\n    crop_z0:crop_z0+crop_height,\n    crop_y0:crop_y0+crop_width\n    ]\n</code></pre> This may seem quite confusing, but hopefully the figure below illustrates what's going on here:</p> <p> </p> </li> </ul> <p>Step 3</p> <p>As discussed above, an image is essentially a series of pixels each with a blue, green and red value associated with it to represent the actual image colours.  From the original image that we have just obtained and cropped, we then want to get rid of any colours other than those associated with the pillar that we want the robot to detect.  We therefore need to apply a filter to the pixels, which we will ultimately use to discard any pixel data that isn't related to the coloured pillar, whilst retaining data that is.  </p> <p>This process is called masking and, to achieve this, we need to set some colour thresholds. This can be difficult to do in a standard Blue-Green-Red (BGR) or Red-Green-Blue (RGB) colour space, and you can see a good example of this in this article from RealPython.com.  We will apply some steps discussed in this article to convert our cropped image into a Hue-Saturation-Value (HSV) colour space instead, which makes the process of colour masking a bit easier.</p> <ol> <li> <p>First, analyse the Hue and Saturation values of the cropped image. To do this, first navigate to the \"myrosdata/object_detection\" directory, where the raw image has been saved:</p> <p>TERMINAL 2: <pre><code>cd ~/myrosdata/object_detection\n</code></pre></p> <p>Then, run the following ROS Node (from the <code>tuos_examples</code> package), supplying the name of the cropped image as an additional argument:</p> <pre><code>rosrun tuos_examples image_colours.py step2_cropping.jpg\n</code></pre> </li> <li> <p>The node should produce a scatter plot, illustrating the Hue and Saturation values of each of the pixels in the image. Each data point in the plot represents a single image pixel and each is coloured to match its RGB value:</p> <p> </p> </li> <li> <p>You should see from the image that all the pixels related to the coloured pillar that we want to detect are clustered together.  We can use this information to specify a range of Hue and Saturation values that can be used to mask our image: filtering out any colours that sit outside this range and thus allowing us to isolate the pillar itself. The pixels also have a Value (or \"Brightness\"), which isn't shown in this plot. As a rule of thumb, a range of brightness values between 100 and 255 generally works quite well.</p> <p> </p> <p>In this case then, we select upper and lower HSV thresholds as follows:</p> <pre><code>lower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\n</code></pre> <p>Use the plot that has been generated here to determine your own upper and lower thresholds. </p> <p>OpenCV contains a built-in function to detect which pixels of an image fall within a specified HSV range: <code>cv2.inRange()</code>.  This outputs a matrix, the same size and shape as the number of pixels in the image, but containing only <code>True</code> (<code>1</code>) or <code>False</code> (<code>0</code>) values, illustrating which pixels do have a value within the specified range and which don't.  This is known as a Boolean Mask (essentially, a series of ones or zeroes).  We can then apply this mask to the image, using a Bitwise AND operation, to get rid of any image pixels whose mask value is <code>False</code> and keep any flagged as <code>True</code> (or in range).</p> </li> <li> <p>To do this, first locate the following line in your <code>object_detection.py</code> node:</p> <pre><code>show_and_save_image(cropped_img, img_name = \"step2_cropping\")\n</code></pre> </li> <li> <p>Underneath this, add the following:</p> <pre><code>hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\nlower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\nimg_mask = cv2.inRange(hsv_img, lower_threshold, upper_threshold)\n\nshow_and_save_image(img_mask, img_name = \"step3_image_mask\")\n</code></pre> </li> <li> <p>Now, run the node again. Three images should be generated and saved now.  As shown in the figure below, the third image should simply be a black and white representation of the cropped image, where the white regions should indicate the areas of the image where pixel values fall within the HSV range specified earlier.  Notice (from the text printed to the terminal) that the cropped image and the image mask have the same dimensions, but the image mask file has a significantly smaller file size.  While the mask contains the same number of pixels, these pixels only have a value of <code>1</code> or <code>0</code>, whereas - in the cropped image of the same pixel size - each pixel has a Red, Green and Blue value: each ranging between <code>0</code> and <code>255</code>, which represents significantly more data.</p> <p> </p> </li> </ol> <p>Step 4 </p> <p>Finally, we can apply this mask to the cropped image, generating a final version of it where only pixels marked as <code>True</code> in the mask retain their RGB values, and the rest are simply removed.  As discussed earlier, we use a Bitwise AND operation to do this and, once again, OpenCV has a built-in function to do this: <code>cv2.bitwise_and()</code>.</p> <ol> <li> <p>Locate the following line in your <code>object_detection.py</code> node:</p> <pre><code>show_and_save_image(img_mask, img_name = \"step3_image_mask\")\n</code></pre> </li> <li> <p>And, underneath this, add the following:</p> <pre><code>filtered_img = cv2.bitwise_and(cropped_img, cropped_img, mask = img_mask)\n\nshow_and_save_image(filtered_img, img_name = \"step4_filtered_image\")\n</code></pre> </li> <li> <p>Run this node again, and a fourth image should also be generated now, this time showing the cropped image taken from the robot's camera, but only containing data related to coloured pillar, with all other background image data removed (and rendered black):</p> <p> </p> </li> </ol>"},{"location":"com2009/assignment1/part6/#image-moments","title":"Image Moments","text":"<p>You have now successfully isolated an object of interest within your robot's field of vision, but perhaps we want to make our robot move towards it, or - conversely - make our robot navigate around it and avoid crashing into it!  We therefore also need to know the position of the object in relation to the robot's viewpoint, and we can do this using image moments.</p> <p>The work we have just done above led to us obtaining what is referred to as a colour blob.  OpenCV also has built-in tools to allow us to calculate the centroid of a colour blob like this, allowing us to determine where exactly within an image the object of interest is located (in terms of pixels).  This is done using the principle of image moments: essentially statistical parameters related to an image, telling us how a collection of pixels (i.e. the blob of colour that we have just isolated) are distributed within it.  You can read more about Image Moments here, which tells us that the central coordinates of a colour blob can be obtained by considering some key moments of the image mask that we obtained from thresholding earlier:</p> <ul> <li><code>M<sub>00</sub></code>: the sum of all non-zero pixels in the image mask (i.e. the size of the colour blob, in pixels)</li> <li><code>M<sub>10</sub></code>: the sum of all the non-zero pixels in the horizontal (y) axis, weighted by row number</li> <li><code>M<sub>01</sub></code>: the sum of all the non-zero pixels in the vertical (z) axis, weighted by column number</li> </ul> <p>Remember</p> <p>We refer to the horizontal as the y-axis and the vertical as the z-axis here, to match the terminology that we have used previously to define our robot's principal axes.</p> <p>We don't really need to worry about the derivation of these moments too much though.  OpenCV has a built-in <code>moments()</code> function that we can use to obtain this information from an image mask (such as the one that we generated earlier):</p> <pre><code>m = cv2.moments(img_mask)\n</code></pre> <p>So, using this we can obtain the <code>y</code> and <code>z</code> coordinates of the blob centroid quite simply:</p> <pre><code>cy = m['m10']/(m['m00']+1e-5)\ncz = m['m01']/(m['m00']+1e-5) \n</code></pre> <p>Question</p> <p>We're adding a very small number to the <code>M<sub>00</sub></code> moment here to make sure that the divisor in the above equations is never zero and thus ensuring that we never get caught out by any \"divide-by-zero\" errors. Why might this be necessary?</p> <p></p> <p>Once again, there is a built-in OpenCV tool that we can use to add a circle onto an image to illustrate the centroid location within the robot's viewpoint: <code>cv2.circle()</code>.  This is how we produced the red circle that you can see in the figure above.  You can see how this is implemented in a complete worked example of the <code>object_detection.py</code> node from the previous exercise. </p> <p>In our case, we can't actually change the position of our robot in the z axis, so the <code>cz</code> centroid component here might not be that important to us for navigation purposes.  We may however want to use the centroid coordinate <code>cy</code> to understand where a feature is located horizontally in our robot's field of vision, and use this information to turn towards it (or away from it, depending on what we are trying to achieve).  We can then use this as the basis for some real closed-loop control.</p>"},{"location":"com2009/assignment1/part6/#ex3","title":"Exercise 3: Locating image features using Image Moments","text":"<p>Inside the <code>tuos_examples</code> package there is a node that has been developed to illustrate how all the OpenCV tools that you have explored so far could be used to search an environment and stop a robot when it is looking directly at an object of interest. All the tools that are used in this node should be familiar to you by now, and in this exercise you're going to make a copy of this node and modify it to enhance its functionality.</p> <ol> <li> <p>The node is called <code>colour_search.py</code>, and it is located in the <code>src</code> folder of the <code>tuos_examples</code> package. Copy this into the <code>src</code> folder of your own <code>part6_vision</code> package by first ensuring that you are located in the desired destination folder:</p> <p>TERMINAL 2: <pre><code>roscd part6_vision/src\n</code></pre></p> </li> <li> <p>Then, copy the <code>colour_search.py</code> node using <code>cp</code> as follows:</p> <p>TERMINAL 2: <pre><code>cp ~/catkin_ws/src/tuos_ros/tuos_examples/src/colour_search.py ./\n</code></pre></p> </li> <li> <p>You'll need to copy the <code>tb3.py</code> module across from the <code>tuos_examples</code> package, as this is used by the <code>colour_search.py</code> node to make the robot move:</p> <p>TERMINAL 2: <pre><code>cp ~/catkin_ws/src/tuos_ros/tuos_examples/src/tb3.py ./\n</code></pre></p> </li> <li> <p>Open up the <code>colour_search.py</code> file in VS Code to view the content.  Have a look through it and see if you can make sense of how it works.  The overall structure should be fairly familiar to you by now: we have a Python class structure, a Subscriber with a callback function, a main loop where all the robot control takes place and a lot of the OpenCV tools that you have explored so far in this session.  Essentially this node functions as follows:</p> <ol> <li>The robot turns on the spot whilst obtaining images from its camera (by subscribing to the <code>/camera/rgb/image_raw</code> topic).</li> <li>Camera images are obtained, cropped, then a threshold is applied to the cropped images to detect the blue pillar in the simulated environment.</li> <li>If the robot can't see a blue pillar then it turns on the spot quickly.</li> <li>Once detected, the centroid of the blue blob representing the pillar is calculated to obtain its current location in the robot's viewpoint.</li> <li>As soon as the blue pillar comes into view the robot starts to turn more slowly instead.</li> <li>The robot stops turning as soon as it determines that the pillar is situated directly in front of it (determined using the <code>cy</code> component of the blue blob centroid).</li> <li>The robot then waits for a while and then starts to turn again.</li> <li>The whole process repeats until it finds the blue pillar once again.</li> </ol> </li> <li>Run the node as it is to see this in action.  Observe the messages that are printed to the terminal throughout execution.</li> <li>Your task is to then modify the node so that it stops in front of every coloured pillar in the arena (there are four in total). For this, you may need to use some of the methods that you have explored in the previous exercises.<ol> <li>You might first want to use some of the methods that we used to obtain and analyse some images from the robot's camera:<ol> <li>Use the <code>turtlebot3_teleop</code> node to manually move the robot, making it look at every coloured pillar in the arena individually.</li> <li>Run the <code>object_detection.py</code> node that you developed in the previous exercise to capture an image, crop it, save it to the filesystem and then feed this cropped image into the <code>image_colours.py</code> node from the <code>tuos_examples</code> package (as you did earlier)</li> <li>From the plot that is generated by the <code>image_colours.py</code> node, determine some appropriate HSV thresholds to apply for each coloured pillar in the arena.</li> </ol> </li> <li>Once you have the right thresholds, then you can add these to your <code>colour_search.py</code> node so that it has the ability to detect every pillar in the same way that it currently detects the blue one.</li> </ol> </li> </ol>"},{"location":"com2009/assignment1/part6/#pid","title":"PID Control and Line Following","text":"<p>Line following is a handy skill for a robot to have! We can achieve this on our TurtleBot3 using its camera system and the image processing techniques that have been covered so far in this session.</p> <p>COM2009 Lecture 6 introduces a well established algorithm for closed-loop control known as PID Control, and this can be used to achieve such line following behaviour.</p> <p>At the heart of this is the principle of Negative-Feedback control, which considers a Reference Input, a Feedback Signal and the Error between these.</p> <p></p> <p> </p>      Negative-Feedback Control     Adapted from Arturo Urquizo (via Wikimedia Commons) <p>The Reference Input represents a desired state that we would like our system to maintain. If we want our TurtleBot3 to successfully follow a coloured line on the floor, we will need it to keep the colour blob that represents that coloured line in the centre of its view point at all times. The desired state would therefore be to maintain the <code>cy</code> centroid of the colour blob in the centre of its vision.</p> <p>A Feedback Signal informs us of what the current state of the system actually is. In our case, this feedback signal would be the real-time location of the coloured line in the live camera images, i.e. its <code>cy</code> centroid (obtained using processing methods such as those covered in Exercise 3 above). </p> <p>The difference between these two things is the Error, and the PID control algorithm provides us with a means to control this error and minimise it, so that our robot's actual state matches the desired state. i.e.: the coloured line is always in the centre of its viewpoint.</p> <p></p> <p></p> <p></p> <p>The PID algorithm is as follows:</p> \\[ u(t)=K_{P} e(t) + K_{I}\\int e(t)dt + K_{D}\\dfrac{d}{dt}e(t) \\] <p>Where \\(u(t)\\) is the Controlled Output, \\(e(t)\\) is the Error (as illustrated in the figure above) and \\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\) are Proportional, Integral and Differential Gains respectively. These three gains are constants that must be established for any given system through a process called tuning. This tuning process is discussed in COM2009 Lecture 6, but you will also explore this in the practical exercise that follows.</p>"},{"location":"com2009/assignment1/part6/#ex4","title":"Exercise 4: Line Following","text":""},{"location":"com2009/assignment1/part6/#ex4a","title":"Part A: Setup","text":"<ol> <li>Make sure that all ROS processes from the previous exercise are shut down now, including the <code>colour_search.py</code> node, and the Gazebo simulation in TERMINAL 1.</li> <li> <p>In TERMINAL 1 launch a new simulation from the <code>tuos_simulations</code> package:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_simulations line_following_setup.launch\n</code></pre></p> <p>Your robot should be launched onto a long thin track with a straight pink line painted down the middle of the floor:</p> <p> </p> </li> <li> <p>In TERMINAL 2 you should still be located in your <code>part6_vision/src</code> directory, but if not then go there now:</p> <p>TERMINAL 2: <pre><code>roscd part6_vision/src\n</code></pre></p> </li> <li> <p>Perform the necessary steps to create a new empty Python file called <code>line_follower.py</code> and prepare it for execution.</p> </li> <li> <p>Once that's done open up the empty file in VS Code.</p> <p></p> </li> <li> <p>Start with the code template provided here. This template contains three \"TODOs\" that you need to complete, all of which are explained in detail in the code annotations, so read these carefully. Ultimately, you did all of this in Exercise 2, so go back here if you need a reminder on how any of this works. </p> <p>Your aim here is to get the code to generate a cropped image, with the coloured line isolated and located within it, like this:</p> <p> </p> </li> </ol>"},{"location":"com2009/assignment1/part6/#ex4b","title":"Part B: Implementing and Tuning a Proportional Controller","text":"<p>Referring back to the equation for the PID algorithm as discussed above, the Proportional, Integral and Differential components all have different effects on a system in terms of its ability to maintain the desired state (the reference input). The gain terms associated with each of these components (\\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\)) must be tuned appropriately for any given system in order to achieve stability of control.</p> <p>A PID Controller can actually take three different forms:</p> <ol> <li>\"P\" Control: Only a Proportional gain (\\(K_{P}\\)) is used, all other gains are set to zero.</li> <li>\"PI\" Control: Proportional and Integral gains (\\(K_{P}\\) and \\(K_{I}\\)) are applied, the Differential gain is set to zero. </li> <li>\"PID\" Control: The controller makes use of all three gain terms (\\(K_{P}\\), \\(K_{I}\\) and \\(K_{D}\\))</li> </ol> <p>In order to allow our TurtleBot3 to follow a line, we actually only really need a \"P\" Controller, so our control equation becomes quite simple, reducing to:</p> \\[ u(t)=K_{P} e(t) \\] <p>The next task then is to adapt our <code>line_follower.py</code> node to implement this control algorithm and find a proportional gain that is appropriate for our system.</p> <ol> <li> <p>Return to your <code>line_follower.py</code> file. Underneath the line that reads:</p> <pre><code>cv2.waitKey(1)\n</code></pre> <p>Paste the following additional code:</p> <pre><code>kp = 0.01\nreference_input = {BLANK}\nfeedback_signal = cy\nerror = feedback_signal - reference_input \n\nang_vel = kp * error\nprint(f\"Error = {error:.1f} pixels | Control Signal = {ang_vel:.2f} rad/s\")\n</code></pre> <p></p> <p>Fill in the Blank!</p> <p>What is the Reference Input to the control system (<code>reference_input</code>)? Refer to this figure from earlier. </p> <p>Here we have implemented our \"P\" Controller. The Control Signal that is being calculated here is the angular velocity that will be applied to our robot (the code won't make the robot move just yet, but we'll get to that bit shortly!) The Controlled Output will therefore be the angular position (i.e. the yaw) of the robot.  </p> </li> <li> <p>Run the code as it is, and consider the following:</p> <ol> <li>What proportional gain (\\(K_{P}\\)) are we applying?</li> <li>What is the maximum angular velocity that can be applied to our robot? Is the angular velocity that has been calculated actually appropriate?</li> <li>Is the angular velocity that has been calculated positive or negative? Will this make the robot turn in the right direction and move towards the line?  </li> </ol> </li> <li> <p>Let's address the third question (c) first...</p> <p>A positive angular velocity should make the robot turn anti-clockwise (i.e. to the left), and a negative angular velocity should make the robot turn clockwise (to the right). The line should currently be to the left of the robot, which means a positive angular velocity would be required in order to make the robot turn towards it. If the value of the Control Signal that is being calculated by our proportional controller (as printed to the terminal) is negative, then this isn't correct, so we need to change the sign of our proportional gain (\\(K_{P}\\)) in order to correct this:</p> <pre><code>kp = -0.01\n</code></pre> </li> <li> <p>Next, let's address the second of the above questions (b)...</p> <p>The maximum angular velocity that can be applied to our robot is \u00b11.82 rad/s. If our proportional controller is calculating a value for the Control Signal that is greater than 1.82, or less than -1.82 then this needs to be limited. In between the following two lines of code:</p> <pre><code>ang_vel = kp * error\nprint(f\"Error = {error:.1f} pixels | Control Signal = {ang_vel:.2f} rad/s\")\n</code></pre> <p>Insert the following: <pre><code>if ang_vel &lt; -1.82:\n    ang_vel = -1.82\nelif ang_vel &gt; 1.82:\n    ang_vel = 1.82\n</code></pre></p> </li> <li> <p>Finally, we need to think about the actual proportional gain that is being applied. This is where we need to actually tune our system by finding a proportional gain value that controls our system appropriately.</p> <p>Return to your <code>line_follower.py</code> file. Underneath the line that reads:</p> <pre><code>print(f\"Error = {error:.1f} pixels | Control Signal = {ang_vel:.2f} rad/s\")\n</code></pre> <p>Paste the following:</p> <pre><code>self.robot_controller.set_move_cmd(linear = 0.1, angular = ang_vel)\nself.robot_controller.{BLANK}\n</code></pre> <p></p> <p>Fill in the Blank!</p> <p>There is a method within the <code>Tb3Move()</code> class which allows us to publish a velocity command to the <code>/cmd_vel</code> topic. What is it? (Have a look at the <code>tb3.py</code> source code, or the <code>colour_search.py</code> file from Exercise 3 if you need a reminder).</p> <p>Having filled in the <code>{BLANK}</code>, the code will now make the robot move with a constant linear velocity of 0.1 m/s at all times, while its angular velocity will be determined by our proportional controller, based on the controller error and the proportional gain parameter <code>kp</code>.</p> <p>The figure below illustrates the effects different values of proportional gain can have on a system.</p> <p>      Courtesy of Prof. Roger Moore     Taken from COM2009 Lecture 6: PID Control    </p> <p>Run the code and see what happens. You should find that the robot behaves quite erratically, indicating that <code>kp</code> (at an absolute value of 0.01) is probably too large.</p> </li> <li> <p>Try reducing <code>kp</code> by a factor of 100: <code>kp = -0.0001</code>. Before you run the code again, you can reset the Gazebo simulation by pressing Ctrl+Shift+R so that the robot returns to the starting position.</p> <p>You should find that the robot now gradually approaches the line, but it can take a while for it to do so.</p> </li> <li> <p>Next, increase <code>kp</code> by a factor of 10: <code>kp = -0.001</code>. Once again, reset the robot back to its starting position in Gazebo by using Ctrl+Shift+R to reset the simulation.</p> <p>The robot should now reach the line much quicker, and follow the line well once it reaches it.</p> </li> <li> <p>Could <code>kp</code> be modified any more to improve the control further? Play around a bit more and see what happens. We'll but this to the test on a more challenging track in the next part of this exercise.</p> </li> </ol>"},{"location":"com2009/assignment1/part6/#ex4c","title":"Part C: Advanced Line Following","text":"<ol> <li> <p>Now, in TERMINAL 1 run a new simulation:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_simulations line_following.launch\n</code></pre></p> <p>Your robot should be launched into an environment with a more interesting line to follow:</p> <p> </p> </li> <li> <p>In TERMINAL 2, run your <code>line_follower.py</code> node and see how it performs. Does your proportional gain need to be adjusted further to optimise the performance?</p> </li> <li> <p>Next, think about conditions where the line can't initially be seen...</p> <p>As you know, the angular velocity is determined by considering the <code>cy</code> component of a colour blob representing the line. What happens in situations where the blob of colour isn't there though?  What influence would this have on the Control Signals that are calculated by the proportional controller? To consider this further, try launching the robot in the same arena but in a different location instead, and think about how you might approach this situation:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_simulations line_following.launch x_pos:=3 y_pos:=-3 yaw:=0\n</code></pre></p> </li> <li> <p>Finally, what happens when the robot reaches the finish line? How could you add additional functionality to ensure that the robot stops when it reaches this point? What features of the arena could you use to trigger this?</p> </li> </ol>"},{"location":"com2009/assignment1/part6/#wrapping-up","title":"Wrapping Up","text":"<p>In this session you have learnt how to use data from a robot's camera to extract further information about its environment.  The camera allows our robot to \"see\" and the information that we obtain from this device can allow us to develop more advanced robotic behaviours such as searching for objects, follow things or - conversely - moving away or avoiding them.  You have learnt how to do some basic tasks with OpenCV, but this is a huge and very capable library of computer vision tools, and we encourage you to explore this further yourselves to enhance some of the basic principles that we have shown you today.</p>"},{"location":"com2009/assignment1/part6/#backup","title":"WSL-ROS Managed Desktop Users: Save your work!","text":"<p>Remember, to save the work you have done in WSL-ROS during this session so that you can restore it on a different machine at a later date. Run the following script in any idle WSL-ROS Terminal Instance now:</p> <pre><code>wsl_ros backup\n</code></pre> <p>You'll then be able to restore it to a fresh WSL-ROS environment whenever you need it again (<code>wsl_ros restore</code>).  </p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> <li> <p>Camera topic names are slightly different on the real robots though, so look out for that!\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/assignment1/part1/publisher/","title":"Part 1 Publisher Node","text":""},{"location":"com2009/assignment1/part1/publisher/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>publisher.py</code> file and review the annotations to understand how it all works.</p> <p>Tip</p> <p>Don't forget the Shebang! See below for further details...</p> publisher.py<pre><code>#!/usr/bin/env python3\n# A simple ROS publisher node in Python\n\nimport rospy # (1)!\nfrom std_msgs.msg import String # (2)!\n\nclass Publisher(): # (3)!\n\n    def __init__(self): # (4)!\n        self.node_name = \"simple_publisher\" # (5)!\n        topic_name = \"chatter\" # (6)!\n\n        self.pub = rospy.Publisher(topic_name, String, queue_size=10) # (7)!\n        rospy.init_node(self.node_name, anonymous=True) # (8)!\n        self.rate = rospy.Rate(10) # (9)!\n\n        self.ctrl_c = False # (10)!\n        rospy.on_shutdown(self.shutdownhook) \n\n        rospy.loginfo(f\"The '{self.node_name}' node is active...\") # (11)!\n\n    def shutdownhook(self): # (12)!\n        print(f\"Stopping the '{self.node_name}' node at: {rospy.get_time()}\")\n        self.ctrl_c = True\n\n    def main(self):\n        while not self.ctrl_c: # (13)!\n            publisher_message = f\"rospy time is: {rospy.get_time()}\"\n            self.pub.publish(publisher_message)\n            self.rate.sleep()\n\nif __name__ == '__main__': # (14)!\n    node = Publisher() # (15)!\n    node.main() # (16)!\n</code></pre> <ol> <li> <p><code>rospy</code> is the Python client library for ROS, and we need to import this in order to create ROS Nodes in Python.</p> </li> <li> <p>We also need to import the <code>String</code> message type from the <code>std_msgs.msg</code> library for publishing our messages.</p> </li> <li> <p>We create a Python class called <code>Publisher()</code> to encapsulate all the functionality of our node.</p> </li> <li> <p>The <code>__init__()</code> method is called as soon as an instance of the <code>Publisher()</code> class is created.</p> </li> <li> <p>We define a name for this node and assign it to <code>self.node_name</code>. We can call the node anything that we want, but it's good to give it a meaningful name as this is the name that will be used to register the node on the ROS Network.</p> </li> <li> <p>We also define the name of a topic that we want to publish messages to (<code>\"chatter\"</code> in this case). If this is the name of a topic that already exists on the ROS Network, then we need to ensure that we use the correct message type, in order to be able to publish to it. In this case however, a topic called \"chatter\" shouldn't currently exist, so it will be created for us, and we can choose whatever type of message we want to use (a <code>String</code> message from the <code>std_msgs</code> library in our case).</p> </li> <li> <p>We then create an instance of a <code>rospy.Publisher()</code> object within our class: this creates the topic on the ROS Network (if it doesn't already exist). We therefore need to tell it the name of the topic that we want to create, and we also need to specify that we will be publishing <code>String</code> type messages.</p> </li> <li> <p>Then, we initialise our Python node, using the name that we defined earlier (<code>\"simple_publisher\"</code>), setting the <code>anonymous</code> flag to <code>True</code> to ensure that the node name is unique, by appending random numbers to it (we'll observe this when we run the node shortly).</p> </li> <li> <p>Then, we instantiate a <code>rospy.Rate()</code> object and set the frequency to 10 Hz, so that our publisher will publish messages at this frequency.</p> </li> <li> <p>This is used to shut down a ROS node effectively:</p> <ol> <li>First, we create a <code>ctrl_c</code> variable within the parent class and initialise it to <code>False</code>.</li> <li>Then, we use the <code>rospy.on_shutdown()</code> method to register a shutdown hook (in this case a function called <code>shutdownhook</code>). This will be called when rospy detects that the node has been asked to stop (i.e. by a user entering <code>Ctrl+C</code> in the terminal, for example). The shutdown hook function must take no arguments.</li> </ol> </li> <li> <p>Finally, we issue a message to indicate that our node is active (this will appear in the terminal that we run the node in):</p> </li> <li> <p>This method is called by the <code>rospy.on_shutdown()</code> method when the node is stopped. </p> <p>Here, we can include any important shutdown processes (making a robot stop moving, for instance). In this case, we just print a message to the terminal and then set the <code>ctrl_c</code> variable to <code>True</code>, which will stop the <code>main()</code> method...</p> </li> <li> <p>The <code>while</code> loop here makes sure that the <code>main()</code> runs continuously, until the node is shut down (via the <code>self.ctrl_c</code> flag):</p> <p>Inside the <code>while</code> loop we create a publisher message (a simple string in this case), publish it using the <code>pub</code> object we created in the initialisation stage, and then use the <code>rate</code> object (also created earlier) to then make the node \"sleep\" for as long as required to satisfy the frequency that we defined earlier.</p> </li> <li> <p>This <code>__name__</code> check, ensures that our node is the main executable (i.e. it has been executed directly (via <code>rosrun</code>), and hasn't been called by another script):</p> </li> <li> <p>We create an instance of the <code>Publisher</code> class that we created above (which executes the <code>__init__</code> method automatically). </p> </li> <li> <p>We call the <code>main()</code> of our <code>Publisher()</code> class to execute the core functionality of the node. </p> </li> </ol>"},{"location":"com2009/assignment1/part1/publisher/#shebang","title":"The Shebang","text":"<p>The very first line of code looks like a comment, but it is actually a very crucial part of the script:</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>This is called the Shebang, and it tells the operating system which interpreter to use to execute the code. In our case here, it tells the operating system where to find the right Python interpreter that should be used to actually run the code.</p> <p> \u2190 Back to Part 1 - Exercise 5 </p>"},{"location":"com2009/assignment1/part1/subscriber/","title":"Part 1 Subscriber Node","text":""},{"location":"com2009/assignment1/part1/subscriber/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>subscriber.py</code> file and (again) make sure you read the annotations to understand how it all works!</p> subscriber.py<pre><code>#!/usr/bin/env python3\n# A simple ROS subscriber node in Python\n\nimport rospy # (1)!\nfrom std_msgs.msg import String\n\nclass Subscriber(): # (2)!\n\n    def callback(self, topic_message): # (3)!\n        print(f\"The '{self.node_name}' node obtained the following message: '{topic_message.data}'\")\n\n    def __init__(self): # (4)!\n        self.node_name = \"simple_subscriber\"\n        topic_name = {BLANK}\n\n        rospy.init_node(self.node_name, anonymous=True)\n        self.sub = rospy.Subscriber(topic_name, String, self.callback)\n        rospy.loginfo(f\"The '{self.node_name}' node is active...\")\n\n    def main(self):\n        rospy.spin() # (5)!\n\nif __name__ == '__main__': # (6)!\n    node = Subscriber()\n    node.main()\n</code></pre> <ol> <li> <p>As with our publisher node, we need to import the <code>rospy</code> client library and the <code>String</code> message type from the <code>std_msgs.msg</code> library in order to write a Python ROS Node and use the relevant ROS messages:</p> </li> <li> <p>This time, we create a Python Class called <code>Subscriber()</code> instead.</p> </li> <li> <p>When building a subscriber, we need a callback function. Within this function, we define what we want to do with the messages that we obtain from the topic we're listening (subscribing) to:</p> <p>In this case, we simply want to print the <code>String</code> message to the terminal.</p> </li> <li> <p>Here we define the initialisation operations for the class:</p> <ol> <li>Here, we firstly initialise a rospy node with a custom name (in the same way as we initialised the publisher node earlier). </li> <li>Then, we create a <code>rospy.Subscriber</code> object, set this to listen to the topic that we want to receive messages from, specify the message type used by this topic, and then define the callback function to use to process the data whenever a message comes in.</li> <li>Then, we send a message to the terminal to indicate that our node is running.</li> </ol> </li> <li> <p>The <code>rospy.spin()</code> method simply makes sure that our <code>main()</code> keeps running until the node is shut down externally (i.e. by a user entering <code>Ctrl+C</code>).</p> </li> <li> <p>Finally, the code is executed by again performing a <code>__name__</code> check, creating an instance of the <code>Subscriber()</code> class and calling the <code>main()</code> method from that class.</p> </li> </ol> <p></p> <p>Fill in the Blank!</p> <p>Replace the <code>{BLANK}</code> in the code above with the name of the topic that our <code>publisher.py</code> node was set up to publish to!</p>"},{"location":"com2009/assignment1/part1/subscriber/#dfts","title":"Don't Forget the Shebang!","text":"<p>First, don't forget the shebang, it's very important!</p> <pre><code>#!/usr/bin/env python3\n</code></pre>"},{"location":"com2009/assignment1/part1/subscriber/#a-simpler-approach","title":"A Simpler Approach","text":"<p>The above code uses a Python Class structure.  This approach will be very useful when we start to do more complex things later in the course, but for this exercise you could also achieve the same using the following simplified approach:</p> <pre><code>#!/usr/bin/env python3\n\nimport rospy\nfrom std_msgs.msg import String\n\nnode_name = \"simple_subscriber\"\ntopic_name = {BLANK}\n\ndef callback_function(topic_message):\n    print(f\"The '{node_name}' node obtained the following message: '{topic_message.data}'\")\n\nrospy.init_node(node_name, anonymous=True)\nsub = rospy.Subscriber(topic_name, String, callback_function)\nrospy.loginfo(f\"The '{node_name}' node is active...\")\n\nrospy.spin()\n</code></pre> <p> \u2190 Back to Part 1 - Exercise 6 </p>"},{"location":"com2009/assignment1/part2/move_square/","title":"A Move Square Python Template for Part 2","text":"<p>A combined publisher-subscriber node to achieve odometry-based control...</p> <p>Below you will find a template Python script to show you how you can both publish to <code>/cmd_vel</code> and subscribe to <code>/odom</code> in the same node.  This will help you build a closed-loop controller to make your robot follow a square motion path of size: 1m x 1m. </p> <p>You can publish velocity commands to <code>/cmd_vel</code> to make the robot move, monitor the robot's position and orientation in real-time, determine when the desired movement has been completed, and then update the velocity commands accordingly.  </p>"},{"location":"com2009/assignment1/part2/move_square/#suggested-approach","title":"Suggested Approach","text":"<p>Moving in a square can be achieved by switching between two different movement states sequentially: Moving forwards and turning on the spot. At the start of each movement step we can read the robot's current odometry, and then use this as a reference to compare to, and to tell us when the robot's position/orientation has changed by the required amount, e.g.:</p> <ol> <li>With the robot stationary, read the odometry to determine its current X and Y position in the environment.</li> <li>Move forwards until the robot's X and Y position indicate that it has moved linearly by 0.5m.</li> <li>Stop moving forwards.</li> <li>Read the robot's odometry to determine its current orientation (\"yaw\"/<code>\u03b8<sub>z</sub></code>).</li> <li>Turn on the spot until the robot's orientation changes by 90\u00b0.</li> <li>Stop turning.</li> <li>Repeat.  </li> </ol> move_square.py<pre><code>import rospy\nfrom geometry_msgs.msg import Twist # (1)!\nfrom nav_msgs.msg import Odometry # (2)!\nfrom tf.transformations import euler_from_quaternion # (3)!\nfrom math import sqrt, pow, pi # (4)!\n\nclass Square():\n    def callback(self, topic_data: Odometry):\n        pose = topic_data.pose.pose # (5)!\n        position = pose.position\n        orientation = pose.orientation\n\n        pos_x = position.x # (6)!\n        pos_y = position.y\n\n        (roll, pitch, yaw) = euler_from_quaternion(\n            [orientation.x, orientation.y, orientation.z, orientation.w], \"sxyz\"\n        ) # (7)!\n\n        self.x = pos_x # (8)!\n        self.y = pos_y\n        self.theta_z = yaw\n\n        if not self.first_message: # (9)!\n            self.first_message = True\n\n    def __init__(self):\n        node_name = \"move_square\"\n        self.first_message = False\n        self.turn = False # (10)!\n\n        # (11)!\n        self.pub = rospy.Publisher(\"cmd_vel\", Twist, queue_size=10)\n        self.sub = rospy.Subscriber(\"odom\", Odometry, self.callback)\n\n        rospy.init_node(node_name, anonymous=True)\n        self.rate = rospy.Rate(10)  # hz\n\n        # (12)!\n        self.x = 0.0\n        self.y = 0.0\n        self.theta_z = 0.0\n        self.x0 = 0.0\n        self.y0 = 0.0\n        self.theta_z0 = 0.0\n\n        self.vel = Twist() # (13)!\n\n        self.ctrl_c = False\n        rospy.on_shutdown(self.shutdownhook)\n\n        rospy.loginfo(f\"the {node_name} node has been initialised...\")\n\n    def shutdownhook(self):\n        self.pub.publish(Twist()) # (14)!\n        self.ctrl_c = True\n\n    def main(self):\n        while not self.ctrl_c:\n            # here is where your code would go to control the motion of your\n            # robot. Add code here to make your robot move in a square of\n            # dimensions 1 x 1m...\n\n            # publish whatever velocity command has been set in your code above:\n            self.pub.publish(self.vel)\n            self.rate.sleep() # maintain the loop rate @ 10 hz\n\nif __name__ == \"__main__\":\n    node = Square()\n    node.main()\n</code></pre> <ol> <li>Import the <code>Twist</code> message for publishing velocity commands to <code>/cmd_vel</code>.</li> <li>Import the <code>Odometry</code> message, for use when subscribing to the <code>/odom</code> topic.</li> <li>Import the <code>euler_from_quaternion</code> function to convert orientation from quaternions (as provided in the <code>Odometry</code> message) to Euler angles (about the principal axes).</li> <li> <p>Finally, import some useful mathematical operations (and <code>pi</code>), which you may find useful:</p> Mathematical Operation Python Implementation \\(\\sqrt{a+b}\\) <code>sqrt(a+b)</code> \\(a^{2}+(bc)^{3}\\) <code>pow(a, 2) + pow(b*c, 3)</code> \\(\\pi r^2\\) <code>pi * pow(r, 2)</code> </li> <li> <p>Obtain the relevant data from the <code>/odom</code> topic. For this, we need to know the robot's position and orientation in its environment, all of which is contained within the \"pose\" part of the <code>Odometry</code> message:</p> <pre><code>pose = topic_data.pose.pose\n</code></pre> <p>From there, we can separate out the position and orientation parts:</p> <pre><code>position = pose.position\n</code></pre> <pre><code>orientation = pose.orientation\n</code></pre> </li> <li> <p>Here we obtain the robot's current position coordinates.</p> </li> <li> <p>And here we obtain the robot's current orientation (in quaternions) and convert it to Euler angles (in radians) about the principal axes, where:</p> <ul> <li>\"roll\" = <code>\u03b8<sub>x</sub></code></li> <li>\"pitch\" = <code>\u03b8<sub>y</sub></code></li> <li>\"yaw\" = <code>\u03b8<sub>z</sub></code></li> </ul> </li> <li> <p>We're only interested in <code>x</code>, <code>y</code> and <code>\u03b8<sub>z</sub></code>, so we assign these to class variables <code>self.x</code>, <code>self.y</code> and <code>self.theta_z</code>, so that we can access them elsewhere within our <code>Square()</code> class.</p> </li> <li> <p>Sometimes, it can take a few moments for the first topic message to come through, and it's useful to know when that's happened so that you know you are dealing with actual topic data! Here, we're just setting a flag to <code>True</code> once the callback function has executed for the first time (i.e. the first topic message has been received).</p> </li> <li> <p>This might be useful in the <code>main()</code> class method (below), to switch between turning and moving forwards...</p> </li> <li> <p>This node needs to read message data from one topic (<code>/odom</code>) and write messages on another (<code>/cmd_vel</code>), so we need to set up a subscriber and a publisher here accordingly. </p> </li> <li> <p>Here, we define some variables that we can use to store relevant bits of odometry data while our node is running (and read it back to implement feedback control):</p> <ul> <li><code>self.x</code>, <code>self.y</code> and <code>self.theta_z</code> will be used by the callback function to store the robot's current pose</li> <li><code>self.x0</code>, <code>self.y0</code> and <code>self.theta_z0</code> can be used in the <code>main()</code> method to keep a record of where the robot was at a given moment in time (and determine how far it has moved since that point)</li> </ul> </li> <li> <p>Here we establish a <code>Twist</code> message, which we can populate with velocities and then publish to <code>/cmd_vel</code> within the <code>main()</code> method in order to make the robot move.</p> </li> <li> <p>Publish an empty <code>Twist</code> message to stop the robot (by default all velocities will be zero).</p> </li> </ol>"},{"location":"com2009/assignment1/part2/move_square/#alternative-approach-waypoint-tracking","title":"Alternative Approach: Waypoint Tracking","text":"<p>A square motion path can be fully defined by the coordinates of its four corners, and we can make the robot move to each of these corners one-by-one, using its odometry system to monitor its real-time position, and adapting linear and angular velocities accordingly.</p> <p>This is slightly more complicated, and you might want to wait until you have a bit more experience with ROS before tackling it this way (we'll also cover this in the COM2009 lecture course).</p> <p> \u2190 Back to Part 2 - Exercise 5 </p>"},{"location":"com2009/assignment1/part2/twist-tips/","title":"Working with Twist Messages in Python","text":"<p>From the Part 1 publisher exercise, we know how to publish a <code>String</code> type message to a topic in Python, but how do we apply the same principles to a <code>Twist</code> message (on the <code>/cmd_vel</code> topic)? Let's have a look at this... </p> <p>First, you need to import the <code>rospy</code> library, as well as the <code>Twist</code> message type from the <code>geometry_msgs</code> library:</p> <pre><code>import rospy\nfrom geometry_msgs.msg import Twist\n</code></pre> <p>Then, create an instance of a <code>rospy.Publisher()</code> and assign it to an object called <code>pub</code>. When we create the object we tell the <code>Publisher()</code> method which topic we want to publish this message to (via the first input argument), and also that we will be publishing a message of the <code>Twist</code> type (the second input argument):</p> <pre><code>pub = rospy.Publisher({topic name}, Twist, queue_size=10) # a queue size of 10 usually works!\n</code></pre> <p>Then we need to create a <code>Twist()</code> message instance and assign it to an object (which we'll call <code>vel_cmd</code>):</p> <pre><code>vel_cmd = Twist()\n</code></pre> <p>We know from earlier that the <code>geometry_msgs/Twist</code> message has the format:</p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x\n  float64 y\n  float64 z\ngeometry_msgs/Vector3 angular\n  float64 x\n  float64 y\n  float64 z\n</code></pre> <p>We also know, that only velocity commands issued to the following two parameters will actually have any effect on the velocity of our robot:</p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x\n</code></pre> <p>...and:</p> <pre><code>geometry_msgs/Vector3 angular\n  float64 z\n</code></pre> <p>As such, we set appropriate velocity values to these attributes of the <code>Twist()</code> message (assigned to <code>vel_cmd</code>):</p> <pre><code>vel_cmd.linear.x = 0.0 # m/s\nvel_cmd.angular.z = 0.0 # rad/s\n</code></pre> <p>We can then publish this to the relevant topic on the ROS network by supplying it to the <code>rospy.Publisher().publish()</code> method (which we instantiated as <code>pub</code> earlier):</p> <pre><code>pub.publish(vel_cmd)\n</code></pre> <p>Use these pointers when working on your <code>move_circle.py</code> node!</p> <p> \u2190 Back to Part 2 - Exercise 4 </p>"},{"location":"com2009/assignment1/part4/move_client/","title":"Part 4 Move Service-Client","text":"<p>Copy all the code below into your <code>move_client.py</code> file and review the annotations to understand how it all works.</p> <p>DFTS!!</p> <p>(Don't forget the shebang!) <pre><code>#!/usr/bin/env python3\n</code></pre></p> move_client.py<pre><code>#!/usr/bin/env python3\n\nimport rospy # (1)!\nfrom tuos_msgs.srv import SetBool, {BLANK} # (2)!\n\nservice_name = \"move_service\" # (3)!\n\nrospy.init_node(f\"{service_name}_client\") # (4)!\n\nrospy.wait_for_service(service_name) # (5)!\n\nservice = rospy.ServiceProxy(service_name, SetBool) # (6)!\n\nrequest_to_server = {BLANK}() # (7)!\nrequest_to_server.request_signal = True # (8)!\n\nresponse_from_server = service(request_to_server) # (9)!\nprint(response_from_server) # (10)!\n</code></pre> <ol> <li> <p>Again, the first step when building a Python node is to import the <code>rospy</code> library so that Python and ROS can interact. </p> </li> <li> <p>This service client will use the <code>SetBool</code> service message from the <code>tuos_msgs</code> package, so we import the full definition of the <code>SetBool</code> Service Message, as well as the portion of the message that we will need to use to actually issue a service call.</p> </li> <li> <p>Define the name of the service that we want to call, and assign this to a variable called <code>service_name</code> (for convenience, since we'll refer to this a couple of times). </p> </li> <li> <p>Initialise the client node (give it a name).</p> </li> <li> <p>Wait until the service that we want to call is actually running, execution of this node will not progress beyond this point until the service is detected on the ROS network (launched by the Server).</p> </li> <li> <p>Once it is running, we create a connection to it and specify the service message type that it uses (as defined above).</p> </li> <li> <p>Create an instance of the <code>{BLANK}</code> part of the service message, and populate this with the data that the server is expecting.</p> </li> <li> <p>Remember: Using <code>rossrv info</code> on this service message in a terminal tells us the attribute names for both the Request and Response:</p> <p><pre><code>rossrv info tuos_msgs/SetBool\n</code></pre> ...gives us the following: <pre><code>bool request_signal\n---\nbool response_signal\nstring response_message\n</code></pre></p> </li> <li> <p>Use the <code>rospy.ServiceProxy</code> instance that we created earlier (called <code>service</code>) to actually send the <code>request_to_server</code> message to the service and obtain a response back from the Server (once it's complete).</p> </li> <li> <p>To finish off, we print the response to the terminal to give the user some feedback. Job done!</p> </li> </ol> <p></p> <p>Fill in the Blank!</p> <p>Consider the <code>import</code> statement for the service Server that we created earlier... Which part of the <code>SetBool</code> Service message was imported here? Now consider that you need to build a client to call this service... which part of the <code>SetBool</code> Service message is needed in order to call a service? </p> <p>Note: the same <code>{BLANK}</code> appears in two places in the code above - the answer is the same in both places!</p> <p> \u2190 Back to Part 4 - Exercise 2 </p>"},{"location":"com2009/assignment1/part4/move_server/","title":"Part 4 Move Service-Server","text":"<p>Copy all the code below into your <code>move_server.py</code> file and review the annotations to understand how it all works.</p> <p>Remember</p> <p>Don't forget the shebang!</p> <pre><code>#!/usr/bin/env python3\n</code></pre> move_server.py<pre><code>#!/usr/bin/env python3 \n\nimport rospy\nfrom {BLANK}.msg import Twist # (1)!\nfrom tuos_msgs.srv import SetBool, SetBoolResponse # (2)!\n\nclass moveService():\n\n    def __init__(self):\n        service_name = \"move_service\"\n        rospy.init_node(f\"{service_name}_server\") # (14)!\n\n        self.service = rospy.Service(service_name, SetBool, self.srv_callback) # (15)!\n        self.pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10) # (3)!\n\n        rospy.loginfo(f\"the '{service_name}' Server is ready to be called...\") # (16)!\n\n    def srv_callback(self, request_from_client): # (4)!\n        vel = Twist()\n        response_from_server = SetBoolResponse() # (5)!\n\n        if request_from_client.request_signal == True: # (6)!\n            print(f\"Server received a 'true' request and the robot will now move for 5 seconds...\") # (7)!\n\n            StartTime = rospy.get_rostime() # (8)!\n\n            vel.linear.x = 0.1\n            self.pub.publish(vel) # (9)!\n\n            rospy.loginfo('Published the velocity command to /cmd_vel')\n            while (rospy.get_rostime().secs - StartTime.secs) &lt; 5: # (10)!\n                continue\n\n            rospy.loginfo('5 seconds have elapsed, stopping the robot...')\n\n            vel.linear.x = 0.0\n            self.pub.publish(vel) # (11)!\n\n            response_from_server.response_signal = True # (12)!\n            response_from_server.response_message = \"Request complete.\"\n        else: # (13)!\n            response_from_server.response_signal = False\n            response_from_server.response_message = \"Nothing happened, set request_signal to 'true' next time.\"\n        return response_from_server\n\n    def main(self):\n        rospy.spin() # (17)!\n\nif __name__ == '__main__':\n    server = moveService()\n    server.main() \n</code></pre> <ol> <li> <p>As you should know by now, in order to develop any ROS node in Python we first need to import the <code>rospy</code> library so that we can interact with ROS. We're also going to be issuing velocity commands to the robot, so we need to import the <code>Twist</code> message from the correct message package as well.</p> </li> <li> <p>We also need to import the Service Message that we want to use for the service that we will set up. This service will use the <code>SetBool</code> service message from a custom <code>tuos_msgs</code> package that we've created for you.</p> <p>Here, we import two different things from the <code>tuos_msgs</code> package:</p> <ol> <li>A definition of the full service message: <code>SetBool</code>, which we need to use when we create the service later.</li> <li>The Response portion of the service message: <code>SetBoolResponse</code>, which we will use to issue a response to the service caller.</li> </ol> </li> <li> <p>Here, we set up a publisher to the <code>/cmd_vel</code> topic, so that we can publish velocity commands to the robot (using <code>Twist</code> messages). Hopefully this part is starting to become familiar to you by now!</p> </li> <li> <p>Here we define a callback function for the server called <code>srv_callback</code>. Any code within this function will be executed whenever the service is called.</p> <p>The function can take one input argument only, in this case we are calling it <code>request_from_client</code>. This is where the <code>rospy.Service</code> instance that we set up earlier will put the data that it obtains from a <code>/move_service</code> call, whenever a Request is made.</p> </li> <li> <p>We create an instance of the Response portion of the <code>SetBool</code> service message, which we will populate with data later on (based on the outcome of the actions that the service server performs).</p> </li> <li> <p>We then analyse the service Request data (this is the data that is passed to the Server node, whenever a call to the service is made by a caller, or client). We know how to access the data within the service request from using the <code>rossrv info</code> command, which provides us with the following information:</p> <pre><code>rossrv info tuos_msgs/SetBool:\n\nbool request_signal\n---\nbool response_signal\nstring response_message\n</code></pre> <p>The Request message will therefore contain a boolean value called <code>request_signal</code>, so we can call this value from the input to our callback function (which we called <code>request_from_client</code>). Using an <code>if</code> statement, we check if this value is <code>True</code> or <code>False</code>, and then define some actions for each situation accordingly...</p> </li> <li> <p>Print a status message to tell the Service caller that a <code>True</code> value has been received.</p> </li> <li> <p>Get the current ROS time.</p> </li> <li> <p>Set a linear velocity for the robot, publish this to the <code>/cmd_vel</code> topic using the publisher that we set up in the <code>__init__()</code> method earlier (<code>self.pub</code>).</p> </li> <li> <p>Here, we use a while loop to act as a 5-second timer (by keeping an eye on the current ROS time using <code>get_rostime()</code>). Once 5 seconds have elapsed, this while loop will end.</p> </li> <li> <p>Once the time has elapsed, we publish another velocity command to make the robot stop.</p> </li> <li> <p>Finally, we can format a service Response using the <code>SetBoolResponse</code> instance that we set up earlier (<code>response_from_server</code>). Again, we know the names of the attributes in the service response from the <code>rossrv info</code> command:</p> <pre><code>rossrv info tuos_msgs/SetBool:\n\nbool request_signal\n---\nbool response_signal\nstring response_message\n</code></pre> </li> <li> <p>If the value of the <code>request_from_client.request_signal</code> was actually found to be <code>False</code> by our <code>if</code> statement earlier, then we do nothing other than send a service response, to indicate that nothing has happened!</p> </li> <li> <p>Initialise our ROS Node with a name (<code>\"move_service_server\"</code>).</p> </li> <li> <p>Create a <code>rospy.Service</code> instance where we define:</p> <ol> <li>The name of the service that this node will launch (<code>service_name = \"move_service\"</code> at the beginning of the <code>__init__()</code> method).</li> <li>The full service message format that the service will use, in this case: <code>SetBool</code>, which we imported earlier.</li> <li>A callback function, in this case called <code>srv_callback</code>, which will define what we want this service Server to do once the service is called.</li> </ol> </li> <li> <p>Send some information to the terminal to indicate that the node has been launched successfully, and that the Service is ready to be called.</p> </li> <li> <p>The <code>rospy.spin()</code> function keeps our node running indefinitely (so that the callback function can continue to execute, whenever the service is called). </p> </li> </ol> <p></p> <p>Fill in the Blank!</p> <p>Which message package does the <code>Twist</code> message belong to?</p> <p> \u2190 Back to Part 4 - Exercise 1 </p>"},{"location":"com2009/assignment1/part4/scan_callback/","title":"A LaserScan Callback Function","text":"<p>Based on what you have learnt throughout this course so far, you should now be able to set up a subscriber within your code to subscribe to the <code>/scan</code> topic:</p> <pre><code>self.subscriber = rospy.Subscriber('/scan', LaserScan, self.callback)\n</code></pre> <p>The above assumes that you are using a Python Class structure in your code (which you really should be!), and we also assume that you have already imported the <code>LaserScan</code> message type correctly at the start of your code as well:</p> <pre><code>from sensor_msgs.msg import LaserScan\n</code></pre> <p>You could then develop a callback function (called <code>callback()</code> in this case) using the approach illustrated below. </p> <p>Note</p> <p>This example requires the <code>numpy</code> Python library, which you'll need to import at the start of your code in the following way: <pre><code>import numpy as np\n</code></pre></p> <p>The <code>callback()</code> could then be developed to obtain (for instance) a 40\u00b0 arc of <code>LaserScan</code> data ahead of the robot:</p> <pre><code>def callback(self, scan_data: LaserScan):\n    left_arc = scan_data.ranges[0:21]\n    right_arc = scan_data.ranges[-20:]\n    front_arc = np.array(left_arc[::-1] + right_arc[::-1])\n    self.min_distance = front_arc.min()\n\n    # Optional Extra:\n    arc_angles = np.arange(-20, 21)\n    self.object_angle = arc_angles[np.argmin(front_arc)]\n</code></pre> <p>The distance to the closest object in front of our robot is then available throughout our class, as an attribute called: <code>self.min_distance</code>.</p> <p>What we're doing here is illustrated in the figure below:</p> <p></p> <p>Let's talk through it too:</p> <ol> <li> <p>From the front of the robot, we obtain a 20\u00b0 arc of scan data either side of the x-axis:</p> <pre><code>left_arc = scan_data.ranges[0:21]\nright_arc = scan_data.ranges[-20:]\n</code></pre> </li> <li> <p>Then, we combine the <code>left_arc</code> and <code>right_arc</code> data arrays, flip them so that the data is arranged from left (-20\u00b0) to right (+20\u00b0), and then convert to a <code>numpy</code> array:</p> <pre><code>front_arc = np.array(left_arc[::-1] + right_arc[::-1])\n</code></pre> </li> <li> <p>Then, we obtain the minimum distance value within the <code>front_arc</code> array, to tell us the distance to the closest thing up ahead:    </p> <pre><code>self.min_distance = front_arc.min()\n</code></pre> </li> <li> <p>Optionally, we can also determine the angular position of the closest object up ahead too...</p> <ol> <li>Create another <code>numpy</code> array to represent the angles (in degrees) associated with each of the data-points in the <code>front_arc</code> array above:     <pre><code>arc_angles = np.arange(-20, 21)\n</code></pre></li> <li>Determine the angle at which the minimum distance value is located in front of the robot, using the <code>numpy.argmin()</code> method:     <pre><code>self.object_angle = arc_angles[np.argmin(front_arc)]\n</code></pre></li> </ol> </li> </ol> <p> \u2190 Back to Part 4 - Exercise 4 </p>"},{"location":"com2009/assignment1/part5/action_client/","title":"Part 5 Camera Sweep Action Client","text":"<p>Copy all the code below into your <code>move_client.py</code> file.  Then, review the code annotations to understand how it all works.</p> <p>(Oh, and DFTS!)</p> action_client.py<pre><code>#!/usr/bin/env python3\n\nimport rospy # (1)!\nimport actionlib # (2)!\n\nfrom tuos_msgs.msg import CameraSweepAction, CameraSweepGoal, CameraSweepFeedback # (3)!\n\nnode_name = \"camera_sweep_action_client\"\naction_server_name = \"/camera_sweep_action_server\"\n\ncaptured_images = 0\ndef feedback_callback(feedback_data: CameraSweepFeedback): # (4)!\n    global captured_images\n    captured_images = feedback_data.{BLANK}\n    print(f\"FEEDBACK: Current yaw: {feedback_data.current_angle:.1f} degrees. \"\n          f\"Image(s) captured so far: {captured_images}...\")\n\nrospy.init_node(node_name) # (5)!\n\nclient = actionlib.SimpleActionClient(action_server_name, \n            CameraSweepAction) # (6)!\nclient.wait_for_server() # (7)!\n\ngoal = CameraSweepGoal()\ngoal.sweep_angle = 0\ngoal.image_count = 0\n\nclient.send_goal(goal, feedback_cb=feedback_callback) # (8)!\n\nclient.wait_for_result() # (9)!\n\nprint(f\"RESULT:\") # (10)!\nprint(f\"  * Action State = {client.get_state()}\")\nprint(f\"  * {captured_images} images saved to {client.get_result()}\")\n</code></pre> <ol> <li> <p>As you know by now, in order to develop ROS nodes using Python we need to use the <code>rospy</code> library. </p> </li> <li> <p>If we want to work with ROS Actions, we also need to import <code>actionlib</code>.</p> </li> <li> <p>We know that the <code>/camera_sweep_action_server</code> uses <code>CameraSweepAction</code> messages from the <code>tuos_msgs</code> package, so we import the full message definition: <code>CameraSweepAction</code> as well as the <code>Goal</code> message (which we use to actually make a call to the server). </p> <p>As we now know, ROS Actions provide feedback, so we're also importing the <code>Feedback</code> part of the <code>CameraSweepAction</code> message into our client node as well, so that it can be kept up to date with what the Action Server is doing, in real-time.</p> </li> <li> <p>To process this feedback data, we need to define a feedback callback function, to handle the data whenever a new feedback message comes in.</p> <p>Here we're using Python's \"Type Annotations\" feature:</p> <p><pre><code>feedback_data: CameraSweepFeedback\n</code></pre> ... which informs the interpreter that the <code>feedback_data</code> that is received by the <code>feedback_callback</code> function will be of the <code>CameraSweepFeedback</code> type.</p> <p>All this really does is allow autocomplete functionality to work with in our editor (VS Code), whenever we want to pull an attribute from the <code>feedback_data</code> object.</p> </li> <li> <p>Standard practice when we construct ROS nodes: we must initialise them with a name, where the <code>node_name</code> variable was assigned earlier on in the code: </p> <pre><code>node_name = \"camera_sweep_action_client\"\n</code></pre> </li> <li> <p>Then, we connect to the action server using the <code>actionlib.SimpleActionClient()</code> method, provide this with the name of the server that we wish to connect to and the type of messages that are used on the server (in this case <code>CameraSweepAction</code> messages).</p> <p>(The <code>action_server_name</code> variable was also assigned earlier on in the code too: <code>action_server_name = \"/camera_sweep_action_server\"</code>)</p> </li> <li> <p>This makes the node wait until the action server is live on the ROS Network (if it isn't already). Execution of the code can't continue past this point until the Action Server is visible on the ROS Network.</p> </li> <li> <p>Once the server is available, we construct a goal message and send this to the action server, whilst also pointing it to the callback function that we defined earlier, which will be used to process the feedback messages.</p> <p>Tip</p> <p>Both goal parameters have been set to <code>0</code> (above), so you'll need to change these in order for the client to successfully make a call to the server!</p> </li> <li> <p>Then, we simply wait for the action to complete.</p> </li> <li> <p>Once it has completed, the server provides us with a result, so we simply print that (as well as the current state of the action) to the terminal. </p> </li> </ol> <p></p> <p>Fill in the Blank!</p> <p>Which attribute of the <code>feedback_data</code> object tells us how many images have been captured over the course of the Camera Sweep Action? There are a number of ways we can work this out:</p> <ol> <li>You could use the same approach as we used earlier. </li> <li>You could run <code>rosmsg info tuos_msgs/CameraSweepFeedback</code> in a terminal.</li> <li>You could use the autocomplete/variable suggestions provided in VS Code!</li> </ol> <p> \u2190 Back to Part 5 - Exercise 2 </p>"},{"location":"com2009/assignment1/part5/preemptive_action_client/","title":"Part 5 Preemptive Action Client","text":"<p>Copy all the code below into your <code>preemptive_action_client.py</code> file and then review the annotations!</p> preemptive_action_client.py<pre><code>#!/usr/bin/env python3\n\nimport rospy\nimport actionlib\n\nfrom tuos_msgs.msg import CameraSweepAction, CameraSweepGoal, CameraSweepFeedback\n\nclass PreemptiveActionClient(): # (1)!\n    goal = CameraSweepGoal() # (2)!\n\n    def feedback_callback(self, feedback_data: CameraSweepFeedback): # (3)!\n        self.captured_images = feedback_data.current_image\n        print(f\"FEEDBACK: Current yaw: {feedback_data.current_angle:.1f} degrees. \"\n              f\"Image(s) captured so far: {self.captured_images}...\")\n\n    def __init__(self): # (4)!\n        self.captured_images = 0\n        self.action_complete = False\n\n        node_name = \"preemptive_camera_sweep_action_client\"\n        action_server_name = \"/camera_sweep_action_server\"\n\n        rospy.init_node(node_name)\n\n        self.rate = rospy.Rate(1)\n\n        self.client = actionlib.SimpleActionClient(action_server_name, \n                    CameraSweepAction)\n        self.client.wait_for_server()\n\n        rospy.on_shutdown(self.shutdownhook)\n\n    def shutdownhook(self): # (5)!\n        if not self.action_complete:\n            rospy.logwarn(\"Received a shutdown request. Cancelling Goal...\")\n            self.client.cancel_goal()\n            rospy.logwarn(\"Goal Cancelled...\")\n\n        # get the result: (6) \n        rospy.sleep(1) # wait for the result to come in\n        print(\"RESULT:\")\n        print(f\"  * Action State = {self.client.get_state()}\")\n        print(f\"  * {self.captured_images} image(s) saved to {self.client.get_result()}\")\n\n    def send_goal(self, images, angle): # (7)!\n        self.goal.sweep_angle = angle\n        self.goal.image_count = images\n\n        # send the goal to the action server:\n        self.client.send_goal(self.goal, feedback_cb=self.feedback_callback)        \n\n    def main(self):\n        self.send_goal(images = 0, angle = 0) # (8)!\n        i = 1 # (9)!\n        print(\"While we're waiting, let's do our seven-times tables...\")\n        while self.client.get_state() &lt; 2:\n            print(f\"STATE: Current state code is {self.client.get_state()}\")\n            print(f\"TIMES TABLES: {i} times 7 is {i*7}\")\n            i += 1\n            self.rate.sleep()\n        self.action_complete = True # (10)!\n\nif __name__ == '__main__':\n    {BLANK} # (11)!\n</code></pre> <ol> <li> <p>Everything is now contained within a Python Class.</p> </li> <li> <p>Instantiate a goal message object, which we'll use later to call the Camera Sweep action.</p> </li> <li> <p>The feedback callback function is exactly the same as the one in the <code>action_client.py</code> from Exercise 2. Because we're working inside a Python Class now, we can make the <code>captured_images</code> variable available beyond the scope of this function by using the <code>self</code> prefix (previously this was achieved using the <code>global</code> statement).</p> </li> <li> <p>Classes require an <code>__init__()</code> method, which will be executed as soon as the class is instantiated. Here, we do all our initialisations:</p> <ul> <li>Initialise some variables (<code>captured_images</code>, <code>action_complete</code>) and make them available throughout the class by prefixing with <code>self</code>.</li> <li>Initialise the node (with a name).</li> <li>Set a rate (1 Hz).</li> <li>Create a connection to the action server and wait for it to become available.</li> <li>Specify a function to be executed when the node is stopped (<code>shutdown_ops()</code>).</li> </ul> <p>... none of this should be new to you now!</p> </li> <li> <p>The actual shutdown operations are defined here. This is how we make sure that the current goal is cancelled (using <code>cancel_goal()</code>), so that the action doesn't keep on running if this node is stopped prematurely (before the action has completed).  This function will also execute when the action server completes successfully, so we use an <code>action_complete</code> flag to check whether this is the case (i.e. to avoid trying to cancel the goal if it's already finished!):</p> </li> <li> <p>This bit will execute regardless of whether the action completed successfully or was preempted. Here, we're getting three things and printing them to the terminal:</p> <ol> <li>The result from the action server.</li> <li>The action state (using the <code>get_state()</code> method).</li> <li>The final number of captured images (obtained from the last feedback message that was issued before the action stopped).</li> </ol> </li> <li> <p>The way the goal is defined and issued to the server is exactly the same as before, except this time it's done within this class method, so that it can be called from <code>main()</code>.</p> </li> <li> <p>Call the goal.</p> </li> <li> <p>All this is the same as before (<code>action_client.py</code>), i.e. monitor the state of the action with a <code>while</code> loop and do some concurrent operations (seven-times tables again!).</p> </li> <li> <p>The only difference is that we set this flag to <code>True</code> if the action manages to complete successfully.</p> </li> <li> <p>Fill in the Blank!</p> </li> </ol> <p>Fill in the Blank!</p> <p>We have contained all our code inside a nice Python Class now, but how do we actually instantiate it and invoke the Action Call? (We've been doing this from the very beginning, and the process is very much the same here!)</p> <p> \u2190 Back to Part 5 - Exercise 3 </p>"},{"location":"com2009/assignment1/part5/search_client/","title":"Part 5 Search Client Template","text":"search_client.py<pre><code>#! /usr/bin/env python3\n# search_client.py\n\nimport rospy\nimport actionlib\n\nfrom tuos_msgs.msg import SearchAction, SearchGoal, SearchFeedback\n\nclass SearchActionClient():\n    goal = SearchGoal()\n\n    def feedback_callback(self, feedback_data: SearchFeedback):\n        ## TODO: get the current distance travelled, from the feedback message\n        ## and assign this to a class variable...\n        self.distance = ...\n\n\n\n    def __init__(self):\n        self.distance = 0.0\n\n        self.action_complete = False\n        rospy.init_node(\"search_action_client\")\n        self.rate = rospy.Rate(1)\n\n        ## TODO: setup a \"simple action client\" with a callback function\n        ## and wait for the server to be available...\n        self.client = ...\n\n\n\n        rospy.on_shutdown(self.shutdownhook)\n\n    def shutdownhook(self):\n        if not self.action_complete:\n            rospy.logwarn(\"Received a shutdown request. Cancelling Goal...\")\n            ## TODO: cancel the goal request, if this node is shutdown before the action has completed...\n\n\n\n            rospy.logwarn(\"Goal Cancelled...\")\n\n        ## TODO: Print the result here...\n\n\n\n    def main(self):\n        ## TODO: assign values to all goal parameters\n        ## and send the goal to the action server...\n        self.goal...\n\n\n\n        while self.client.get_state() &lt; 2:\n            ## TODO: Construct an if statement and cancel the goal if the \n            ## distance travelled exceeds 2 meters...\n            if self.distance ...\n\n\n                # break out of the while loop to stop the node:\n                break\n\n            self.rate.sleep()\n\n        self.action_complete = True\n\nif __name__ == '__main__':\n    ## TODO: Instantiate the node and call the main() method from it...\n</code></pre> <p> \u2190 Back to Part 5 - Exercise 4 </p>"},{"location":"com2009/assignment1/part5/search_server/","title":"Part 5 Search Server Template","text":"search_server.py Template<pre><code>#! /usr/bin/env python3\n# search_server.py\n\n# Import the core Python modules for ROS and to implement ROS Actions:\nimport rospy\nimport actionlib\n\n# Import all the necessary ROS message types:\nfrom tuos_msgs.msg import SearchAction, SearchFeedback, SearchResult, SearchGoal\n\n# Import the tb3 modules from tb3.py\nfrom tb3 import Tb3Move, Tb3Odometry, Tb3LaserScan\n\n# Import some other useful Python Modules\nfrom math import sqrt, pow\n\nclass SearchActionServer():\n    feedback = SearchFeedback() \n    result = SearchResult()\n\n    def __init__(self):\n        rospy.init_node(\"search_action_server\")\n\n        ## TODO: create a \"simple action server\" with a callback function, and start it...\n        self.actionserver = actionlib.SimpleActionServer(...)\n\n\n\n\n        # pull in some useful publisher/subscriber functions from the tb3.py module:\n        self.vel_controller = Tb3Move()\n        self.tb3_odom = Tb3Odometry()\n        self.tb3_lidar = Tb3LaserScan()\n\n        rospy.loginfo(\"The 'Search Action Server' is active...\")\n\n    # The action's \"callback function\":\n    def action_server_launcher(self, goal: SearchGoal):\n        rate = rospy.Rate(10)\n\n        ## TODO: Implement some checks on the \"goal\" input parameter(s)\n        success = True\n        if goal...\n\n\n            success = False\n\n\n        if not success:\n            ## TODO: abort the action server if an invalid goal has been requested...\n\n\n\n            return\n\n        ## TODO: Print a message to indicate that the requested goal was valid\n        print(f\"...\")\n\n        # Get the robot's current odometry from the Tb3Odometry() class:\n        self.posx0 = self.tb3_odom.posx\n        self.posy0 = self.tb3_odom.posy\n        # Get information about objects up ahead from the Tb3LaserScan() class:\n        self.closest_object = self.tb3_lidar.min_distance\n        self.closest_object_location = self.tb3_lidar.closest_object_position\n\n        ## TODO: set the robot's forward velocity (as specified in the \"goal\")...\n        self.vel_controller...\n\n        ## TODO: establish a conditional statement so that the  \n        ## while loop continues as long as the distance to the closest object\n        ## ahead of the robot is always greater than the \"approach distance\"\n        ## (as specified in the \"goal\")...\n        while {something} &gt; {something_else}:\n            # update LaserScan data:\n            self.closest_object = self.tb3_lidar.min_distance\n            self.closest_object_location = self.tb3_lidar.closest_object_position\n\n            ## TODO: publish a velocity command to make the robot start moving \n            self.vel_controller...\n\n            # check if there has been a request to cancel the action mid-way through:\n            if self.actionserver.is_preempt_requested():\n                ## TODO: take appropriate action if the action is cancelled (pre-empted)...\n\n\n\n                success = False\n                # exit the loop:\n                break\n\n            # determine how far the robot has travelled so far:\n            self.distance = sqrt(pow(self.posx0 - self.tb3_odom.posx, 2) + pow(self.posy0 - self.tb3_odom.posy, 2))\n\n            ## TODO: update all feedback message values and publish a feedback message:\n            self.feedback...\n\n\n\n            ## TODO: update all result parameters:\n            self.result...\n\n\n\n            rate.sleep()\n\n        if success:\n            rospy.loginfo(\"approach completed successfully.\")\n            ## TODO: Set the action server to \"succeeded\" and stop the robot...\n\n    def main(self):\n        rospy.spin()\n\nif __name__ == '__main__':\n    node = SearchActionServer()\n    node.main()\n</code></pre> <p>Important</p> <p>The template above uses the <code>tb3.py</code> module from the <code>tuos_examples</code> package, which contains various helper functions to make the robot move and to read data (a.k.a. subscribe) from some key topics that will be useful for the task at hand. To use this, you'll need to copy the module across to your own <code>part5_actions/src</code> folder so that your <code>search_server.py</code> node can import it. In TERMINAL 2, copy the <code>.py</code> file as follows:</p> <pre><code>cp ~/catkin_ws/src/tuos_ros/tuos_examples/src/tb3.py ~/catkin_ws/src/part5_actions/src/\n</code></pre> <p> \u2190 Back to Part 5 - Exercise 4 </p>"},{"location":"com2009/assignment1/part6/line_follower/","title":"Part 6 Line Following (Setup)","text":"<p>Use this code as a starting point for Part A of the Line Following exercise.</p> line_follower.py<pre><code>#!/usr/bin/env python3\n\nimport rospy\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom sensor_msgs.msg import Image\n\nfrom tb3 import Tb3Move\n\nclass LineFollower():\n    def __init__(self):\n        node_name = \"line_follower\"\n        rospy.init_node(node_name, anonymous=True)\n        self.rate = rospy.Rate(5)\n\n        self.cvbridge_interface = CvBridge()\n        self.img_sub = rospy.Subscriber(\n            \"/camera/rgb/image_raw\", Image, self.camera_cb)\n        self.robot_controller = Tb3Move()\n\n        self.ctrl_c = False\n        rospy.on_shutdown(self.shutdown_ops)\n\n    def shutdown_ops(self):\n        self.robot_controller.stop()\n        cv2.destroyAllWindows()\n        self.ctrl_c = True\n\n    def camera_cb(self, img_data):\n        try:\n            cv_img = self.cvbridge_interface.imgmsg_to_cv2(\n                img_data, desired_encoding=\"bgr8\")\n        except CvBridgeError as e:\n            print(e)\n\n        cv2.imshow(\"camera image\", cv_img)\n\n        height, width, _ = cv_img.shape\n        ## TODO 1 (1)\n\n        ## TODO 2 (2)\n\n        ## TODO 3 (3)\n\n        cv2.waitKey(1)\n\n    def main(self):\n        while not self.ctrl_c:\n            self.rate.sleep()\n\nif __name__ == '__main__':\n    node = LineFollower()\n    try:\n        node.main()\n    except rospy.ROSInterruptException:\n        pass\n</code></pre> <ol> <li> <p>Image Cropping </p> <p>Apply some cropping to the raw camera image (<code>cv_img</code>). </p> <p>Crop it to around 1/5 of its original height, and to a width so that the pink line is just visible at the edge of the image. </p> <p>Call your new cropped image something like <code>cropped_img</code>. You could then use the <code>cv2.imshow()</code> method to display this in an additional pop-up window when the node is run: </p> <pre><code>cv2.imshow(\"cropped_image\", cropped_img)\n</code></pre> <p> </p> </li> <li> <p>Colour Detection</p> <p>Filter the cropped image by selecting appropriate HSV values so that the pink line can be isolated from the rest of the image.</p> <p>You may need to use the <code>tuos_examples\\image_colours.py</code> node again to help you identify the correct Hue and Saturation value range.</p> <p>Use <code>cv2.cvtColor()</code> to convert your <code>cropped_img</code> into an HSV colour representation:</p> <pre><code>hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\n</code></pre> <p>Use <code>cv2.inRange()</code> to create a mask with the HSV value range that you have determined:</p> <pre><code>line_mask = cv2.inRange(\n    hsv_img, {lower_hsv_values}, {upper_hsv_values}\n)\n</code></pre> <p>And then use <code>cv2.bitwise_and()</code> to create a new image with the mask applied, so that the coloured line is isolated:</p> <pre><code>line_isolated = cv2.bitwise_and(\n    cropped_img, cropped_img, mask = line_mask\n)\n</code></pre> <p> </p> </li> <li> <p>Locating the line</p> <p>Finally, find the horizontal position of the line in the robot's viewpoint.</p> <p>Calculate the image moments of the pink colour blob that represents the line (<code>line_mask</code>) using the <code>cv2.moments()</code> method. Remember that it's the \\(c_{y}\\) component that we're interested in here:</p> \\[ c_{y}=\\dfrac{M_{10}}{M_{00}} \\] <p>Ultimately, this will provide us with the feedback signal that we can use for a proportional controller that we will implement in the next part of the exercise.</p> <p>Once you've obtained the image moments (and <code>cy</code>), use <code>cv2.circle()</code> to mark the centroid of the line on the filtered image (<code>line_isolated</code>) with a circle. For this, you'll also need to calculate the \\(c_{z}\\) component of the centroid:</p> \\[ c_{z}=\\dfrac{M_{01}}{M_{00}} \\] <p>Remember that once you've done all this you can display the filtered image of the isolated line (with the circle to denote the centroid location) using <code>cv2.imshow()</code> again:</p> <pre><code>cv2.imshow(\"filtered line\", line_isolated)\n</code></pre> <p> </p> </li> </ol> <p> \u2190 Back to Part 6 - Exercise 4 (Part A) </p>"},{"location":"com2009/assignment1/part6/object_detection/","title":"Part 6 Object Detection Node","text":"<p>Copy all the code below into your <code>object_detection.py</code> file, and make sure you read the annotations!</p> <p>.. oh, and I'm sure I don't need to say it by now, but... DFTS!</p> object_detection.py<pre><code>#!/usr/bin/env python3\n\nimport rospy\nfrom pathlib import Path # (1)!\n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError # (2)!\n\nfrom sensor_msgs.msg import Image # (3)!\n\n# Initialisations: (4)\nnode_name = \"object_detection\"\nrospy.init_node(node_name)\nprint(f\"Launched the '{node_name}' node. Currently waiting for an image...\")\nrate = rospy.Rate(5)\n\nbase_image_path = Path.home().joinpath(\"myrosdata/object_detection/\")\nbase_image_path.mkdir(parents=True, exist_ok=True) # (5)!\n\ncvbridge_interface = CvBridge() # (6)!\n\nwaiting_for_image = True # (7)!\n\ndef show_and_save_image(img, img_name): # (8)!\n    full_image_path = base_image_path.joinpath(f\"{img_name}.jpg\") # (9)!\n\n    print(\"Opening the image in a new window...\")\n    cv2.imshow(img_name, img) # (10)!\n    print(f\"Saving the image to '{full_image_path}'...\")\n    cv2.imwrite(str(full_image_path), img) # (11)!\n    print(f\"Saved an image to '{full_image_path}'\\n\"\n        f\"image dims = {img.shape[0]}x{img.shape[1]}px\\n\"\n        f\"file size = {full_image_path.stat().st_size} bytes\") # (12)!\n    print(\"**IMPORTANT: Close the image pop-up window to continue!**\")\n    cv2.waitKey(0) # (13)!\n\ndef camera_cb(img_data): # (14)!\n    global waiting_for_image # (15)!\n    try:\n        cv_img = cvbridge_interface.imgmsg_to_cv2(img_data, desired_encoding=\"bgr8\") # (16)!\n    except CvBridgeError as e:\n        print(e)\n\n    if waiting_for_image == True: # (17)!\n        height, width, channels = cv_img.shape\n\n        print(f\"Obtained an image of height {height}px and width {width}px.\")\n\n        show_and_save_image(cv_img, img_name = \"step1_original\")\n\n        waiting_for_image = False\n\nrospy.Subscriber(\"/camera/rgb/image_raw\", Image, camera_cb) # (18)!\n\nwhile waiting_for_image: # (19)!\n    rate.sleep()\n\ncv2.destroyAllWindows() # (20)!\n</code></pre> <ol> <li> <p>Of course, we always need to import <code>rospy</code> so that Python can work with ROS. What we're also importing here is the Python <code>Path</code> class from the <code>pathlib</code> module, which will be used to do a few file operations.</p> </li> <li> <p>Then, we're importing the OpenCV library for Python (remember the Python API that we talked about earlier), which is called <code>cv2</code>, and also that ROS-to-OpenCV bridge interface that we talked about earlier too: <code>cv_bridge</code>.</p> <p>From <code>cv_bridge</code> we're importing the <code>CvBridge</code> and <code>CvBridgeError</code> classes from the <code>cv_bridge</code> library specifically.</p> </li> <li> <p>We need to subscribe to an image topic in order to obtain the messages being published to it. You should've already identified the type of message that is published to the <code>/camera/rgb/image_raw</code> topic, so we import that message type here (from the <code>sensor_msgs</code> package) so that we can build a subscriber to the topic later.</p> </li> <li> <p>Next, we're doing a number of initialisations that should be very familiar to you by now:</p> <ol> <li>Giving our node a name.</li> <li>Initialising the node (i.e. registering it on the ROS network using <code>rospy.init_node()</code>).</li> <li>Specifying a rate at which we want the node to run.</li> </ol> </li> <li> <p>Then, we define a filesystem location that we'll use to save images to. We want this to exist in a folder called \"myrosdata/object_detection\" in the home directory, so we can use Pathlib's <code>Path.home().joinpath(...)</code> to define it (a handy way to access the User's home directory, without needing to know the Users name). Then, we use the Pathlib <code>Path.mkdir()</code> method to create this directory if it doesn't exist already.</p> </li> <li> <p>Here, we create an instance of the <code>CvBridge</code> class that we imported earlier, and which we'll use later on to convert ROS image data into a format that OpenCV can understand.</p> </li> <li> <p>We're creating a flag to indicate whether the node has obtained an image yet or not. For this exercise, we only want to obtain a single image, so we will set the <code>waiting_for_image</code> flag to <code>False</code> in our camera callback function once an image has been obtained, to avoid capturing any more.</p> </li> <li> <p>This function defines some image operations that we will need to repeat multiple times (this will become apparent later). The further annotations explain more about what's going on inside this function...</p> </li> <li> <p>Construct a full file path for an image (using the <code>Path.joinpath()</code> method) from:</p> <ol> <li>The <code>base_image_path</code> that we defined earlier and </li> <li> <p>An image name that is passed into this function via the <code>img_name</code> argument.</p> <p>We'll use this to save the file to our filesystem later on.</p> </li> </ol> </li> <li> <p>Display the actual image in a pop-up window:</p> <ol> <li>The image data is passed into the function via the <code>img</code> argument,</li> <li>We need to give the pop-up window a name, so in this case we are using the <code>img_name</code> argument that has also been passed into the function.</li> </ol> </li> <li> <p>This saves the image to a <code>.jpg</code> file.  We're supplying the <code>full_image_path</code> that was created above, and also the actual image data (<code>img</code>) so that the function knows what image we want to save.</p> </li> <li> <p>We're printing a message to the terminal to inform us of (a) where the image has been saved to, (b) how big the image was (in terms of its pixel dimensions) and (c) how big the image file is (in bytes).</p> </li> <li> <p>We're supplying a value of <code>0</code> here, which tells this function to wait indefinitely before allowing our <code>show_and_save_image()</code> function to end. If we had supplied a value here (say: <code>1</code>) then the function would simply wait 1 millisecond and then close the pop-up window down. In our case however, we want some time to actually look at the image and then close the window down ourselves, manually. Once the window has been closed, the execution of our code is able to continue...</p> </li> <li> <p>Here, we're defining a callback function for a <code>rospy.Subscriber()</code>...</p> </li> <li> <p>We want to make changes to the <code>waiting_for_image</code> flag inside this function, but make sure that these changes are also observed outside the function too (i.e. by the <code>while</code> loop that we talked about above).  So, we change the scope of the variable to <code>global</code>.</p> </li> <li> <p>We're using the CvBridge interface to take our ROS image data and convert it to a format that OpenCV will be able to understand.  In this case we are specifying conversion (or \"encoding\") to an 8-bit BGR (Blue-Green-Red) image format: <code>\"bgr8\"</code>.</p> <p>We contain this within a <code>try-except</code> block though, which is the recommended procedure when doing this.  Here we try to convert an image using the desired encoding, and if a <code>CvBridgeError</code> is raised then we print this error to the terminal.  Should this happen, this particular execution of the camera callback function will stop.</p> </li> <li> <p>Then we check the <code>waiting_for_image</code> flag to see if this is the first image that has been received by the node.  If so, then:</p> <ol> <li>Obtain the height and width of the image (in pixels), as well as the number of colour channels.</li> <li>Print the image dimensions to the terminal.</li> <li>Pass the image data to the <code>show_and_save_image()</code> function (as discussed earlier). We also pass a descriptive name for the image to this function too (<code>img_name</code>).</li> <li>Finally, we set the <code>waiting_for_image</code> flag to <code>False</code> so that we only ever perform these processing steps once (we only want to capture one image remember!).  This will then trigger the main <code>while</code> loop to stop, thus causing the overall execution of the node to stop too.</li> </ol> </li> <li> <p>Create subscriber to the <code>/camera/rgb/image_raw</code> topic, telling the <code>rospy.Subscriber()</code> function the message type that is used by this topic (<code>sensor_msgs/Image</code> - as imported above), and we point it to a callback function (<code>camera_cb</code>, in this case), to define the processes that should be performed every time a message is obtained on this topic (in this case, the messages will be our camera images)</p> </li> <li> <p>Go into a <code>while</code> loop, and use the <code>rate.sleep()</code> method to maintain this loop at a speed of 5 Hz (as defined earlier) whilst checking the <code>waiting_for_image</code> flag to see if an image has been obtained by our subscriber yet.  We only really want to obtain a single image here, so once the <code>waiting_for_image</code> flag changes to <code>False</code>, the <code>while</code> loop will stop.</p> </li> <li> <p>Finally, <code>cv2.destroyAllWindows()</code> ensures that any OpenCV image pop-up windows that may still be active or in memory are destroyed before the node shuts down. </p> </li> </ol> <p> \u2190 Back to Part 6 - Exercise 2 </p>"},{"location":"com2009/assignment1/part6/object_detection_complete/","title":"Part 6 Object Detection Node (Complete)","text":"<p>Here's a full example of the <code>object_detection.py</code> node that you should have developed during Part 6 Exercise 2.  Also included here is an illustration of how to use the <code>cv2.circle()</code> method to create a marker on an image illustrating the centroid of the detected feature, as discussed here.</p> object_detection_complete.py<pre><code>#!/usr/bin/env python3\n\nimport rospy\nfrom pathlib import Path\n\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\n\nfrom sensor_msgs.msg import Image\n\nnode_name = \"object_detection\"\nrospy.init_node(node_name)\nprint(f\"Launched the '{node_name}' node. Currently waiting for an image...\")\nrate = rospy.Rate(5)\n\nbase_image_path = Path.home().joinpath(\"myrosdata/object_detection/\")\nbase_image_path.mkdir(parents=True, exist_ok=True)\n\ncvbridge_interface = CvBridge()\n\nwaiting_for_image = True\n\ndef show_and_save_image(img, img_name):\n    full_image_path = base_image_path.joinpath(f\"{img_name}.jpg\")\n\n    print(\"Opening the image in a new window...\")\n    cv2.imshow(img_name, img)\n    print(f\"Saving the image to '{full_image_path}'...\")\n    cv2.imwrite(str(full_image_path), img)\n    print(f\"Saved an image to '{full_image_path}'\\n\"\n        f\"image dims = {img.shape[0]}x{img.shape[1]}px\\n\"\n        f\"file size = {full_image_path.stat().st_size} bytes\")\n    print(\"**IMPORTANT: Close the image pop-up window to continue!**\")\n    cv2.waitKey(0)\n\ndef camera_cb(img_data):\n    global waiting_for_image  \n    try:\n        cv_img = cvbridge_interface.imgmsg_to_cv2(img_data, desired_encoding=\"bgr8\")\n    except CvBridgeError as e:\n        print(e)\n\n    if waiting_for_image == True:\n        height, width, channels = cv_img.shape\n\n        print(f\"Obtained an image of height {height}px and width {width}px.\")\n\n        show_and_save_image(cv_img, img_name = \"step1_original\")\n\n        crop_width = width - 400\n        crop_height = 400\n        crop_y0 = int((width / 2) - (crop_width / 2))\n        crop_z0 = int((height / 2) - (crop_height / 2))\n        cropped_img = cv_img[crop_z0:crop_z0+crop_height, crop_y0:crop_y0+crop_width]\n\n        show_and_save_image(cropped_img, img_name = \"step2_cropping\")\n\n        hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\n        lower_threshold = (115, 225, 100)\n        upper_threshold = (130, 255, 255)\n        img_mask = cv2.inRange(hsv_img, lower_threshold, upper_threshold)\n\n        show_and_save_image(img_mask, img_name = \"step3_image_mask\")\n\n        filtered_img = cv2.bitwise_and(cropped_img, cropped_img, mask = img_mask)\n\n        # FINDING THE IMAGE CENTROID: (1)\n        m = cv2.moments(img_mask) # (2)!\n        cy = m['m10'] / (m['m00'] + 1e-5)\n        cz = m['m01'] / (m['m00'] + 1e-5) # (3)!\n        cv2.circle(\n            filtered_img, \n            (int(cy), int(cz)), \n            10, (0, 0, 255), 2) # (4)!\n\n        show_and_save_image(filtered_img, img_name = \"step4_filtered_image\")\n\n        waiting_for_image = False\n\nrospy.Subscriber(\"/camera/rgb/image_raw\", Image, camera_cb)\n\nwhile waiting_for_image:\n    rate.sleep()\n\ncv2.destroyAllWindows()\n</code></pre> <ol> <li> <p>Everything here should be familiar to you from earlier in this exercise, except for this section...</p> </li> <li> <p>Here, we obtain the moments of our colour blob by providing the boolean representation of it (i.e. the <code>img_mask</code>) to the <code>cv2.moments()</code> function.</p> </li> <li> <p>Then, we are determining where the central point of this colour blob is located by calculating the <code>cy</code> and <code>cz</code> coordinates of it.  This provides us with pixel coordinates relative to the top left-hand corner of the image.</p> </li> <li> <p>Finally, this function allows us to draw a circle on our image at the centroid location so that we can visualise it.  Into this function we pass:</p> <ol> <li>The image that we want the circle to be drawn on.  In this case: <code>filtered_img</code>.</li> <li>The location that we want the circle to be placed, specifying the horizontal and vertical pixel coordinates respectively: <code>(int(cy), int(cz))</code>.</li> <li>How big we want the circle to be: here we specify a radius of 10 pixels.</li> <li>The colour of the circle, specifying this using a Blue-Green-Red colour space: <code>(0, 0, 255)</code> (i.e.: pure red in this case)</li> <li>Finally, the thickness of the line that will be used to draw the circle, in pixels.</li> </ol> </li> </ol> <p> \u2190 Back to Part 6 - Exercise 2 </p>"},{"location":"com2009/assignment2/","title":"Assignment #2: Team Robotics Project","text":""},{"location":"com2009/assignment2/#overview","title":"Overview","text":"<p>In Assignment #2 you will put into practice everything that you learn about ROS in Assignment #1, and explore the capabilities of the framework further.</p> <p>You will attend a 2-hour lab session per week in Diamond Computer Room 5 for the full 12-week semester. You will work in teams to develop ROS Nodes for our TurtleBot3 Waffles that allow them to successfully complete a series of robotics tasks, most of which in a real-world environment. There are four tasks to complete in total, all of which will be assessed as part of Assignment #2.</p>"},{"location":"com2009/assignment2/#the-tasks","title":"The Tasks","text":"<p>There are four tasks in total that you must complete for this Assignment. Each task is marked and goes towards your final mark for Assignment #2. Tasks 1, 2 &amp; 4 will all be assessed on a real TurtleBot3 Waffle in the robot arena in Computer Room 5, Task 3 will be assessed in simulation. All will be marked based on how well the robot completes each of the tasks. An overview of the tasks and the marking breakdown is shown in the table below. </p> <p> Task Details Assessment Format Marks 1 Velocity Control Real World 10/100 2 Avoiding Obstacles Real World 20/100 An 'out-of-the-box' submission for Part A - 10/100 3 Maze Navigation Simulation 20/100 4 Exploration &amp; Search Real World 40/100 <p></p> <p>As shown above, there are 100 marks available in total for Assignment #2.</p>"},{"location":"com2009/assignment2/#assessment","title":"Assessment","text":"<p>This assignment is worth 30% of the overall mark for the COM2009 course. As a team you will be assessed on a ROS package that you develop to satisfy the above tasks.</p> <p>For Tasks 1, 2 &amp; 4: Each submission will be assessed by extracting your ROS package on one of the robotics laptops that you will use extensively in the labs throughout the assignment. Nodes within your package will then be executed on the laptop to control a real robot in the Diamond Computer Room 5 Robot Arena.</p> <p>For Task 3: Each submission will be extracted into the WSL-ROS environment running on one of the WSL-ROS laptops (also available during the lab sessions). Nodes within your package will then be executed to control a TurtleBot3 Waffle in a simulated environment.</p> <p>Feedback: Within 3 weeks of submission you will receive your marks and a recording of the assessment so that you can see how well your robot performed in each of the tasks, and so that you can see exactly how your marks were awarded!</p>"},{"location":"com2009/assignment2/#submission-deadlines","title":"Submission Deadlines","text":"<p>The assignment is essentially split into two separate parts (A &amp; B), with two submission deadlines as summarised in the table below (see Blackboard for exact dates and times).</p> <p> Part Task(s) Deadline A 1 &amp; 2 Week 6 B 3 &amp; 4 Week 12 <p></p> <p>For each submission, you'll need to upload a ROS package (as a <code>.tar</code> file) to a submission portal on Blackboard. </p> <p>Before you get started on any of the programming tasks you should (as a team) create a single ROS package (further details on the next page). You can then add all the necessary functionality for each task as you go along. For each submission, you'll then need to create a copy of your package in its current state by creating a <code>.tar</code> archive of it, and submit this to Blackboard by the specified deadline (the export process is explained here). </p> <p>Note</p> <p>You should work on each task as a team, and you only need to make one submission per team for each task.</p>"},{"location":"com2009/assignment2/#your-ros-package","title":"Your ROS Package","text":""},{"location":"com2009/assignment2/#launching-your-code","title":"Launching Your Code","text":"<p>In order to launch the necessary functionality within your package for a given task you will need to include correctly named launch files: <code>task1.launch</code>, <code>task2.launch</code>, etc. This will allow you to ensure that all the required functionality is executed when your submission is assessed, and also ensures that we know exactly how to launch this functionality in order to assess it. Full details of the requirements for each launch file are provided on the associated task page.</p> <p>Warning</p> <p>It's up to you to ensure that your code launches as intended for a given task. If it doesn't, then you'll be awarded zero marks, so make sure you test it all out prior to submission!</p> <p>For more information on how to create <code>.launch</code> files, refer to the following resources:</p> <ol> <li>Assignment #1, Part 1, Exercise 8: Creating a Launch File</li> <li>The ROS Extras: Launch Files section of this course site </li> </ol>"},{"location":"com2009/assignment2/#key-requirements","title":"Key Requirements","text":"<p>In order to be awarded any marks for any task outlined in the table above, you must ensure that the following key requirements are met in regard to the ROS package that you submit (as well as any additional requirements specific to a given task):</p> <ol> <li> <p>Your package must be submitted to Blackboard as a <code>.tar</code> file with the following naming convention:</p> <pre><code>com2009_team{}.tar\n</code></pre> <p>Where the <code>{}</code> is replaced with your own team number. See here for how to create a <code>.tar</code> archive of your package.</p> </li> <li> <p>Your ROS package directory, when extracted, must be named:</p> <pre><code>com2009_team{}/\n</code></pre> <p>Again, replacing the <code>{}</code> with your own team number!</p> </li> <li> <p>Your ROS package name must also be the same, so that the following would work (for example):</p> <pre><code>roslaunch com2009_team100 task1.launch\n</code></pre> <p>(assuming you are Team 100!)</p> </li> <li> <p>Finally (and most importantly), your ROS package must work \"out-of-the-box,\" i.e. the Teaching Team won't make any modifications or fix any errors for you! </p> </li> </ol> <p>Warning</p> <p>Failure to follow these requirements could result in you being awarded zero marks!</p>"},{"location":"com2009/assignment2/getting-started/","title":"Getting Started in Week 1","text":"<p>Before you get started on Assignment #2 (as detailed in the pages that follow), you should work through the following tasks in your teams during the first lab in Week 1. </p>"},{"location":"com2009/assignment2/getting-started/#set-up-your-teams-ros-package","title":"Set Up Your Team's ROS Package","text":"<p>As discussed on the Assignment #2 Overview, everything that your team submit for this lab assignment must be contained within a single ROS package. Inside this you will develop all the necessary nodes to make a TurtleBot3 Waffle (real or simulated) complete each of the assignment tasks. Each task will be assessed by the Teaching Team via launch files that you must also provide within your package. You'll therefore need to create one launch file per task (we'll talk more about this later).</p> <p>The first step however is to create your team's ROS Package.</p> <p>Only one member of your team needs to actually do this bit, and it's best to do this from within your own ROS installation (or WSL-ROS), rather than on the robotics laptop that you will use to work with the real robots in the lab. Select a team member who has access to their own ROS installation in the lab now (i.e. via a personal laptop), or access WSL-ROS using one of the \"WSL-ROS laptops\" that are also available in the lab.</p> <p>Follow the steps below to create your team's ROS package, initialise it as a Git repo, and push it to GitHub to allow for sharing and collaboration amongst your team<sup>1</sup>. You should then be able to transfer your package to one of the robotics laptops easily in the lab when required (we'll provide further details on how to do this later).</p>"},{"location":"com2009/assignment2/getting-started/#git","title":"Configure Git","text":"<p>Warning</p> <p>Only do this bit on your own personal ROS installations (or in WSL-ROS), NOT on one of the robotics laptops!</p> <p>If you haven't done so already, you'll need to make sure Git is configured properly in your local ROS installation before you do anything else.</p> <ol> <li> <p>From a terminal instance located in your home directory (<code>cd ~</code>) run the following commands to update your personal details in the global Git config file on your machine:</p> <p><pre><code>git config --global user.name \"{your name}\"\n</code></pre> ...replacing <code>{your name}</code> with your actual name! E.g.: <code>git config --global user.name \"John Smith\"</code></p> <p><pre><code>git config --global user.email \"{your email address}\"\n</code></pre> ...replacing <code>{your email address}</code> with your actual email address!</p> </li> <li> <p>If you're working in WSL-ROS on a University machine, don't forget to run <code>wsl_ros backup</code> to save these changes to your external WSL-ROS backup file, so that they will always be restored whenever you run <code>wsl_ros restore</code> in a fresh WSL-ROS instance on another machine. </p> <p>Note</p> <p>All team members will actually need to do this bit before interacting with Git!</p> <p>Regardless of which team member is setting up your team's ROS package to begin with, you'll all need to interact with Git for this assignment, and you should therefore each set up your own individual Git configurations (via the steps above) before working individually on your team's ROS package.</p> </li> </ol>"},{"location":"com2009/assignment2/getting-started/#create-pkg","title":"Create Your Team's Assignment #2 ROS package","text":"<p>Remember</p> <p>Only one team member needs to do this next bit! But you'll then need to share it with the rest of your team members via an online code repository (e.g. GitHub), by making the rest of your team members collaborators.</p> <ol> <li> <p>In your local ROS installation, navigate to the <code>catkin_ws/src</code> directory:</p> <pre><code>cd ~/catkin_ws/src/\n</code></pre> </li> <li> <p>Use the <code>catkin_create_pkg</code> tool to create a new ROS package (as covered in Assignment #1):</p> <pre><code>catkin_create_pkg com2009_team{} rospy\n</code></pre> <p>...replacing the <code>{}</code> with your team number!</p> <p>Warning</p> <p>It's really important that you follow the naming conventions that we specify when defining your package (and creating your launch files). If you don't then you could receive no marks!</p> </li> <li> <p>Run <code>catkin build</code> and then re-source your environment:</p> <p><pre><code>catkin build com2009_team{}\n</code></pre> <pre><code>source ~/.bashrc\n</code></pre></p> WSL-ROS Tip <p>Use the <code>src</code> alias for this command!</p> </li> <li> <p>Then navigate into the package directory that should have just been created:</p> <pre><code>cd com2009_team{}/\n</code></pre> </li> <li> <p>This should already contain a <code>src</code> folder for you to populate with all your Python ROS nodes. Create a <code>launch</code> folder in here too, which you will use to store all your launch files:</p> <pre><code>mkdir launch\n</code></pre> </li> <li> <p>Create a placeholder file in the <code>src</code> and <code>launch</code> directories, just to make sure that these folders both get pushed to GitHub when we get to that part in the following section:</p> <pre><code>touch src/placeholder &amp;&amp; touch launch/placeholder\n</code></pre> <p>(you can delete these later on, once you start creating your own Nodes and launch files.)</p> </li> <li> <p>Then follow the steps in the next section to import this project to GitHub (other online code repositories should work similarly)...</p> </li> </ol>"},{"location":"com2009/assignment2/getting-started/#github","title":"Push Your Package to GitHub","text":"<p>Remember</p> <p>Only one member of your team needs to do this bit too!</p> <p>These instructions are taken from this GitHub Docs page. Instructions may vary if you are using other online code repositories (such as GitLab for instance), so check with your target provider.</p> <ol> <li> <p>Create a new (private) repository on GitHub.com, but DON'T initialise the new repository with a README, license, or gitignore (you can do this later, it'll cause issues if you do it at this stage).</p> <p> From docs.github.com </p> <p>Call this repository <code>com2009_team{}</code> to match the ROS package that you've just created.</p> </li> <li> <p>Head back to your local ROS installation, where you created your ROS package in the previous section. Make sure your terminal is located in the root of your new package directory: </p> <pre><code>roscd com2009_team{}\n</code></pre> </li> <li> <p>Initialise the package as a Git repo:</p> <pre><code>git init -b main\n</code></pre> </li> <li> <p>Stage all the initial files in your package (including the placeholders) for an initial commit:</p> <pre><code>git add .\n</code></pre> <p>Warning</p> <p>Don't forget the <code>.</code> at the end there!</p> </li> <li> <p>Then commit them:</p> <pre><code>git commit -m \"First commit\"\n</code></pre> </li> <li> <p>Head back to GitHub on the web. At the top of your repository on GitHub.com's Quick Setup page, click the  button to copy the remote repository URL (HTTPS).</p> <p> From docs.github.com </p> </li> <li> <p>Then, go back to your local terminal and add the URL to the remote repository:</p> <pre><code>git remote add origin {REMOTE_URL}\n</code></pre> <p>Change <code>{REMOTE_URL}</code> to the URL that you copied in the previous step, and get rid of the curly brackets (<code>{}</code>)!</p> <p>Then verify the new remote URL:  <pre><code>git remote -v\n</code></pre></p> </li> <li> <p>Finally, push the changes from your package (your \"local\" repo), to your \"remote\" repository on GitHub:</p> <pre><code>git push origin main\n</code></pre> <p>You'll then be asked to enter your GitHub username, followed by a password. This password is not your GitHub account password!  </p> <p>Warning</p> <p>Your GitHub account password won't work here! You'll need to generate a personal access token (classic) and use this instead!</p> </li> <li> <p>Back on GitHub, add your team members to the repo as collaborators. All team members should then be able to pull the remote repo into their own Catkin Workspaces (<code>cd ~/catkin_ws/src/</code> &amp; <code>git clone {REMOTE_URL}</code>), make contributions and push these back to the remote repo as required (using their own GitHub account credentials and personal access tokens).</p> </li> </ol> <p>You'll need to copy your ROS package onto the Robot Laptops when working on the Real-Robot based tasks, which we'll cover in more detail later. </p>"},{"location":"com2009/assignment2/getting-started/#getting-to-know-the-real-robots","title":"Getting to Know the Real Robots","text":"<p>All the Assignment #2 Tasks will be assessed using real robots, and you'll therefore have access to the robots for every lab session so that you can work on these tasks as you wish. All the details on how the robots work, how to get them up and running and start programming them can be found in the \"Waffles\" section of this course site. You should proceed now as follows (in your teams):</p> <ol> <li>Everyone must complete a health and safety quiz (on Blackboard) before you (or your team) work with the real robots for the first time. Head to Blackboard and do this now, if you haven't already.</li> <li>Each team has been assigned a specific robot (there's a list on Blackboard). When you're ready, speak to a member of the teaching team who will provide you with the robot that has been assigned to you.</li> <li> <p>Work through each page of the \"Waffles\" section of this site (in order):</p> <ol> <li>Read about the hardware.</li> <li>Learn how to launch ROS and get the robots up and running.</li> <li>Have a go at the initial exercises on the Basics of ROS &amp; the Waffles, which will help to get you started and understand how ROS and the robots work.</li> <li>Review the Shutdown Procedures and follow the steps here to shut down the robot and power off the robotics laptop at the end of each lab session.</li> <li> <p>Finally, there are some Fact-Finding Missions which are also really important, but you'll need to have completed certain parts of Assignment #1 before you can complete them all. Either way, have a read through them at this stage and work through the bits that you can (if any).</p> <p>Important</p> <p>Make sure (as a team) you have completed ALL the Fact-Finding Missions before the end of Week 6.   </p> </li> </ol> </li> </ol> <ol> <li> <p>As a University of Sheffield student, you can apply for the GitHub Student Developer Pack, which gives you access to a range of developer tools including GitHub Pro. GitHub Pro allows you to have unlimited collaborators on your repositories, which might help you to collaborate on your ROS package with your team.\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/assignment2/submission/","title":"Exporting your ROS Package for Submission","text":"<p>When it comes to submission time, it's important that you follow the steps below carefully to create an archive of your ROS package correctly. We recommend that you do this from WSL-ROS, or your own local ROS installation, rather than one of the Robotics Laptops.</p> <ol> <li> <p>First, run the following command, which will create a folder in your home directory called <code>myrosdata</code> (if it doesn't already exist):</p> <pre><code>mkdir -p ~/myrosdata/\n</code></pre> </li> <li> <p>Then, navigate to your <code>catkin_ws/src</code> directory:</p> <pre><code>roscd &amp;&amp; cd ../src/\n</code></pre> </li> <li> <p>Use the <code>tar</code> command to create an archive of your team's package:</p> <pre><code>tar -cvf ~/myrosdata/com2009_team{}.tar com2009_team{}\n</code></pre> <p>... replacing <code>{}</code> with your own team number, of course!</p> <p>This will create a <code>.tar</code> archive of your package in the <code>~/myrosdata</code> folder. </p> </li> <li> <p>If you're using WSL-ROS (or any other WSL-based ROS installation) then you can then access this from the Windows File Explorer. In the terminal enter the following command:</p> <pre><code>cd ~/myrosdata/ &amp;&amp; explorer.exe .\n</code></pre> </li> <li> <p>An Explorer window should open, and in there you should see the <code>com2009_team{}.tar</code> file that you just created. Copy and paste this to somewhere convenient on your machine.</p> </li> <li> <p>Submit this <code>.tar</code> file to the appropriate submission portal on Blackboard.</p> </li> </ol>"},{"location":"com2009/assignment2/parta/","title":"Assignment #2 Part A","text":"<p>Total Marks: 40/100</p> <p>Submission: Week 6 </p> <p>Tasks:</p> <ul> <li>Task 1: Velocity Control</li> <li>Task 2: Avoiding Obstacles</li> </ul>"},{"location":"com2009/assignment2/parta/submission/","title":"Submitting Your ROS Package for Part A","text":"<p>Follow the instructions here for preparing your ROS package for submission.</p> <p>In addition to the 30 marks available for Task 1 &amp; 2, there are a further 10 marks available in Part A for making an 'out-of-the-box' submission.</p>"},{"location":"com2009/assignment2/parta/submission/#an-out-of-the-box-submission","title":"An 'Out-of-the-Box' Submission","text":"<p>You will be awarded the full 10 marks if:</p> <ul> <li>You prepare your ROS package correctly, as per the instructions here.</li> <li>Your package follows the \"Key Requirements\" specified here.</li> <li>Your package contains correctly defined launch files for each of the tasks: <code>task1.launch</code> &amp; <code>task2.launch</code>.</li> <li>The package can be successfully deployed on one of the Robotics Laptops (via the launch files, as above) without any intervention from the teaching team.</li> </ul>"},{"location":"com2009/assignment2/parta/submission/#resubmission-for-part-a","title":"Resubmission for Part A","text":"<p>If the package that your team submit for Part A is not deployable on the first submission then you'll have an opportunity to resubmit this in the weeks following the submission deadline. You will receive 0/10 marks for an 'out-of-the-box' submission, but this will at least give you the opportunity to see what went wrong, and for your team to still pick up the remaining 30 marks that are available for Tasks 1 &amp; 2. </p> <p>Warning</p> <p>We will be offering a resubmission opportunity for Part A ONLY. There won't be a re-submission opportunity for Part B, so get it right the second time to avoid missing out on the remaining 60 marks for Assignment #2.</p>"},{"location":"com2009/assignment2/parta/task1/","title":"Task 1: Velocity Control","text":"<p>Submit a working ROS package that can be successfully deployed to control a real TurtleBot3 Waffle, making it follow a prescribed motion profile whilst printing key information to the terminal.</p> <p>Course Checkpoints</p> <p>You should aim to have completed the following additional parts of the COM2009 ROS Course to support your work on this task: </p> <ul> <li>Assignment #1: Part 2, up to (and including) Exercise 4.</li> <li>Fact-Finding Missions: Mission 1 (Publishing Velocity Commands).</li> </ul>"},{"location":"com2009/assignment2/parta/task1/#summary","title":"Summary","text":"<p>The main objective of this task is to create a ROS node (or multiple nodes) that make your robot follow a figure-of-eight pattern on the robot arena floor. The figure-of-eight trajectory should be generated by following two loops, both 1 meter in diameter, as shown below. </p> <p> </p> The figure-of-eight path for Task 1. <p>Whilst doing this, you will also need to print some robot odometry data to the terminal at regular intervals (see below for the specifics). In order to get the terminal message formatting right, you might want to have a look at the documentation on Python String Formatting, or refer to any of the code examples that involve printing messages to the terminal in Assignment #1.</p>"},{"location":"com2009/assignment2/parta/task1/#details","title":"Details","text":"<ol> <li>The robot must start by moving anti-clockwise, following a circular motion path of 1 m diameter (\"Loop 1,\" as shown in the figure above).</li> <li>Once complete, the robot must then turn clockwise to follow a second circular path, again of 1 m diameter (\"Loop 2\").</li> <li>After Loop 2 the robot must stop, at which point it should be located back at its starting point.</li> <li> <p>The velocity of the robot should be defined to ensure that the whole sequence takes approximately 60 seconds to complete (5 seconds).</p> <p>The timer will start as soon as the robot starts moving.</p> </li> <li> <p>The robot's current pose should be printed to the terminal throughout, where messages should be of the following format (exactly): </p> <pre><code>x={x} [m], y={y} [m], yaw={yaw} [degrees].\n</code></pre> <p>Where <code>{x}</code>, <code>{y}</code> and <code>{yaw}</code> should be replaced with the correct real-time odometry data as follows:</p> <ol> <li><code>{x}</code>: the robot's linear position in the X axis, quoted in meters to two decimal places.</li> <li><code>{y}</code>: the robot's linear position in the Y axis, quoted in meters to two decimal places.</li> <li><code>{yaw}</code>: the robot's orientation about the Z axis, quoted in degrees to one decimal place.</li> </ol> <p>The data should be quoted relative to its starting position at the beginning of the task, e.g. at the start of the task (before the robot has moved) the terminal messages should read:</p> <pre><code>x=0.00 [m], y=0.00 [m], yaw=0.0 [degrees].\n</code></pre> <p>These message should be printed to the terminal at a rate of 1Hz. It doesn't matter if the messages continue to be printed to the terminal after the robot has stopped (i.e. after the figure-of-eight has been completed).</p> </li> <li> <p>The ROS package that you submit must contain a launch file called <code>task1.launch</code>, which we (the COM2009 teaching team) will use to execute the correct functionality from within your package. This functionality must be launch-able via the command: </p> <pre><code>roslaunch com2009_team{} task1.launch\n</code></pre> <p>... where <code>{}</code> will be replaced with your team number.</p> <p>(ROS will already be running on the robot before we attempt to execute your launch file). </p> </li> </ol>"},{"location":"com2009/assignment2/parta/task1/#a-note-on-odometry","title":"A note on Odometry","text":"<p>When the robot is placed in the arena at the start of the task its odometry may not necessarily read zero, so you will need to compensate for this. You'll therefore need to grab the robot pose from the <code>/odom</code> topic before your robot starts moving, and then use that as the zero-reference to convert all the subsequent odometry readings that you obtain throughout the task.</p> <p>Odometry and keeping track of the robot's pose is discussed in detail in Assignment #1 Part 2.</p>"},{"location":"com2009/assignment2/parta/task1/#simulation-resources","title":"Simulation Resources","text":"<p>It's easier to develop your node(s) in simulation before testing things out on a real robot. You can use the standard <code>empty_world</code> environment to do this, which can be launched in using the following command:</p> <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre> WSL-ROS Users <p>You can also launch this using the <code>tb3_empty_world</code> command-line alias.</p> <p>For the real task, there will be cylindrical objects placed at the theoretical centre of each of the figure-of-eight loops, so your robot will need to move around these as it completes the task. We have therefore also created a simulation environment that is representative of the real world environment during the assessment. This is available in a package called <code>com2009_simulations</code>, which is part of the <code>tuos_ros</code> Course Repo. The instructions for downloading and installing this within your own local ROS installation are available here.</p> <p>If you've already installed this (as part of Assignment #1 perhaps), then it's worth making sure that you have the most up-to-date version (as discussed here):</p> <pre><code>roscd &amp;&amp; cd ../src/tuos_ros/ &amp;&amp; git pull\n</code></pre> <p>Once you've done all this, then you should be able to launch the Task 1 development arena with the following <code>roslaunch</code> command:</p> <pre><code>roslaunch com2009_simulations task1.launch\n</code></pre> <p> </p> The Task 1 development arena. <p>Note</p> <p>There won't be any loop markers on the real robot arena floor during the assessment.</p>"},{"location":"com2009/assignment2/parta/task1/#marking","title":"Marking","text":"<p>This task will be assessed by the teaching team as part of the Part A submission in Week 6 (i.e. along with Task 2). If things don't work for this first submission then we'll let you know what happened, so that you can try to resolve things for the Part B submission later in the semester.</p> <p>There are 10 marks available for this task in total, summarised as follows:</p> <p> Criteria Marks Details A: The Motion Path 5/10 How closely the real robot follows a true figure-of-eight path in the robot arena, based on the criteria table below. B: Terminal Messages 5/10 The correct formatting of your odometry messages, and the validity of the data that is presented in the terminal as the robot performs the task, based on the criteria table below. <p></p>"},{"location":"com2009/assignment2/parta/task1/#criterion-a-the-motion-path","title":"Criterion A: The Motion Path","text":"<p>Marks: 5/10</p> <p> Criteria Details Marks A1: Direction of travel The robot must move anticlockwise for the first loop (\"Loop 1\") and then clockwise for the second (\"Loop 2\"). 1 A2: Loop 1 The loop must be ~1 m in diameter, centered about the red beacon. 1 A3: Loop 2 The loop must be ~1 m in diameter, centered about the blue beacon. 1 A4: Stopping Once the robot completes its figure of eight, it must stop with both wheels within 10 cm of the start line. 1 A5: Timing The robot must complete the full figure of eight and stop in 55-65 seconds. 1 <p></p>"},{"location":"com2009/assignment2/parta/task1/#criterion-b-terminal-messages","title":"Criterion B: Terminal Messages","text":"<p>Marks: 5/10</p> <p> Criteria Details Marks B1: Rate Messages should be printed to the terminal at a rate of 1 Hz. 1 B2: Format The messages printed to the terminal should be formatted exactly as detailed above. 1 B3: Data Each message value (<code>x</code>, <code>y</code> and <code>yaw</code>) should be plausible, that is: they represent the actual pose of the robot, based on all readings being set to zero at the start/finish point (as illustrated above). In addition, each value must be quoted in the correct units (meters or degrees, as appropriate). 3 <p></p>"},{"location":"com2009/assignment2/parta/task2/","title":"Task 2: Avoiding Obstacles","text":"<p>Develop the ROS node(s) that allow a TurtleBot3 Waffle to autonomously explore an environment containing various obstacles. The robot must explore as much of the environment as possible in 90 seconds without crashing into anything!</p> <p>Course Checkpoints</p> <p>You should aim to have completed the following additional parts of the COM2009 ROS Course to support your work on this task: </p> <ul> <li>Assignment #1: Up to and including Part 5 (in full).</li> <li>Fact-Finding Missions:<ul> <li>Mission 1 (Publishing Velocity Commands),</li> <li>Mission 4 (Out of Range LiDAR Data).</li> </ul> </li> </ul>"},{"location":"com2009/assignment2/parta/task2/#summary","title":"Summary","text":"<p>Assignment #1 Part 3 introduces the Waffle's LiDAR sensor. This sensor is very useful, as it tells us the distance to any objects that are present in the robot's environment. In Part 5 Exercise 4 we look at how this data, in combination with the ROS Action framework, can be used as the basis for a basic obstacle avoidance control system. Building on this in Part 5, Advanced Exercise 1, we discuss how this could be developed further into an effective search strategy by developing an action client that could make successive calls to an the action server to keep the robot moving randomly, and indefinitely, around an arena whilst avoiding obstacles.</p> <p>This is one approach that you could use for this task, but there are other (and potentially simpler) ways that this could be achieved too. </p> <p>In COM2009 Lecture 3 (\"Sensing, Actuation &amp; Control\"), for instance, you are introduced to Cybernetic Control Principles and some of Braitenberg's \"Vehicles,\" which are discussed and implemented on a Lego robot during the lecture! In particular, \"Vehicle 3b\" might well be relevant to consider as a simple method to achieve an obstacle avoidance behaviour.</p> <p>Another aspect of this task is exploration: your robot will be awarded more marks for navigating around more of the environment. Consider the search strategies that are discussed in Lecture 8 (\"Local Guidance Strategies\"), such as \"Brownian Motion\" and \"Levy Walks.\" Could something along these lines be implemented on the  Waffle?</p>"},{"location":"com2009/assignment2/parta/task2/#details","title":"Details","text":"<p>The environment that your robot will need to explore for this will, of course, be the Diamond Computer Room 5 Robot Arena, which is a square arena of 4x4m. For the task, the arena will contain a number of \"obstacles,\" i.e.: short wooden walls and coloured cylinders. Your robot will need to be able to detect these obstacles and navigate around them in order to fully explore the space.</p> <ol> <li>The robot will start in the centre of the arena.</li> <li> <p>It must explore the environment for 90 seconds without touching any of the arena walls or the obstacles within it.</p> <p>The 90-second timer will start as soon as the robot starts moving within the arena.</p> </li> <li> <p>If the robot makes contact with anything before the time has elapsed then the attempt is over.</p> </li> <li>The arena floor will be divided into 16 equal-sized zones and the robot must enter as many of the outer 12 zones as possible during the attempt.</li> <li> <p>The robot must be moving for the entire duration of the task. Simply just turning on the spot for the whole time doesn't count!</p> <p></p> </li> <li> <p>The ROS package that you submit must contain a launch file called <code>task2.launch</code>, such that the functionality that you develop for Task 2 can be launched from your package via the command:</p> <pre><code>roslaunch com2009_team{} task2.launch\n</code></pre> <p>Test this out before submission to make sure that it works!</p> </li> <li> <p>ROS will already be running on the robot before we attempt to execute your launch file on the laptop that the robot has been paired with. </p> </li> </ol> <p>Note</p> <p>The location, orientation and quantity of obstacles in the arena will not be revealed beforehand, so the ROS package that you develop will need to be able to accommodate an unknown environment. </p>"},{"location":"com2009/assignment2/parta/task2/#simulation-resources","title":"Simulation Resources","text":"<p>Within the <code>com2009_simulations</code> package there is an example arena which can be used to develop and test out your team's obstacle avoidance node(s) for this task.</p> <p>Info</p> <p>Make sure you check for updates to the Course Repo to ensure that you have the most up-to-date version of these simulations.</p> <p>The simulation can be launched using the following <code>roslaunch</code> command:</p> <pre><code>roslaunch com2009_simulations task2.launch\n</code></pre> <p></p> <p> </p> The Obstacle Avoidance development arena. <p>Warning</p> <p>The location, orientation and quantity of obstacles will be different to those in this simulation!</p>"},{"location":"com2009/assignment2/parta/task2/#marking","title":"Marking","text":"<p>There are 20 marks available for Task 2 in total, awarded as follows:</p> <p> Criteria Marks Details A: Run Time 8/20 You will be awarded marks for the amount of time that your robot spends exploring the environment before 90 seconds has elapsed, or the robot makes contact with anything in its environment (as per the table below). The robot must leave the central zone (the red zone in the simulation) in order to be eligible for any of these marks. B: Exploration 12/20 You will be awarded 1 mark for each of the outer 12 arena zones that the robot manages to enter (i.e. excluding the four zones in the middle). The robot only needs to enter each of the 12 zones once, but its full body must be inside the zone marking to be awarded the mark. <p></p>"},{"location":"com2009/assignment2/parta/task2/#run-time","title":"Criterion A: Run Time","text":"<p>Marks: 8/20</p> <p>Marks will be awarded as follows:</p> <p> Time (Seconds) Marks 0-9 0 10-19 1 20-29 2 30-39 3 40-49 4 50-59 5 60-89 6 The full 90! 8 <p></p>"},{"location":"com2009/assignment2/partb/","title":"Assignment #2 Part B","text":"<p>Total Marks: 60/100</p> <p>Submission: Week 12 </p> <p>Tasks:</p> <ul> <li>Task 3: Maze Navigation (assessed in simulation)</li> <li>Task 4: Exploration &amp; Search</li> </ul>"},{"location":"com2009/assignment2/partb/task3/","title":"Task 3: Maze Navigation","text":"<p>Update (20th March 2024)</p> <p>This task will now be assessed in simulation, NOT on a real robot!</p> <p>Develop the ROS node(s) to enable a TurtleBot3 Waffle to navigate a maze in 150 seconds or less without crashing into anything.</p> <p>Course Checkpoints</p> <p>You should aim to have completed the following additional parts of the COM2009 ROS Course to support your work on this task: </p> <ul> <li>Assignment #1: Up to and including Part 5 (in full).</li> <li>Fact-Finding Missions:<ul> <li>Mission 1 (Publishing Velocity Commands),</li> <li>Mission 4 (Out of Range LiDAR Data).</li> </ul> </li> </ul>"},{"location":"com2009/assignment2/partb/task3/#summary","title":"Summary","text":"<p>For this task your robot will need to navigate a maze of corridors without touching the walls.</p> <p>As with Task 2, the LiDAR sensor will be essential here, this time allowing the robot to detect the maze walls and maintain a safe distance from them. Your robot will need to progress through a maze all the way to the end in order to be awarded maximum marks. The robot's Odometry System may prove useful too; allowing it to keep track of where it is and where it has already been, so that it continues to progress through the maze and doesn't waste time going back on itself. The maze may contain dead-ends, so you'll need to make sure your robot is able to navigate its way out of these and (ideally) not go down the same ones repeatedly!</p> <p>One common method for solving mazes is to use a wall following algorithm. The maze that the Waffle will need to navigate for this task won't contain any islands, i.e. it will be \"simply connected,\" so this is one method you might choose to adopt here. The drawback however, is that you'll need to decide in advance whether to implement a left- or right-hand wall follower, without knowing what the final maze is going to look like. Depending on the maze configuration, one could prove much less efficient than the other!</p>"},{"location":"com2009/assignment2/partb/task3/#details","title":"Details","text":"<p>The maze that the robot will need to navigate for this task will span the full 4x4m area of the Computer Room 5 Robot Arena occupy a 5x5m square arena in a simulated environment. There'll only be wooden walls in the arena for this task though, no other objects. The maze will be constructed to ensure there is always enough space for a TurtleBot3 Waffle to comfortably pass through any apertures or corridors.</p> <ol> <li>The robot will start at one end of a maze and must autonomously navigate all the way to the other end (or get as far as possible).</li> <li>The robot must do this without touching any of the arena walls, and penalties will be applied for those that do (See the table below).</li> <li>The robot's progress will be measured at ~10% increments throughout the maze, using distance markings on the arena floor (the finish line representing 100%).</li> <li> <p>The robot will have a maximum of 150 seconds to navigate the maze. </p> <p>The 150-second timer will start as soon as the robot starts moving.</p> <p></p> </li> <li> <p>Your team's ROS package must contain a launch file called <code>task3.launch</code>, such that the functionality that you develop for this maze navigation task can be launched from your package via the command:</p> <pre><code>roslaunch com2009_team{} task3.launch\n</code></pre> <p>(ROS will already be running on the robot before we attempt to execute your launch file on the laptop that the robot has been paired with The robot will have already been launched into the simulated environment before we attempt to execute your launch file.)</p> </li> </ol>"},{"location":"com2009/assignment2/partb/task3/#simulation-resources","title":"Simulation Resources","text":"<p>Info</p> <p>Make sure you check for updates to the Course Repo to ensure that you have the most up-to-date version of these simulations.</p> <p>Within the <code>com2009_simulations</code> package there are two example mazes, which can be launch using <code>roslaunch</code> as follows:</p> Maze Variant AMaze Variant B <pre><code>roslaunch com2009_simulations task3.launch\n</code></pre> <p> Maze Navigation Arena (Example A) </p> <pre><code>roslaunch com2009_simulations task3b.launch\n</code></pre> <p> Maze Navigation Arena (Example B) </p> <p>You can use the above to develop your ROS node(s) for Task 3, but note that the maze that will be used for the final assessment will be different to the examples!</p>"},{"location":"com2009/assignment2/partb/task3/#marks","title":"Marking","text":"<p>There are 20 marks available for this task in total, awarded based on the following criteria:</p> <p> Criteria Marks Details A: Progress Through the Maze 15/20 Marks will be awarded based on the furthest progress marker that your robot manages to cross within the 150-second time limit. It doesn't matter if the robot happens to turn around and move back behind a marker again at any point during the assessment. Progress markers will represent progress through the maze at 10% increments (approximately). Marks will be awarded at 10% increments only (i.e. no fractional marks), but the whole of the robot must have crossed the progress marker in order to be awarded the associated marks (as per the table below). B: An 'incident-free run' 5/20 If the robot completes the task (or the 150 seconds elapses) without it making contact with anything in the arena then your team will be awarded the maximum marks here. Marks will be deducted for each contact that the robot makes with the environment, to a minimum of 0 (i.e. no negative marking will be applied: the minimum mark that you can receive for this is zero). Your robot must at least pass the 10% progress marker to be eligible for these marks. Once five incidents have been recorded then the assessment will be stopped. <p></p>"},{"location":"com2009/assignment2/partb/task3/#progress","title":"Criterion A: Progress Through the Maze","text":"<p>Marks: 15/20</p> <p>As shown in the table below, for the first half of the maze (up to 50%) one mark will be awarded per progress marker that is passed, and from 60% onwards this increases to two marks per progress marker:</p> <p> Progress (%) Marks 10 1 20 2 30 3 40 4 50 5 60 7 70 9 80 11 90 13 100 15 <p></p>"},{"location":"com2009/assignment2/partb/task4/","title":"Task 4: Exploration & Search","text":"<p>Develop the ROS node(s) that allow a TurtleBot3 Waffle to autonomously explore the full Computer Room 5 robot arena, navigating through a series of rooms as quickly as possible whilst documenting its exploration with a photo of a beacon and a map of the environment as it goes! </p> <p>Course Checkpoints</p> <p>You should aim to have completed the following additional parts of the COM2009 ROS Course to support your work on this task: </p> <ul> <li>Assignment #1: All of it, in full!</li> <li>Fact-Finding Missions: Missions 1-5 (i.e. all of them!)</li> </ul>"},{"location":"com2009/assignment2/partb/task4/#summary","title":"Summary","text":"<p>For this task the robot arena will contain a series of \"rooms\" each with a coloured, cylindrical beacon in it. The main aim is to safely explore each of the rooms in the shortest time possible (emphasis on \"safely\" here, meaning you need to also try not to crash into anything in the process!) At the same time, you'll need to search for a beacon of a particular colour as well as documenting your exploration by building a map of the environment with SLAM.</p>"},{"location":"com2009/assignment2/partb/task4/#simulation-resources","title":"Simulation Resources","text":"<p>Once again, there's a simulation to help you develop your code for this task outside the lab sessions. This also helps to illustrate the nature of the task. You can launch the simulation from the <code>com2009_simulations</code> package with the following <code>roslaunch</code> command:</p> <pre><code>roslaunch com2009_simulations task4.launch\n</code></pre> <p> <p> </p> The development arena for Task 4. <p></p> <p>Remember</p> <p>Make sure you check for updates to the course repo to ensure that you have the most up-to-date version of this.</p> <p>The real robot arena might look something like this for the real assessment, where (similarly to the simulated environment) \"rooms\" will be constructed of wooden walls 180 mm tall, 10 mm thick and either 440 mm or 880 mm in length, and each room will contain a cylindrical beacon of 200 mm diameter and 250 mm height.</p> <p>Note</p> <p>The actual layout of the real robot arena will be different to the simulation!</p> <ol> <li>Rooms will be different shapes and sizes and in different locations, but there will always be four of them.</li> <li>The robot might not necessarily be located at the same starting point as in the simulation.</li> <li>Beacons will be the same shape, size and colour as those in the simulation (yellow, red, green and blue). Detecting colours is a lot harder in the real-world than it is in simulation though, so you'll need to do a lot of testing on a real robot if you want to get this working robustly (you will have access to all the beacons during the lab sessions).</li> </ol> <p>Warning</p> <p>Even if your code works perfectly in simulation, that doesn't mean that it will work in the real world. Test things thoroughly on the real Waffles as much as possible!</p>"},{"location":"com2009/assignment2/partb/task4/#details","title":"Details","text":"<p>The robot will have 3 minutes (180 seconds) in total to complete this task.</p> <ol> <li>The arena floor will be marked out into 9 equal-sized zones. You will be awarded marks for each of the zones that the robot enters within the time available (excluding the one it starts in).</li> <li>In addition to this, the robot will need to try to explore the four rooms that will also be present in the arena. There will be marks available not only for the number of rooms that the robot manages to explore, but also the speed at which it manages to explore them all (see the marking section below for more details).</li> <li>Your robot will need to do this whilst avoiding contact with anything in the environment (\"incidents\"). Once an incident has taken place, we'll move the robot away slightly so that it is free to move again, but after five incidents have occurred the assessment will be stopped.</li> <li> <p>Your team's ROS package must contain a launch file named <code>task4.launch</code>, such that (for the assessment) we are able to launch all the nodes that you have developed for this task via the following command:</p> <pre><code>roslaunch com2009_team{} task4.launch\n</code></pre> </li> </ol> <p>Having developed the core functionality for the task, you'll then need to think about a couple of more advanced features...</p>"},{"location":"com2009/assignment2/partb/task4/#advanced-feature-1-a-photo-of-a-beacon","title":"Advanced Feature 1: A photo of a beacon","text":"<p>As with all previous tasks, we will launch the ROS node(s) from within your package for this task using <code>roslaunch</code>. For this one however, we will also attempt to supply an additional argument when we make the command-line call:</p> <pre><code>roslaunch com2009_team{} task4.launch target_colour:={colour}\n</code></pre> <p>...where <code>{colour}</code> will be replaced with either <code>yellow</code>, <code>red</code>, <code>green</code> or <code>blue</code> (the target colour will be selected randomly). Based on this input, your robot will need to capture an image of the beacon in the arena of that colour!</p> <p>Remember</p> <p>You should know from this the fact-finding mission that the camera image topic name is different on the real robot!</p> <p>The root of your package directory must contain a directory called <code>snaps</code>, and the image must be saved into this directory with the file name: <code>task4_beacon.jpg</code>. </p> <p>You will therefore need to define your launch file to accommodate the <code>target_colour</code> command-line argument. In addition to this, inside your launch file you'll also need to pass the value of this to a ROS node within your package, so that the node knows which beacon to actually look for (i.e. your node needs to know whether to look for a yellow, red, green or blue beacon). We didn't actually cover this kind of launch file functionality in Assignment #1, but there are a whole load of additional resources available in the Launch Files section of this course site, which should help you with this.</p> <p></p> <p>We will test whether your launch file has been correctly built to accept the <code>target_colour</code> command-line argument using autocomplete in the terminal. After typing the first four characters of the argument name, i.e.: \"targ\", the rest of the name should be completed for us when we press the Tab key, as illustrated below: </p> <p> <p></p> <p></p> <p></p> <p>To illustrate that the value of the <code>target_colour</code> command-line argument has been correctly passed to a ROS Node within your package, you should configure your Task 4 Node (or any one of your nodes, if you have multiple) to print a message to the terminal as soon as it starts. The message should be formatted exactly as follows:</p> <pre><code>TASK 4 BEACON: The target is {colour}.\n</code></pre> <p>...where <code>{colour}</code> (including the curly brackets!) is replaced with the actual colour that was passed to your <code>task4.launch</code> file (<code>yellow</code>, <code>red</code>, <code>green</code> or <code>blue</code>). </p>"},{"location":"com2009/assignment2/partb/task4/#advanced-feature-2-mapping-with-slam","title":"Advanced Feature 2: Mapping with SLAM","text":"<p>Marks are also available if, whilst your robot is completing this task, you can also run SLAM and generate a map of the environment in the background.</p> <p>In Assignment #1 Part 3 Exercise 2 you launched SLAM using the following <code>roslaunch</code> command:</p> <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch\n</code></pre> <p>Once you've had a look at the Launch Files section it should be clear how to launch other launch files from within your own. Alternatively, you could navigate to the <code>turtlebot3_slam</code> package in your local ROS installation (or on the robot laptops) to have a look at the content of the launch file that we executed from within this package for Exercise 2, to see how you might be able launch the same (or similar) functionality within your own <code>task4.launch</code> file.</p> <p>When it comes to saving the map that has been generated by SLAM, think about how we did this in the Part 3 Exercise. This involved calling a <code>map_saver</code> node from the command-line. It's possible, however, to call nodes (and indeed launch files too) from within other ROS nodes using the ROS Launch Python API.</p> <p>The root of your package directory must contain a directory called <code>maps</code>, and the map file that you obtain must be saved into this directory with the name: <code>task4_map</code>.</p>"},{"location":"com2009/assignment2/partb/task4/#marking","title":"Marking","text":"<p>There are 40 marks available for this task in total, awarded based on the criteria outlined below.</p> <p> Criteria Marks Details A: Arena exploration 8/40 For this task, the arena will be divided into nine equal-sized zones. You will be awarded 1 mark for each zone that your robot manages to enter, excluding the one it starts within. The robot only needs to enter each zone once, but its full body must be inside the zone marking to be awarded the associated mark. B: Room exploration 12/40 Marks will be awarded based on the maximum number of rooms that your robot manages to explore within the 180-second time limit and the speed by which it does this. The marking details are outlined here. C: An 'incident-free run' 5/40 Similarly to Task 3, if your robot completes the task (or the 180 seconds elapses) without it making contact with anything in the arena then you will be awarded a full 5 marks for this, with deductions then applied for each unique \"incident\" (to a minimum of 0/5). Your robot must at least leave the zone that it starts in to be eligible for these marks. Once five incidents have been recorded then the assessment will be stopped. D1: A Photo of a Beacon 10/40 Further details below. D2: Mapping with SLAM 5/40 Further details below. <p></p>"},{"location":"com2009/assignment2/partb/task4/#room-explore","title":"Criterion B: Room exploration","text":"<p>The marks available per room explored will be awarded as follows:</p> <p> Time (seconds) 1 room 2 rooms 3 rooms 4 rooms 150-180 1.0 4.0 7.0 12.0 120-149 1.5 4.5 7.5 12.0 90-129 2.0 5.0 8.0 12.0 60-89 2.5 5.5 9.0 12.0 &lt;60 3.0 6.0 9.0 12.0 <p></p> <p>The 180-second timer will start as soon as the robot starts moving within the arena.</p>"},{"location":"com2009/assignment2/partb/task4/#crit-d1","title":"Criterion D1: A Photo of a Beacon","text":"<p> Criteria Details Marks D1.a Your <code>task4.launch</code> file accepts the <code>target_colour</code> command line argument (assessed by checking for autocompletion, as discussed above) and a message is printed to the terminal to indicate that the correct target colour has been passed to a node in your package (this must occur straight away on executing your launch file and the message format must be exactly as specified here). 2 D1.b At the end of the assessment a single image file, called <code>task4_beacon.jpg</code>, has been obtained from the robot's camera (during the course of the assessment), and this is located in a folder called <code>snaps</code> at the root of your package directory i.e.: <code>com2009_team{}/snaps/task4_beacon.jpg</code>. 2 D1.c Your <code>com2009_team{}/snaps/task4_beacon.jpg</code> image file contains any part of the correct beacon. 3 D1.d Your <code>com2009_team{}/snaps/task4_beacon.jpg</code> image file contains the correct beacon, where the entire width of the beacon can be observed. 3 <p></p>"},{"location":"com2009/assignment2/partb/task4/#crit-d2","title":"Criterion D2: Mapping with SLAM","text":"<p> Criteria Details Marks D2.a By the end of the assessment a map of the robot arena (or any part of it) must have been generated. Two files should exist: a <code>.pgm</code> and a <code>.yaml</code>, both of which must be called <code>task4_map</code>, and both must be located in a <code>maps</code> folder at the root of your package directory i.e. <code>com2009_team{}/maps/task4_map.pgm</code> and <code>com2009_team{}/maps/task4_map.yaml</code>. 2 D2.b Your <code>com2009_team{}/maps/task4_map.pgm</code> map that is created during the assessment depicts at least one of the rooms in the arena. 3 <p></p>"},{"location":"extras/ros-msgs/","title":"Creating Your Own Custom ROS Messages","text":"<p>Want to create your own ROS Topic/Service/Action Messages?</p> <p>Defining your own messages is quite straight forward, but there are a few key rules to follow, and these rules differ slightly depending on whether you are trying to create a standard topic message type, a Service message or an Action. You'll also need to make sure that the <code>CMakeLists.txt</code> and <code>package.xml</code> files within your ROS package contain the correct instructions so that the necessary source code is generated correctly when you run <code>catkin build</code> (i.e. so that you can import the messages into your Python (or C++ etc.) nodes).</p> <p>Below are some links to some useful external resources that explain how to create each different type of ROS message for your own specific applications:</p> <ul> <li>Creating ROS Topic and Services Messages (<code>msg</code> and <code>srv</code>)</li> <li>Creating ROS Action Messages</li> </ul>"},{"location":"extras/tuos-ros/","title":"The TUoS ROS Course Repo","text":"<p>A ROS Metapackage called <code>tuos_ros</code> has been put together to support these courses. This package is available on GitHub here. This repo contains the following ROS packages:</p> Package Name Description <code>tuos_examples</code> Some example scripts to support certain exercises in COM2009 Assignment #1 <code>tuos_msgs</code> Some custom ROS messages to support certain exercises in COM2009 Assignment #1 (to learn how to create your own custom ROS messages see here) <code>tuos_simulations</code> Some Gazebo simulations to support certain exercises in COM2009 Assignment #1 <code>tuos_tb3_tools</code> Scripts that run on the real Waffles, and some RViz configs to support real robot work too <code>com2009_simulations</code> Simulation resources to support your development work in COM2009 Assignment #2"},{"location":"extras/tuos-ros/#installing","title":"Installing","text":"<p>The <code>tuos_ros</code> course repo is already installed and ready to go in WSL-ROS, for those who use it, and on the Robotics Laptops. To install the packages in your own local ROS installation, follow the steps here.</p> <ol> <li>Make sure you've already set up a Catkin Workspace in your home directory.</li> <li> <p>Navigate to your Catkin Workspace <code>src</code> directory:</p> <pre><code>cd ~/catkin_ws/src/\n</code></pre> </li> <li> <p>Clone the repo from GitHub:</p> <pre><code>git clone https://github.com/tom-howard/tuos_ros.git\n</code></pre> </li> <li> <p>Run <code>catkin build</code> to compile the packages:</p> <pre><code>catkin build\n</code></pre> </li> <li> <p>Then re-source your <code>.bashrc</code>:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol>"},{"location":"extras/tuos-ros/#verify","title":"Verify","text":"<p>Once you've installed it, then you can verify that the build process has worked using the following command:</p> <pre><code>roscd tuos_ros\n</code></pre> <p>Which should take you to a directory within the repo that's also called <code>tuos_ros</code>, i.e.:</p> <pre><code>~/catkin_ws/src/tuos_ros/tuos_ros\n</code></pre>"},{"location":"extras/tuos-ros/#updating","title":"Updating","text":"<p>The course repo may be updated every now and again, so its worth checking regularly that you have the most up-to-date version. You can do this by pulling down the latest updates from GitHub using <code>git pull</code>:</p> <pre><code>roscd &amp;&amp; cd ../src/tuos_ros/ &amp;&amp; git pull\n</code></pre> <p>If you see the following message:</p> <pre><code>-bash: cd: tuos_ros/: No such file or directory\n</code></pre> <p>... then go back and make sure you've installed the repo first!</p> <p>Then, run <code>catkin build</code> and re-source your environment:</p> <pre><code>roscd &amp;&amp; cd .. &amp;&amp; catkin build &amp;&amp; source ~/.bashrc\n</code></pre>"},{"location":"extras/launch-files/","title":"Launch Files","text":""},{"location":"extras/launch-files/#overview","title":"Overview","text":"<p>As you should know from the COM2009 ROS Course, Nodes can be executed on a ROS network in two ways:</p> <ol> <li> <p>Using the <code>rosrun</code> command:</p> <pre><code>rosrun {package name} {script name}\n</code></pre> </li> <li> <p>Or by using <code>roslaunch</code>:</p> <pre><code>roslaunch {package name} {launch file}\n</code></pre> </li> </ol> <p>In Assignment #1 Part 1 you learn that a launch file is an <code>xml</code> file with a <code>.launch</code> file extension. Inside this we can ask ROS to do a number of different things from one single <code>roslaunch</code> command-line call.</p> <p><code>roslaunch</code> offers a number of advantages over <code>rosrun</code>:</p> <ul> <li>Multiple nodes can be executed simultaneously.</li> <li><code>roslaunch</code> will launch the ROS Master for us, if isn't already running (so we don't have to manually call <code>roscore</code>).</li> <li>From within one <code>.launch</code> file, we can launch other <code>.launch</code> files.</li> <li>We can pass in additional arguments to launch certain things conditionally, or pass specific arguments to certain ROS nodes.</li> <li>As if that wasn't enough, there is also a <code>roslaunch</code> Python API, allowing us to launch nodes from within other (Python) nodes!</li> </ul> <p>In this section we'll talk about some of these advanced features that you may find useful.</p>"},{"location":"extras/launch-files/cla-to-node/","title":"Passing Command-line Arguments to Python Nodes via ROSlaunch","text":"<p>COM2009 Assignment #1 Checkpoint</p> <p>It helps if you've already completed Assignment #1 Part 1 before working on this.</p> <p>Having built a CLI for our Python Node on the previous page, we'll now look at how this all works when we call a node using <code>roslaunch</code> instead. </p> <ol> <li> <p>First, create a launch file called <code>publisher_cli.launch</code>. It probably makes sense to create this inside your <code>part1_pubsub</code> package:</p> <p><pre><code>roscd part1_pubsub/launch\n</code></pre> <pre><code>touch publisher_cli.launch\n</code></pre></p> </li> <li> <p>Inside this add the following:</p> <pre><code>&lt;launch&gt;\n  &lt;arg name=\"roslaunch_colour\" default=\"Black\" /&gt;\n  &lt;arg name=\"roslaunch_number\" default=\"0.999\" /&gt;\n\n  &lt;node pkg=\"tuos_examples\" type=\"publisher_cli.py\"\n    name=\"publisher_cli_node\" output=\"screen\"\n    args=\"-colour $(arg roslaunch_colour) -number $(arg roslaunch_number)\" /&gt;\n&lt;/launch&gt;\n</code></pre> <p>Here, we are specifying command-line arguments for the launch file using <code>&lt;arg&gt;</code> tags, as discussed earlier:</p> <pre><code>&lt;arg name=\"roslaunch_colour\" default=\"Black\" /&gt;\n&lt;arg name=\"roslaunch_number\" default=\"0.999\" /&gt;    \n</code></pre> <p>Next, we use a <code>&lt;node&gt;</code> tag to launch the <code>publisher_cli.py</code> node from the <code>tuos_examples</code> package. All of that should be familiar to you from Part 1. What's new here however is the additional <code>args</code> attribute:</p> <pre><code>args=\"-colour $(arg roslaunch_colour) -number $(arg roslaunch_number)\"\n</code></pre> <p>This is how we pass the <code>roslaunch</code> command-line arguments into the CLI of our ROS Node:</p> <ul> <li><code>-colour</code> and <code>-number</code> are the command-line arguments of the Python Node</li> <li><code>roslaunch_colour</code> and <code>roslaunch_number</code> are the command-line arguments of the launch file</li> </ul> </li> <li> <p>Run the launch file without passing any arguments first, and see what happens:</p> <pre><code>roslaunch part1_pubsub publisher_cli.launch\n</code></pre> </li> <li> <p>Then try again, this time setting alternative values for the available command-line arguments:</p> <pre><code>roslaunch part1_pubsub publisher_cli.launch roslaunch_colour:=purple roslaunch_number:=4\n</code></pre> </li> </ol>"},{"location":"extras/launch-files/launching-launch-files/","title":"Launching Other Launch Files","text":"<p>COM2009 Assignment #1 Checkpoint</p> <p>It helps if you've already completed Part 2 up to and including Exercise 4 before working on this as, in this example, we'll build on the <code>move_circle.py</code> node.</p> <p>We need a simulation of our Waffle active in order to run this, which we can enable with the following command:</p> <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre> <p>Suppose you wanted to save yourself some work and launch both the simulation and the <code>move_circle.py</code> node at the same time...</p> <ol> <li> <p>Firstly, navigate to your <code>part2_navigation</code> package and create a <code>launch</code> directory (if you haven't done so):</p> <p><pre><code>roscd part2_navigation\n</code></pre> <pre><code>mkdir launch\n</code></pre> <pre><code>cd launch/\n</code></pre></p> </li> <li> <p>Then create a launch file. You could call this whatever you want, but for the purposes of this example we'll create one called <code>circle.launch</code>:</p> <pre><code>touch circle.launch\n</code></pre> </li> <li> <p>Inside this file add the following:</p> <pre><code>&lt;launch&gt;\n  &lt;include file=\"$(find turtlebot3_gazebo)/launch/turtlebot3_empty_world.launch\" /&gt;\n  &lt;node pkg=\"part2_navigation\" type=\"move_circle.py\" name=\"circle_node\" output=\"screen\" /&gt;\n&lt;/launch&gt;\n</code></pre> <p>Having completed Assignment #1 Part 1, the <code>&lt;node&gt;</code> tag should already be familiar to you. The <code>&lt;include&gt;</code> tag before it however, might not be...</p> <p>This is what we use to launch other launch files. Here, we are locating the <code>turtlebot3_gazebo</code> package (using <code>find</code>), and asking for the <code>turtlebot3_empty_world.launch</code> file to be executed.</p> </li> <li> <p>From the command-line, execute your newly created launch file as follows:</p> <pre><code>roslaunch part2_navigation circle.launch\n</code></pre> <p>The Waffle will be launched in the \"empty world\" Gazebo environment, and the robot should start moving in a circle straight away!</p> </li> </ol>"},{"location":"extras/launch-files/python-clis/","title":"Building Command-Line Interfaces (CLIs) for Python Nodes","text":"<p>COM2009 Assignment #1 Checkpoint</p> <p>It helps if you've already completed Assignment #1 Part 1 before working on this.</p> <p>Suppose we have a very simple node such as the <code>publisher.py</code> node that you create in Part 1. This node publishes <code>std_msgs/String</code> type messages to a topic called <code>\"chatter\"</code>. Let's have a look at an alternative version of that node that takes in command-line arguments and publishes those to the <code>\"chatter\"</code> topic instead.</p> <p>Take a look at the <code>publisher_cli.py</code> node from the <code>tuos_examples</code> package. </p> <p>Here, we use the Python <code>argparse</code> module to allow us to work with arguments that are passed to the node from the command-line:</p> <pre><code>import argparse\n</code></pre> <p>We instantiate <code>argparse</code> in the <code>__init__()</code> method of the <code>Publisher()</code> class to build a command-line interface (CLI) for the node:</p> <p><pre><code>cli = argparse.ArgumentParser(...)\ncli.add_argument(...)\n</code></pre> (See the code for more details)</p> <p>Arguments that we define with a <code>-</code> at the front will be optional, i.e. we don't have to provide these every time we run the node. We do, however, need to assign a default value for each optional argument in cases where no value is supplied, e.g.:</p> <pre><code>cli.add_argument(\n    \"-colour\",\n    metavar=\"COL\",\n    default=\"Blue\", \n    ...\n</code></pre> <p>The final step is to grab any arguments that are passed to this node when it is called. We use the <code>rospy.myargv()</code> method here, so that this works regardless of whether we call the node using <code>rosrun</code> or <code>roslaunch</code>: </p> <pre><code>self.args = cli.parse_args(rospy.myargv()[1:])\n</code></pre> <p>Having defined the CLI above, <code>argparse</code> then automatically generates help text for us! Try running the following command to see this in action:</p> <pre><code>rosrun tuos_examples publisher_cli.py -h\n</code></pre> <p>Warning</p> <p>You need to have a ROS Master running in order for this to work, so do this in another terminal instance by running the <code>roscore</code> command.</p> <p>Run the node as it is (using <code>rosrun</code>) and see what happens:</p> <pre><code>rosrun tuos_examples publisher_cli.py\n</code></pre> <p>Stop the node (using Ctrl+C) and then run it again, but this time providing a value for the <code>number</code> variable, via the CLI:</p> <pre><code>rosrun tuos_examples publisher_cli.py -number 1.5\n</code></pre> Info <p>We can assign values to Python command-line arguments using a space (as above) or the <code>=</code> operator (<code>-number=1.5</code>). </p> <p>Stop the node again (using Ctrl+C) and also stop the ROS Master that you enabled in another terminal instance.</p>"},{"location":"extras/launch-files/roslaunch-api/","title":"Using the ROS Launch Python API","text":"<p>COM2009 Assignment #1 Checkpoint</p> <p>It helps if you've already completed Assignment #1 up to and including all of Part 3 before working on this.</p> <p>The ROS Launch Python API allows us to launch nodes (and other launch files) from inside our own nodes! The official documentation for the ROS Launch API is the best place to go to find out how to use this, but here's a quick example, if you're short on time. </p> <p>Within the <code>tuos_examples</code> package there is a node called <code>pointless_file_generator</code>.</p> <p>Tip</p> <p>You may need to update your local copy of the course repo to ensure that you have this!</p> <p>This node takes a file path as a command-line argument, and writes a text file of the same name to the file system. From the command line, you can call the node like this:</p> <pre><code>rosrun tuos_examples pointless_file_generator -f FILENAME\n</code></pre> <p>A text file called <code>FILENAME.txt</code> will be saved to your current working directory.</p> <ul> <li>Use the <code>pwd</code> to determine your current working directory,</li> <li>Use the <code>ls</code> command to show the files/directories present at this file system location, </li> <li> <p>Then, use the <code>cat</code> command to display the content of the <code>FILENAME.txt</code> file, e.g.:</p> <pre><code>cat FILENAME.txt\n</code></pre> </li> </ul> <p>Suppose you wanted to call this node programmatically instead (i.e. from within another Python Node). Below is an example of a very simple ROS node that does just that. Once the code below is executed it will launch the <code>pointless_file_generator</code> node from the <code>tuos_examples</code> package, but this time using the ROS Launch API. The key difference here is that instead of <code>FILENAME</code>, we need to define a full path to the text file that we want to generate.</p> roslaunch_api_example.py<pre><code>#!/usr/bin/env python3\n\nimport roslaunch\nimport rospy\n\nfile_path = \"/full/path/to/text/file\"\n\nrospy.init_node(\"pointless_file_generator_caller\", anonymous=True)\nrate = rospy.Rate(0.5)\n\nlaunch = roslaunch.scriptapi.ROSLaunch()\nlaunch.start()\n\nprint(f\"Saving file at time: {rospy.get_time()}...\")\nnode = roslaunch.core.Node(\n    package=\"tuos_examples\",\n    node_type=\"pointless_file_generator\",\n    args=f\"-f {file_path}\",\n    output=\"screen\",\n)\nprocess = launch.launch(node)\nrate.sleep()\n</code></pre> <p>Create a node in one of your own ROS packages and then copy and paste the above code into it. You'll need to update the <code>file_path = \"/full/path/to/text/file\"</code> line to represent the path to a real location on your file system. The file will be then created automatically by the node in the specified location. You must specify a full file system path in order for this to work correctly using the ROS Launch Python API though, and you will only be able to write to a location that's within your Home directory (e.g.: <code>/home/student/path/to/file</code>), otherwise you'll get an error!</p>"},{"location":"extras/launch-files/roslaunch-api/#assignment-2-hint-saving-a-slam-map-programmatically","title":"Assignment #2 Hint: Saving a SLAM Map Programmatically","text":"<p>In the SLAM Exercise in Assignment #1 Part 3 we run SLAM in the background while driving our Waffle around a simulated environment manually, using the <code>turtlebot3_teleop</code> node. Once SLAM has generated a full map we use a <code>rosrun</code> command (in the terminal) to call a <code>map_saver</code> node to save a copy of this map for us: </p> <pre><code>rosrun map_server map_saver -f {map name}\n</code></pre> <p>You can therefore use exactly the same approach as in the example above to achieve this programmatically!</p> <p>Wrap this into one of your own Task 4 ROS nodes, or build a standalone node inside your team's ROS package to call the <code>map_saver</code> node when required. Perhaps you could even wrap the relevant lines of code from the above example inside a while loop and get the node to update a map file at regular intervals (say every 5 seconds?) while your robot explores its environment...</p> <p>Warning</p> <ol> <li>When calling the <code>map_saver</code> node you may actually find that it must be wrapped within a loop in order for the process to work reliably.</li> <li>Don't call the <code>map_saver</code> node too regularly though (i.e. no more than once every few seconds at most), otherwise you'll swamp the ROS network which can result in the system becoming unstable and the robot becoming unresponsive. </li> </ol>"},{"location":"extras/launch-files/roslaunch-clas/","title":"ROSlaunch Command-line Arguments","text":"<p>COM2009 Assignment #1 Checkpoint</p> <p>It helps if you've already completed Assignment #1 Part 1 before working on this.</p> <p>A lot of the launch files that we have use throughout COM2009 Assignment #1 actually have Command-Line Arguments (CLAs) that can be (optionally) supplied when making the <code>roslaunch</code> call. Consider the <code>turtlebot3_empty_world.launch</code> file for instance...</p> <p>Enter the full command and then press the Space and Tab keys on your keyboard like so:</p> <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch[SPACE][TAB][TAB]\n</code></pre> <p>This will reveal the available command-line arguments for this launch file:</p> <pre><code>model  x_pos  y_pos  z_pos\n</code></pre> <p>We can therefore optionally specify custom values for these parameters in order to change what happens when the launch file is executed. Try this, for example:</p> <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch x_pos:=1 y_pos:=1\n</code></pre> <p>When the Gazebo simulation is launched, the robot will be located at coordinates <code>(1, 1)</code> in the <code>X-Y</code> plane, rather than <code>(0, 0)</code>, as would usually be the case.</p> <p>Note</p> <p>We assign values to <code>roslaunch</code> command-line arguments using the <code>:=</code> operator. </p> <p>This is all made possible through the use of <code>&lt;arg&gt;</code> tags at the start of a launch file (just after the opening <code>&lt;launch&gt;</code> tag):</p> <pre><code>&lt;launch&gt;\n  &lt;arg name=\"x_pos\" default=\"0.0\"/&gt;\n  &lt;arg name=\"y_pos\" default=\"0.0\"/&gt;\n  ...\n</code></pre> <p>Within these tags we need to define two attributes:</p> <ol> <li><code>name</code>: the name we want to give to the CLA (<code>name=\"x_pos\"</code>, <code>name=\"y_pos\"</code> etc...)</li> <li> <p><code>default</code>: A default value in order to make the CLA optional</p> <p>(In the above example, the robot will be located at coordinates <code>(0, 0)</code> by default, unless we specify otherwise.)</p> </li> </ol>"},{"location":"others/amr31001/","title":"Industry 4.0","text":"<p>As part of this module you will take part in two lab sessions in the Diamond, where you will learn about how ROS can be used to program and control robots. You'll do some Python programming and look at how sensor data can be used to control a robot's actions. </p> <ul> <li>Lab 1: Mobile Robotics </li> <li>Lab 2: Feedback Control</li> </ul>"},{"location":"others/amr31001/lab1/","title":"Lab 1: Mobile Robotics","text":"<p>Info</p> <p>You should be able to complete exercises 1-6 on this page within a two-hour lab session.</p>"},{"location":"others/amr31001/lab1/#introduction","title":"Introduction","text":"<p>In this first AMR31001 'Industry 4.0' ROS Lab you will learn how to use ROS (the Robot Operating System) to control a robot's motion.</p> <p>ROS is an open-source, industry-standard robot programming framework, used in a range of industries such as agriculture, warehouse and factory automation and advanced manufacturing (the robot arms at the AMRC, for instance, are programmed and controlled using ROS!) </p> <p>ROS allows us to programme robots using a range of different programming languages (including C++, Java, MATLAB etc.), but we'll be using Python for these labs. In addition to this, ROS runs on top of a Linux operating system called 'Ubuntu', and so we'll also learn a bit about how to use this too.</p> <p>We'll be working with robots called 'TurtleBot3 Waffles', which you can find out a bit more about here. </p> <p>Pre-Lab Work</p> <p>You must have completed the Pre-Lab Test before you can make a start on this lab. This is available on the AMR31001 Blackboard Course Page.</p>"},{"location":"others/amr31001/lab1/#aims","title":"Aims","text":"<p>In this lab you'll learn how to use ROS to make a robot move, and we'll also look at how to create our own basic ROS application (or 'Node'), using Python.</p>"},{"location":"others/amr31001/lab1/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Control a TurtleBot3 Waffle Robot, from a laptop, using ROS.</li> <li>Launch ROS applications on the laptop and the robot using <code>roslaunch</code> and <code>rosrun</code>.</li> <li>Interrogate a ROS network using ROS command-line tools.</li> <li>Use ROS Communication Methods to publish messages.</li> <li>Use a Linux operating system and work within a Linux Terminal.</li> </ol>"},{"location":"others/amr31001/lab1/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Launching ROS and Making the Robot Move</li> <li>Exercise 2: Seeing the Waffle's Sensors in Action!</li> <li>Exercise 3: Visualising the ROS Network</li> <li>Exercise 4: Exploring ROS Topics and Messages</li> <li>Exercise 5: Publishing Velocity Commands to the <code>/cmd_vel</code> Topic</li> <li>Exercise 6: Creating a Python node to make the robot move</li> <li>Exercise 7 (Advanced): Alternative Motion Paths</li> </ul>"},{"location":"others/amr31001/lab1/#the-lab","title":"The Lab","text":""},{"location":"others/amr31001/lab1/#getting-started","title":"Getting Started","text":"<p>Before you do anything, you'll need to get your robot up and running, and make sure ROS is launched.</p>"},{"location":"others/amr31001/lab1/#ex1","title":"Exercise 1: Launching ROS and Making the Robot Move","text":"<p>You should have already been provided with a Robot and a Laptop (in fact, you're probably already reading this on the laptop!) </p> <ol> <li> <p>First, identify the robot that you have been provided with. Each of our robots are uniquely named: <code>dia-waffleX</code>, where <code>X</code> is the 'Robot Number' (a number between 1 and 50). Check the label printed on top of the robot to find out which one you have!</p> </li> <li> <p>Open up a terminal instance on the laptop, either by pressing the Ctrl+Alt+T buttons on your keyboard all at the same time, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p> </p> <p>We'll refer to this terminal as TERMINAL 1.</p> </li> <li> <p>In TERMINAL 1 type the following command to pair the laptop and robot, so that they can work together:</p> <p>TERMINAL 1: <pre><code>waffle X pair\n</code></pre> ... replacing <code>X</code> with the number of the robot that you have been provided with.</p> </li> <li> <p>Enter the password for the robot when requested (we'll tell you what this is in the lab).</p> <p>You may see a message like this early on in the pairing process:</p> <p> </p> <p>If so, just type <code>yes</code> and then hit Enter to confirm that you want to continue.</p> </li> <li> <p>Once the pairing process is finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>Then, in the same terminal (TERMINAL 1), enter the following command:</p> <p>TERMINAL 1: <pre><code>waffle X term\n</code></pre> (again, replacing <code>X</code> with the number of your robot).</p> <p>Any text that was in the terminal should now disappear, and a green banner should appear across the bottom of the terminal window:</p> <p> </p> <p>This is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> <li> <p>Now, launch ROS on the robot by entering the following command:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_tb3_tools ros.launch\n</code></pre></p> <p>Tip</p> <p>To paste text into a Linux terminal you'll need to use the Control + Shift + V keyboard keys: Ctrl+Shift+V</p> <p>After a short while, you should see a message like this:</p> <pre><code>[INFO] [#####] Calibration End\ndia-waffleX is up and running!\n</code></pre> <p>ROS is now up and running on the robot, and we're ready to go!</p> <p>You should leave TERMINAL 1 alone now, just leave it running in the background for the rest of the lab.</p> </li> <li> <p>Next, open up a new terminal instance on the laptop (by pressing Ctrl+Alt+T or clicking the Terminal App desktop icon, as you did before). We'll call this one TERMINAL 2.</p> </li> <li> <p>In TERMINAL 2 enter the following command:</p> <p>TERMINAL 2: <pre><code>rosrun turtlebot3_teleop turtlebot3_teleop_key\n</code></pre></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on the keyboard:</p> <p> </p> </li> <li> <p>Enter Ctrl+C in TERMINAL 2 to stop the Teleop node when you've had enough fun.</p> </li> </ol>"},{"location":"others/amr31001/lab1/#packages-and-nodes","title":"Packages and Nodes","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations).  </p> <p>Scripts tell the robot what to do and how to act. In ROS, these scripts are called nodes. ROS Nodes are executable programs that perform specific robot tasks and operations. These are typically written in C++ or Python, but it's possible to write ROS Nodes using other programming languages too.</p> <p>In Exercise 1 you actually launched a whole range of different nodes on the ROS Network (the wireless link between your robot and laptop) using the following two commands: </p> <ol> <li><code>roslaunch tuos_tb3_tools ros.launch</code> (on the robot, in TERMINAL 1).</li> <li><code>rosrun turtlebot3_teleop turtlebot3_teleop_key</code> (on the laptop, in TERMINAL 2).</li> </ol> <p>The first of the above was a <code>roslaunch</code> command, which has the following two parts to it (after the <code>roslaunch</code> bit):</p> <pre><code>roslaunch {[1] Package name} {[2] Launch file}\n</code></pre> <p>Part [1] specifies the name of the ROS package containing the functionality that we want to execute. Part [2] is a file within that package that tells ROS exactly what scripts ('nodes') that we want to launch. We can launch multiple nodes at the same time from a single launch file.</p> <p>The second command was a <code>rosrun</code> command, which has a structure similar to <code>roslaunch</code>:</p> <pre><code>rosrun {[1] Package name} {[2] Node name}\n</code></pre> <p>Here, Part [1] is the same as the <code>roslaunch</code> command, but Part [2] is slightly different: <code>{[2] Node name}</code>. Here we are directly specifying a single script that we want to execute. We therefore use <code>rosrun</code> if we only want to launch a single node on the ROS network (<code>turtlebot3_teleop_key</code> in this case, which is a Python script).</p> <p>Post-lab Quiz</p> <p>What were the names of the two packages that we invoked in Exercise 1?</p>"},{"location":"others/amr31001/lab1/#ex2","title":"Exercise 2: Seeing the Waffle's Sensors in Action!","text":"<p>Our Waffles have some pretty sophisticated sensors on them, allowing them to \"see\" the world around them. We won't really make much use of these during this lab, but this next exercise will allow you to see how the data from these devices could be used to help our robots do some very advanced things (with some clever programming, of course!)</p>"},{"location":"others/amr31001/lab1/#part-1-the-camera","title":"Part 1: The Camera","text":"<ol> <li> <p>There shouldn't be anything running in TERMINAL 2 now, after you closed down the Teleop node at the end of the previous exercise (Ctrl+C). Return to this terminal and launch the <code>rqt_image_view</code> node:</p> <p>TERMINAL 2: <pre><code>rosrun rqt_image_view rqt_image_view\n</code></pre></p> <p>Post-lab Quiz</p> <ol> <li>We're using <code>rosrun</code> here again, what does this mean?</li> <li>Why did we have to type <code>rqt_image_view</code> twice?</li> </ol> </li> <li> <p>A new window should open. Maximise this (if it isn't already) and then select <code>/camera/color/image_raw</code> from the dropdown menu at the top-left of the application window.</p> </li> <li>Live images from the robot's camera should now be visible! Stick your face in front of the camera and see yourself appear on the laptop screen!</li> <li> <p>Close down the window once you've had enough. This should release TERMINAL 2 so that you can enter commands in it again.</p> <p>The camera on the robot is quite a clever device. Inside the unit is two separate image sensors, giving it - effectively - both a left and right eye. The device then combines the data from both of these sensors and uses the combined information to infer depth from the images as well. Let's have a look at that in action now...</p> </li> <li> <p>In TERMINAL 2 enter the following command:</p> <p>TERMINAL 2: <pre><code>roslaunch tuos_tb3_tools rviz.launch\n</code></pre></p> <p>This will launch an application called RViz, which is a handy tool that allows us to visualise the data from all the sensors on-board our robots. When RViz opens, you should see something similar to the following:</p> <p> </p> <p>The strange wobbly sheet of colours in front of the robot is the live image stream from the camera with depth applied to it at the same time. The camera is able to determine how far away each image pixel is from the camera lens, and then uses that to generate this 3-dimensional representation. Nice eh!</p> </li> <li> <p>Again, place your hand or your face in front of the camera and hold steady for a few seconds (there may be a bit of a lag as all of this data is transmitted over the WiFi network). You should see yourself rendered in 3D in front of the robot! </p> </li> </ol>"},{"location":"others/amr31001/lab1/#part-2-the-lidar-sensor","title":"Part 2: The LiDAR Sensor","text":"<p>In RViz you may have also noticed a lot of red dots scattered around the robot. This is a representation of the laser displacement data coming from the LiDAR sensor (the black device on the top of the robot). The LiDAR sensor spins continuously, sending out laser pulses into the environment as it does so. When a pulse hits an object it is reflected back to the sensor, and the time it takes for this to happen is used to calculate how far away the object is.</p> <p>The LiDAR sensor spins and performs this process continuously, so a full 360\u00b0 scan of the environment can be generated. This data is therefore really useful for things like obstacle avoidance and mapping. We'll have a quick look at the latter now.</p> <ol> <li> <p>Close down RViz (click the \"Close without saving\" button, if asked).</p> </li> <li> <p>Head back to TERMINAL 2 and run the following command:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch\n</code></pre></p> <p>A new RViz screen will open up, this time showing the robot from a top-down view, and with the LiDAR data represented by green dots instead.</p> <p> </p> <p>Underneath the green dots you should notice black lines forming. ROS is using a process called SLAM (Simultaneous Localisation and Mapping) to generate a map of the environment, using the data from the LiDAR sensor.</p> </li> <li> <p>Open up a new terminal instance now, we'll call this one TERMINAL 3. Launch the Teleop node in this one, as you did earlier:</p> <p>TERMINAL 3: <pre><code>rosrun turtlebot3_teleop turtlebot3_teleop_key\n</code></pre></p> </li> <li> <p>Drive the robot around a bit and watch how the map in RViz is updated as the robot explores new parts of the environment.</p> </li> <li> <p>Enter Ctrl+C in TERMINAL 3 and then close down this terminal window, we won't need it any more.</p> </li> <li> <p>Close down the RViz window, but keep TERMINAL 2 open for the next exercise...</p> </li> </ol> <p>We've now used both <code>roslaunch</code> and <code>rosrun</code> to launch ROS applications. These are both ROS command-line tools, and there are many others at our disposal as robotics engineers as well. </p> <p>Using <code>rosrun</code> and <code>roslaunch</code>, as we have done so far, it's easy to end up with a lot of different processes or ROS Nodes running on the network, some of which we will interact with, but others may just be running in the background. It is often useful to know exactly what is running on the ROS network, and there are a number of ways to do this.</p>"},{"location":"others/amr31001/lab1/#ex3","title":"Exercise 3: Visualising the ROS Network","text":"<ol> <li> <p>There shouldn't be anything running in TERMINAL 2 now, so return to this terminal and use the <code>rosnode</code> command to list the nodes that are currently running on the robot:</p> <p>TERMINAL 2: <pre><code>rosnode list\n</code></pre></p> <p>You should see a list of 7 items.</p> </li> <li> <p>We can visualise the connections between the active nodes by using a ROS node called <code>rqt_graph</code>. Launch this as follows:</p> <p>TERMINAL 2: <pre><code>rosrun rqt_graph rqt_graph\n</code></pre></p> </li> <li> <p>In the window that opens, select <code>Nodes/Topics (active)</code> from the dropdown menu in the top left. </p> <p>What you should then see is a map of all the nodes in the list from above (as ovals), and arrows to illustrate the flow of information between them. This is a visual representation of the ROS network!</p> <p>Items that have a rectangular border are ROS Topics. ROS Topics are essentially communication channels, and ROS nodes can read (subscribe) or write (publish) to these topics to access sensor data, pass information around the network and make things happen.</p> </li> </ol> <p>A ROS Robot could have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to communicate and share data with the other nodes on the ROS Network.</p>"},{"location":"others/amr31001/lab1/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics using ROS Messages. We were actually publishing messages to a topic when we made the robot move using the Teleop node in the previous exercises.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"others/amr31001/lab1/#ex4","title":"Exercise 4: Exploring ROS Topics and Messages","text":"<p>Much like the <code>rosnode list</code> command, we can use <code>rostopic list</code> to list all the topics that are currently active on the network.</p> <ol> <li> <p>Close down the <code>rqt_graph</code> window if you haven't done so already. This will release TERMINAL 2 so that we can enter commands in it again. Return to this terminal window and enter the following:</p> <p>TERMINAL 2: <pre><code>rostopic list\n</code></pre></p> <p>A much larger list of items should be printed to the terminal now. See if you can spot the <code>/cmd_vel</code> item in the list.</p> <p>This topic is used to control the velocity of the robot ('command velocity').</p> </li> <li> <p>Let's find out more about this using the <code>rostopic info</code> command.</p> <p>TERMINAL 2: <pre><code>rostopic info /cmd_vel\n</code></pre></p> <p>This should provide an output similar to the following: </p> <pre><code>Type: geometry_msgs/Twist\n\nPublishers: None\n\nSubscribers:\n * /turtlebot3_core (http://dia-waffleX:#####/)\n</code></pre> <p>This tells us a few things: </p> <ol> <li>The <code>/cmd_vel</code> topic currently has no publishers (i.e. no other nodes are currently writing data to this topic).</li> <li>The <code>/turtlebot3_core</code> node is subscribing to the topic. The <code>/turtlebot3_core</code> node turns motor commands into actual wheel motion, so it monitors the topic (i.e. subscribes to it) to see when a velocity command is published to it.</li> <li> <p>The type of message used by the <code>/cmd_vel</code> topic is called: <code>geometry_msgs/Twist</code>. </p> <p>The message type has two parts: <code>geometry_msgs</code> and <code>Twist</code>. <code>geometry_msgs</code> is the name of the ROS package that this message belongs to and <code>Twist</code> is the actual message type. </p> <p>We have just learnt then, that if we want to make the robot move we need to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic. </p> </li> </ol> </li> <li> <p>We can use the <code>rosmsg</code> command to find out more about the <code>Twist</code> message:</p> <p>TERMINAL 2: <pre><code>rosmsg info geometry_msgs/Twist\n</code></pre></p> <p>From this, we should obtain the following:</p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x\n  float64 y\n  float64 z\ngeometry_msgs/Vector3 angular\n  float64 x\n  float64 y\n  float64 z\n</code></pre> <p>Hmmm, this looks complicated. Let's find out what it all means...</p> </li> </ol>"},{"location":"others/amr31001/lab1/#velocity-control","title":"Velocity Control","text":"<p>The motion of any mobile robot can be defined in terms of its three principal axes: <code>X</code>, <code>Y</code> and <code>Z</code>. In the context of our TurtleBot3 Waffle, these axes (and the motion about them) are defined as follows:</p> <p></p> <p>In theory then, a robot can move linearly or angularly about any of these three axes, as shown by the arrows in the figure. That's six Degrees of Freedom (DOFs) in total, achieved based on a robot's design and the actuators it is equipped with. Take a look back at the <code>rosmsg info</code> output in TERMINAL 2. Hopefully it's a bit clearer now that these topic messages are formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x  &lt;-- Forwards (or Backwards)\n  float64 y  &lt;-- Left (or Right)\n  float64 z  &lt;-- Up (or Down)\ngeometry_msgs/Vector3 angular\n  float64 x  &lt;-- \"Roll\"\n  float64 y  &lt;-- \"Pitch\"\n  float64 z  &lt;-- \"Yaw\"\n</code></pre> <p>Our TurtleBot3 robot only has two motors, so it doesn't actually have six DOFs! These two motors can be controlled independently, which gives it what is called a \"differential drive\" configuration, but this still only allows it to move with two degrees of freedom in total, as illustrated below.</p> <p></p> <p>It can therefore only move linearly in the x-axis (Forwards/Backwards) and angularly in the z-axis (Yaw). </p> <p>Post-lab Quiz</p> <p>Take note of all this, there may be a question on it!</p>"},{"location":"others/amr31001/lab1/#ex5","title":"Exercise 5: Publishing Velocity Commands to the \"cmd_vel\" Topic","text":"<p>We will use the <code>rostopic</code> command differently now, and actually publish messages from the terminal to make the robot move. In order to do this we need three key bits of information:</p> <ol> <li>The name of the topic that we want to publish to.</li> <li>The type of message that this topic uses.</li> <li>The data format that this message type uses.</li> </ol> <p>Post-lab Quiz</p> <p>We discovered all this in the previous exercise, take note of all three points.</p> <p>We can then use <code>rostopic</code> with the <code>pub</code> option as follows:</p> <pre><code>rostopic pub {topic_name} {message_type} {data}\n</code></pre> <p>As we discovered earlier, the <code>/cmd_vel</code> topic is expecting linear and angular data, each with an <code>x</code>, <code>y</code> and <code>z</code> component to represent a maximum of six DOFs. We have also established however, that our robot only actually has two DOFs, so a number of the <code>Twist</code> message components won't actually have any effect on our robot at all.</p> <p>In any case, the message that we need to publish will end up being quite long because of all the message parameters that need to be provided (regardless of how relevant they are to our own robots). Fortunately, we don't have to type the whole thing out manually though, and we can use the autocomplete functionality of the Linux terminal to do most of the work for us.</p> <ol> <li> <p>Enter the following into TERMINAL 2 but where it says <code>[Space]</code> or <code>[Tab]</code> actually press the corresponding key on the keyboard:</p> <p>TERMINAL 2: <pre><code>rostopic pub /cmd_vel[Space][Tab][Tab]\n</code></pre></p> <p>The full message should then be presented to us:</p> <pre><code>rostopic pub /cmd_vel geometry_msgs/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre> </li> <li> <p>First, you need to add the following text to the end of the message: <code>--rate=10</code>.</p> </li> <li> <p>Then, you can scroll back through the message and edit any of the <code>linear</code>/<code>angular</code> <code>x</code>/<code>y</code>/<code>z</code> values as appropriate. Do this by pressing the \u2190 key on your keyboard, deleting a <code>0.0</code> where appropriate and replacing it with a value of your choosing. In its final format, a message might (for example) look like this:</p> <pre><code>rostopic pub /cmd_vel geometry_msgs/Twist \"linear:\n  x: 0.0\n  y: 101.1\n  z: 0.0\nangular:\n  x: 22.8\n  y: 0.0\n  z: 0.0\" --rate=10\n</code></pre> </li> <li> <p>Using what you learnt above about the way your robot can actually move, change one of the message parameter values in order to make the robot rotate on the spot. Before you do this, it's worth noting the following things:</p> <ol> <li>The unit of linear velocity is meters per second (m/s).</li> <li>The unit of angular velocity is radians per second (rad/s).</li> <li>Our TurtleBot3 robots can move with a maximum linear velocity of 0.26 m/s and a maximum angular velocity of 1.82 rad/s.</li> </ol> </li> <li> <p>Once you've edited the message hit <code>Enter</code> to publish this to the <code>/cmd_vel</code> topic and observe what your robot does!</p> </li> <li> <p>Enter Ctrl+C in TERMINAL 2 to stop the <code>rostopic pub</code> process (which will make the robot stop moving too).</p> </li> <li> <p>Next, find a velocity command that makes the robot move forwards. (Don't forget to press Ctrl+C afterwards.)</p> </li> <li> <p>Finally, enter a velocity command to make the robot move in a circle (you may need to change two parameter values here).</p> </li> <li> <p>Enter Ctrl+C in TERMINAL 2 to stop the robot.</p> </li> </ol>"},{"location":"others/amr31001/lab1/#ex6","title":"Exercise 6: Creating a Python node to make the robot move","text":"<p>Hopefully you can see now that, in order to make a robot move, it's simply a case of publishing the right ROS Message (<code>Twist</code>) to the right ROS Topic (<code>/cmd_vel</code>). Earlier on in the lab we used the Keyboard Teleop node to drive the robot around, a bit like a remote control car. In the background here all that was really happening was that the Teleop node was converting our keyboard button presses into velocity commands and publishing these to the <code>/cmd_vel</code> topic. In the previous exercise we looked at this in more detail by actually formatting the raw <code>Twist</code> messages ourselves, and publishing these to <code>/cmd_vel</code>, directly from the command-line. This approach was very manual though, and there's a limit to what we can really achieve by working in this way (circular and straight line motion is about it!)</p> <p>In reality, robots need to be able to move around complex environments autonomously, which is quite a difficult task, and requires us to build bespoke applications. We can build these applications using Python, and we'll look at the core concepts behind this now, building a simple Node that will allow us to make our robot a bit more \"autonomous\". What we will do here forms the basis of the more complex approaches used by robotics engineers to really bring robots to life!</p> <ol> <li> <p>First, create your own folder on the laptop to store your Python script(s) in. A folder is also known as a \"directory\", and we can make a new one from the command-line by using the <code>mkdir</code> (\"make directory\") command:</p> <p>TERMINAL 2: <pre><code>mkdir -p ~/amr31001/your_name \n</code></pre> Replacing <code>your_name</code> with, well... your name!</p> <p>Tip</p> <p>Don't use any spaces in your name, use underscores (<code>_</code>) instead!</p> </li> <li> <p>Next, navigate into this directory using the <code>cd</code> (\"change directory\") command:</p> <p>TERMINAL 2: <pre><code>cd ~/amr31001/your_name/ \n</code></pre> Again, replacing <code>your_name</code> accordingly.</p> </li> <li> <p>Then create a Python file called <code>move_square.py</code> using the <code>touch</code> command:</p> <p>TERMINAL 2: <pre><code>touch move_square.py\n</code></pre></p> </li> <li> <p>Now we want to edit it, and we'll do that using Visual Studio Code (VS Code):</p> <p>TERMINAL 2: <pre><code>code .\n</code></pre></p> <p>Note</p> <p>Don't forget to include the <code>.</code>, it's important!!</p> </li> <li> <p>Once VS Code launches, open up your <code>move_square.py</code> file, which should be visible in the file explorer on the left-hand side of the VS Code window. Paste the following content into the file:</p> move_square.py<pre><code>import rospy # (1)!\nfrom geometry_msgs.msg import Twist # (2)!\nfrom math import sqrt, pow, pi # (15)!\n\nmovement = \"state1\" # \"state2, state3 etc...\"\ntransition = True\n\nrospy.init_node(\"move_waffle\", anonymous=True) # (3)!\nrate = rospy.Rate(10) # (4)!\n\npub = rospy.Publisher('/cmd_vel', Twist, queue_size=10) # (5)!\nvel = Twist() # (6)!\n\nrospy.loginfo(f\"The node has been initialised...\")\ntimestamp = rospy.get_time() # (7)!\n\nwhile not rospy.is_shutdown(): # (8)!\n    elapsed_time = rospy.get_time() - timestamp # (9)!\n    if transition: # (10)!\n        timestamp = rospy.get_time()\n        transition = False\n        vel.linear.x = 0.0\n        vel.angular.z = 0.0\n        print(f\"Moving to state: {movement}\")\n    elif movement == \"state1\": # (11)!\n        if elapsed_time &gt; 2:\n            movement = \"state2\"\n            transition = True\n        else:\n            vel.linear.x = 0.05\n            vel.angular.z = 0.0\n    elif movement == \"state2\": # (12)!\n        if elapsed_time &gt; 4:\n            movement = \"state1\"\n            transition = True\n        else:\n            vel.angular.z = 0.2\n            vel.linear.x = 0.0\n    pub.publish(vel) # (13)!\n    rate.sleep() # (14)!\n</code></pre> <ol> <li><code>rospy</code> is the ROS client library for Python. We need this so that our Python node can interact with ROS.</li> <li>We know from earlier that in order to make a robot move we need to publish messages to the <code>/cmd_vel</code> topic, and that this topic uses <code>Twist</code> messages from the <code>geometry_msgs</code> package. This is how we import that message, from that package, in order to create velocity commands in Python (which we'll get to shortly...)</li> <li>Before we do anything we need to initialise our node to register it on the ROS network with a name. We're calling it \"move_waffle\" in this case, and we're using <code>anonymous=True</code> to ensure that there are no other nodes of the same name already registered on the network.</li> <li>We want our main <code>while</code> loop (when we get to that bit) to execute 10 times per second (10 Hz), so we create this <code>rate</code> object here which we'll use to control this later...</li> <li>Here we are setting up a publisher to the <code>/cmd_vel</code> topic so that the node can write <code>Twist</code> messages to make the robot move.</li> <li>We're instantiating a <code>Twist</code> message here and calling it <code>vel</code> (we'll assign velocity values to this in the <code>while</code> loop later on). A <code>Twist</code> message contains six different components that we can assign values to. Any idea what these six values might represent?  </li> <li>What time is it right now? (This will be useful to compare against in the while loop.)</li> <li>We're entering the main <code>while</code> loop now. This <code>rospy.is_shutdown()</code> function will read <code>False</code> unless we request for the node to be stopped (by pressing Ctrl+C in the terminal). Once it turns <code>True</code> the <code>while</code> loop stops.</li> <li>Here we're comparing the time now to the time the last time we checked, to tell us how much time has elapsed (in seconds) since then. We'll use that information to decide what to do...  </li> <li>The \"transition\" state is used to stop the robot (if necessary), and check the time again.</li> <li>In \"state1\" we set velocities that will make the robot move forwards (linear-X velocity only). If the elapsed time is greater than 2 seconds however, we move on to \"state2\".</li> <li>In \"state2\" we set velocities that will make the robot turn on the spot (angular-Z velocity only). In this case, if the elapsed time is greater than 4 seconds, we move back to \"state1\".</li> <li>Regardless of what happens in the <code>if</code> statements above, we always publish a velocity command to the <code>/cmd_vel</code> topic here (i.e. every loop iteration).</li> <li>We created a <code>rate</code> object earlier, and we use this now to make sure that each iteration of this <code>while</code> loop takes exactly the right amount of time to maintain the rate of execution that we specified earlier (10 Hz).</li> <li> <p>Here we're importing some mathematical operators that you might find useful in your code for this exercise (or the next one!) </p> Mathematical Operation Python Implementation \\(\\sqrt{a+b}\\) <code>sqrt(a+b)</code> \\(a^{2}+(bc)^{3}\\) <code>pow(a, 2) + pow(b*c, 3)</code> \\(\\pi r^2\\) <code>pi * pow(r, 2)</code> </li> </ol> <p>Click on the  icons above to expand the code annotations. Read these carefully to ensure that you understand what's going on and how this code works.</p> </li> <li> <p>Now, go back to TERMINAL 2 and run the code.</p> <p>Note</p> <p>Make sure the robot is on the floor and has enough room to roam around before you do this!</p> <p>TERMINAL 2: <pre><code>python3 move_square.py\n</code></pre></p> <p>Observe what the robot does. When you've seen enough, enter <code>Ctrl+C</code> in TERMINAL 2 to stop the node from running, which should also stop the robot from moving.</p> </li> <li> <p>As the name may suggest, the aim here is to make the robot follow a square motion path. What you may have observed when you actually ran the code is that the robot doesn't actually do that! We're using a time-based approach to make the robot switch between two different states continuously: moving forwards and turning on the spot.</p> <p>Have a look at the code to work out how much time the robot will currently spend in each state.</p> </li> <li> <p>The aim here is to make the robot follow a 0.5m x 0.5m square motion path.  In order to properly achieve this you'll need to adjust the timings, or the robot's velocity, or both. Edit the code so that the robot actually follows a 0.5m x 0.5m square motion path!</p> </li> </ol>"},{"location":"others/amr31001/lab1/#ex7","title":"Exercise 7 (Advanced): Alternative Motion Paths","text":"<p>If you have time, why don't you have a go at this now...</p> <p>How could you adapt the code further to achieve some more interesting motion profiles?</p> <ol> <li> <p>First, make a copy of the <code>move_square.py</code> code using the <code>cp</code> command:</p> <p>TERMINAL 2: <pre><code>cp move_square.py move_alt.py\n</code></pre> Which will create a new version of the file called <code>move_alt.py</code></p> </li> <li> <p>See if you can modify the <code>move_alt.py</code> code to achieve either of the more complex motion profiles illustrated below.</p> <p> </p> <ol> <li>Profile (a): The robot needs to follow a figure-of-eight shaped path, where a linear and angular velocity command are set simultaneously to generate circular motion. Velocities will need to be defined in order to achieve a path diameter of 1m for each of the two loops. Having set the velocities appropriately, you'll then need to work out how long it would take the robot to complete each loop, so that you can determine when the robot should have got back to its starting point. At this point you'll need to change the turn direction, so that the robot switches from anti-clockwise to clockwise turning. </li> <li>Profile (b): The robot needs to start and end in the same position, but move through intermediate points 1-7, in sequence, to generate the stacked square profile as shown. Each of the two squares must be 1m x 1m in size, so you'll need to find the right velocity and duration pairs for moving forward and turning. You'll also need to change the turn direction once the robot reaches Point 3, and then again at Point 7!</li> </ol> </li> </ol>"},{"location":"others/amr31001/lab1/#wrapping-up","title":"Wrapping Up","text":"<p>Before you leave, please shut down your robot! Enter the following command in TERMINAL 2 to do so:</p> <p>TERMINAL 2: <pre><code>waffle X off\n</code></pre> ... again, replacing <code>X</code> with the number of the robot that you have been working with today.</p> <p>You'll need to enter <code>y</code> and then hit <code>Enter</code> to confirm this.</p> <p>Please then shut down the laptop, which you can do by clicking the battery icon in the top right of the desktop and selecting the \"Power Off / Log Out\" option in the drop-down menu.</p> <p></p> <p> <p>AMR31001 Lab 1 Complete! See you in the new year for Lab 2!</p> <p></p>"},{"location":"others/amr31001/lab2/","title":"Lab 2: Feedback Control","text":""},{"location":"others/amr31001/lab2/#introduction","title":"Introduction","text":"<p>In Lab 1 we explored how ROS works and how to bring a robot to life. Let's quickly recap the key points:</p> <p>ROS Nodes</p> <ul> <li>Are executable programs (Python, C++ scripts) that perform specific robot tasks and operations.</li> <li>Typically, there'll be many ROS Nodes running on a robot simultaneously in order to make it work.</li> <li>We can create our own Nodes on top of what's already running, to add extra functionality.</li> <li>You may recall that we created our own ROS Node in Python, to make our TurtleBot3 Waffle follow a square motion path.</li> </ul> <p></p> <p>Topics and Messages</p> <ul> <li>All the ROS Nodes running on a network can communicate and pass data between one another using a Publisher/Subscriber-based Communication Principle.</li> <li>ROS Topics are key to this - they are essentially the communication channels (or the plumbing) on which all data is passed around between the nodes.</li> <li>Different topics communicate different types of information.</li> <li>Any Node can publish (write) and/or subscribe to (read) any ROS Topic in order to pass information around or make things happen.</li> <li>One of the key ROS Topics that we worked with last time was <code>/cmd_vel</code>, which is a topic that communicates velocity commands to make a robot move.</li> <li>We published <code>Twist</code> messages to this (both via the command line, and in Python) to make our TurtleBot3 Waffle move.</li> </ul> <p></p> <p>Open-Loop Control</p> <p>We used a time-based method to control the motion of our robot in order to get it to generate a square motion path. This type of control is open-loop: we hoped that the robot had moved (or turned) by the amount that was required, but had no feedback to tell us whether this had actually been achieved.</p> <p>In this lab we'll look at how this can be improved, making use of some of our robot's on-board sensors to tell us where the robot is or what it can see in its environment, in order to complete a task more reliably and be able to better adapt to changes and uncertainty in the environment.</p>"},{"location":"others/amr31001/lab2/#aims","title":"Aims","text":"<p>In this lab, we'll build some ROS Nodes (in Python) that incorporate data from some of our robot's sensors. This sensor data is published to specific topics on the ROS Network, and we can build ROS Nodes to subscribe to these. We'll see how the data from these sensors can be used as feedback to inform decision-making, thus allowing us to implement some different forms of closed-loop control, thus making our robot more autonomous. </p>"},{"location":"others/amr31001/lab2/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Interpret the data from a ROS Robot's Odometry System and understand what this tells you about a Robot's position and orientation within its environment.</li> <li>Use feedback from a robot's odometry system to control its position in an environment.</li> <li>Use data from a Robot's LiDAR sensor to make a robot follow a wall.</li> <li>Generate a map of an environment, using SLAM.</li> <li>Make a robot navigate an environment autonomously, using ROS navigation tools.</li> </ol>"},{"location":"others/amr31001/lab2/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Exploring Odometry Data</li> <li>Exercise 2: Odometry-based Navigation</li> <li>Exercise 3: Wall following</li> <li>Exercise 4: SLAM and Autonomous Navigation</li> </ul>"},{"location":"others/amr31001/lab2/#the-lab","title":"The Lab","text":""},{"location":"others/amr31001/lab2/#getting-started","title":"Getting Started","text":""},{"location":"others/amr31001/lab2/#downloading-the-amr31001-ros-package","title":"Downloading the AMR31001 ROS Package","text":"<p>To start with, you'll need to download a ROS package to the Robot Laptop that you are working on today. This package contains all the resources that you'll need for the lab exercises.</p> <ol> <li> <p>Open up a terminal instance on the laptop, either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p> </p> </li> <li> <p>In the terminal, run the following commands in order:</p> <p>Tip</p> <p>To paste the following commands into the terminal use Ctrl+Shift+V</p> <p><pre><code>wget -O build.sh https://raw.githubusercontent.com/tom-howard/amr31001/main/scripts/build.sh\n</code></pre> <pre><code>chmod +x build.sh\n</code></pre> <pre><code>./build.sh\n</code></pre></p> </li> </ol>"},{"location":"others/amr31001/lab2/#launching-ros","title":"Launching ROS","text":"<p>Much the same as last time, you'll now need to get ROS up and running on your robot. </p> <ol> <li> <p>First, identify the number of the robot that you have been provided with.</p> <p>Robots are named: <code>dia-waffleNUM</code>, where <code>NUM</code> is a unique 'Robot Number' (a number between 1 and 50).</p> </li> <li> <p>In the terminal, type the following command to pair the laptop and robot:</p> <p><pre><code>waffle NUM pair\n</code></pre> Replacing <code>NUM</code> with the number of the robot that you have been provided with.</p> </li> <li> <p>You may see a message like this early on in the pairing process:</p> <p> </p> <p>If so, just type <code>yes</code> and then hit Enter to confirm that you want to continue.</p> </li> <li> <p>Enter the password for the robot when requested (if you can't remember what this is from last time then ask a member of the teaching team!)</p> <p>Remember</p> <p>You won't see anything change on the screen when you are entering the password. This is normal, just keep typing!!</p> </li> <li> <p>Once the pairing process is finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>In the same terminal, enter the following command:</p> <p><pre><code>waffle NUM term\n</code></pre> (again, replacing <code>NUM</code> with the number of your robot).</p> <p>A green banner should appear across the bottom of the terminal window:</p> <p> </p> <p>Remember, this is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> <li> <p>Now, launch ROS on the robot by entering the following command:</p> <pre><code>roslaunch tuos_tb3_tools ros.launch\n</code></pre> <p>After a short while, you should see a message like this:</p> <pre><code>[INFO] [#####] --------------------------\n[INFO] [#####] dia-waffleNUM is up and running!\n[INFO] [#####] -------------------------- \n</code></pre> <p>The robot is now up and running, and you're ready to go!</p> </li> <li> <p>Close down this terminal instance. If you see the following message, just click \"Close Terminal.\"</p> <p> </p> </li> </ol>"},{"location":"others/amr31001/lab2/#odometry","title":"Odometry","text":"<p>First, let's look at our robot's odometry system, and what this is useful for.</p> <p>Odometry is the use of data from motion sensors to estimate change in position over time. It is used in robotics by some legged or wheeled robots to estimate their position relative to a starting location. <sup>1</sup></p> <p>Our robot can therefore keep track of its position (and orientation) as it moves around. It does this using data from two sources:</p> <ol> <li>Wheel encoders: Our robot has two wheels, each is equipped with an encoder that measures the number of rotations that the wheel makes. </li> <li>An Inertial Measurement Unit (IMU): Using accelerometers, gyroscopes and compasses, the IMU can monitor the linear and angular velocity of the robot, and which direction it is heading, at all times.</li> </ol> <p>This data is published to a ROS Topic called <code>/odom</code>. </p>"},{"location":"others/amr31001/lab2/#ex1","title":"Exercise 1: Exploring Odometry Data","text":"<p>In the previous lab we used some ROS commands to identify and interrogate active topics on the ROS network, let's give that another go now, but on the <code>/odom</code> topic this time.</p> <ol> <li> <p>Open up a new terminal instance on the laptop (by pressing Ctrl+Alt+T, or clicking the Terminal App desktop icon, as you did before). We\u2019ll call this one TERMINAL 1.</p> </li> <li> <p>As you may recall from last time, we can use the <code>rostopic</code> command to list all the topics that are currently active on the network. Enter the following in TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>rostopic list\n</code></pre></p> <p>A large list of items should appear on the screen. Can you spot the <code>/odom</code> topic?</p> </li> <li> <p>Let's find out more about this using the <code>rostopic info</code> command.</p> <p>TERMINAL 1: <pre><code>rostopic info /odom\n</code></pre></p> <p>This should provide the following output:</p> <pre><code>Type: nav_msgs/Odometry\n\nPublishers:\n  * /turtlebot3_core (http://dia-waffleNUM:#####/)\n\nSubscribers: None\n</code></pre> <p>Post-lab Quiz</p> <p>What does all this mean? We discussed this last time (in relation to the <code>/cmd_vel</code> topic), and you may want to have a look back at this to refresh your memory! </p> <p>One of the key things that this does tell us is that the <code>/odom</code> topic transmits data using a <code>nav_msgs/Odometry</code> message. All topics use standard message types to pass information around the ROS network. This is so that any node on the ROS network knows how to deal with the data, if it needs to. <code>nav_msgs/Odometry</code> is one of these standard message types. </p> </li> <li> <p>We can use the <code>rosmsg</code> command to find out more about this:</p> <p>TERMINAL 1: <pre><code>rosmsg info nav_msgs/Odometry\n</code></pre></p> <p>You'll see a lot of information there, but try to find the line that reads <code>geometry_msgs/Pose pose</code>: </p> <pre><code>geometry_msgs/Pose pose\n  geometry_msgs/Point position\n    float64 x\n    float64 y\n    float64 z\n  geometry_msgs/Quaternion orientation\n    float64 x\n    float64 y\n    float64 z\n    float64 w\n</code></pre> <p>Here's where we'll find information about the robot's position and orientation (aka \"Pose\") in the environment. Let's have a look at this data in real time...</p> </li> <li> <p>We can look at the live data being streamed across the <code>/odom</code> topic, using the <code>rostopic echo</code> command. We know that this topic uses <code>nav_msgs/Odometry</code> type messages, and we know which part of these messages we are interested in (<code>geometry_msgs/Pose pose</code>)</p> <p>TERMINAL 1: <pre><code>rostopic echo /odom/pose/pose\n</code></pre></p> </li> <li> <p>Now, let's drive the robot around a bit and see how this data changes as we do so. Open up a new terminal instance by pressing Ctrl+Alt+T, or clicking the Terminal App desktop icon, as you did before. We'll call this one TERMINAL 2.</p> </li> <li> <p>Remember that node that we used last time that allowed us to control the motion of the robot using different buttons on the keyboard? Let's launch that again now:</p> <p>TERMINAL 2: <pre><code>rosrun turtlebot3_teleop turtlebot3_teleop_key\n</code></pre></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around:</p> <p>As you're doing this, look at how the <code>position</code> and <code>orientation</code> data is changing in TERMINAL 1, in real-time!</p> <p>Post-lab Quiz</p> <p>Which position and orientation values change (by a significant amount) when:</p> <ol> <li>The robot turns on the spot (i.e. only an angular velocity is applied)?</li> <li>The robot moves forwards (i.e. only a linear velocity is applied)?</li> <li>The robot moves in a circle (i.e. both a linear and angular velocity are applied simultaneously)?</li> </ol> <p>Make a note of the answers to these questions, as they may feature in the post-lab quiz!</p> </li> <li> <p>When you've seen enough enter Ctrl+C in TERMINAL 2 to stop the <code>turtlebot3_teleop_keyboard</code> node. Then, enter Ctrl+C in TERMINAL 1 as well, which will stop the live stream of Odometery messages from being displayed.</p> </li> </ol>"},{"location":"others/amr31001/lab2/#summary","title":"Summary","text":"<p>Pose is a combination of a robot's position and orientation in its environment.</p> <p>Position tells us the location (in meters) of the robot in its environment. Wherever the robot was when it was turned on is the reference point, and so the distance values that we observed in the exercise above were all quoted relative to this initial position.</p> <p>You should have noticed that (as the robot moved around) the <code>x</code> and <code>y</code> terms changed, but the <code>z</code> term should have remained at zero. This is because the <code>X-Y</code> plane is the floor, and any change in <code>z</code> position would mean that the robot was floating or flying above the floor! </p> <p>Orientation tells us where the robot is pointing in its environment, expressed in units of Quaternions; a four-term orientation system. You should have noticed some of these values changing too, but it may not have been immediately obvious what the values really meant! For the further exercises in this lab we'll convert this to Euler angles (in degrees/radians) for you, to make the data a bit easier to understand.</p> <p>Ultimately though, our robots position can change in both the <code>X</code> and <code>Y</code> axes (i.e. the plane of the floor), while its orientation can only change about the <code>Z</code> axis (i.e. it can only \"yaw\"): </p> <p></p>"},{"location":"others/amr31001/lab2/#ex2","title":"Exercise 2: Odometry-based Navigation","text":"<p>Now that we know about the odometry system and what it tells us, let's see how this could be used as a feedback signal to inform robot navigation. You may recall that last time you created a ROS Node to make your robot to follow a square motion path on the floor. This was time-based though: given the speed of motion (turning or moving forwards) it was possible to determine the time it would take for the robot to move by a required distance. Having determined this, we then added timers to our node, to control the switch between moving forwards and turning on the spot, in order to generate the square motion path (approximately). </p> <p>In theory though, we can do all this much more effectively with odometry data instead, so let's have a go at that now...</p> <ol> <li> <p>Open up the <code>amr31001</code> ROS package that you downloaded earlier into VS Code using the following command in TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>code ~/catkin_ws/src/amr31001\n</code></pre></p> </li> <li> <p>In VS Code, navigate to the <code>src</code> directory in the File Explorer on the left-hand side, and click on the <code>ex2.py</code> file to display it in the editor.</p> </li> <li> <p>Have a look through the code and see if you can work out what's going on. There are a few things to be aware of:</p> <ol> <li> <p>Motion control is handled by an external Python module called <code>waffle</code>, which is imported on line 4:</p> <pre><code>import waffle\n</code></pre> <p>and instantiated on line 16:</p> <pre><code>motion = waffle.Motion()\n</code></pre> <p>In the main part of the code, this can be used to control the velocity of the robot, using the following methods:</p> <ol> <li><code>motion.move_at_velocity(linear = x, angular = y)</code> to make the robot move at a linear velocity of <code>x</code> (m/s) and/or an angular velocity of <code>y</code> (rad/s).</li> <li><code>motion.stop()</code> to make the robot stop moving.</li> </ol> </li> <li> <p>Subscribing to the <code>/odom</code> topic and the processing of the <code>nav_msgs/Odometry</code> data is also handled by the <code>waffle</code> module, so you don't have to worry about it! This functionality is instantiated on line 17:</p> <pre><code>pose = waffle.Pose()\n</code></pre> <p>So all that you have to do in order to access the robot's odometry data in the main part of the code is call the appropriate attribute:</p> <ol> <li><code>pose.posx</code> to obtain the robot's current position (in meters) in the <code>X</code> axis.</li> <li><code>pose.posy</code> to obtain the robot's current position (in meters) in the <code>Y</code> axis.</li> <li><code>pose.yaw</code> to obtain the robot's current orientation (in degrees) about the <code>Z</code> axis.</li> </ol> </li> </ol> </li> <li> <p>Run the code in TERMINAL 1 and observe what happens:</p> <p>TERMINAL 1: <pre><code>rosrun amr31001 ex2.py\n</code></pre></p> <p>The robot should start turning on the spot, and you should see some interesting information being printed to the terminal. After it has turned by 45\u00b0 the robot should stop. </p> </li> <li> <p>Stop the Node by entering Ctrl+C in TERMINAL 1 and then run it again (<code>rosrun amr31001 ex2.py</code>) if you missed what happened the first time!</p> </li> <li> <p>What you need to do:</p> <ol> <li> <p>In the <code>while()</code> loop there is an <code>if</code> statement with a condition that handles the turning process: </p> <pre><code>elif movement == \"turn\":\n</code></pre> <p>Within this, look at how the robot's yaw angle is being monitored and updated as it turns. Then, look at how the turn angle is being controlled. See if you can adapt this to make the robot turn in 90\u00b0 steps instead.</p> </li> <li> <p>Ultimately, after the robot has turned by 90\u00b0 it needs to then move forwards by 0.5m, in order to achieve a 0.5x0.5m square motion path.</p> <p>Moving forwards is handled by an additional condition within the <code>if</code> statement:</p> <p><pre><code>elif movement == \"move_fwd\":\n</code></pre> See if you can adapt the code within this block to make the robot move forwards by the required amount (0.5 meters) in between each turn. </p> Hint <p>Consider how the turn angle is monitored and updated whist turning (<code>current_yaw</code>), and take a similar approach with the linear displacement (<code>current_distance</code>). Bear in mind that you'll need to consider the euclidean distance, which you'll need to calculate based on the robot's position in both the <code>x</code> and <code>y</code> axis.</p> <p> </p> </li> <li> <p>Make sure that you've saved any changes to the code (in VS Code) before trying to test it out on the robot!</p> <p>Do this by using the Ctrl+S keyboard shortcut, or going to <code>File &gt; Save</code> from the menu at the top of the screen.</p> </li> <li> <p>Once you've saved it, you can re-run the code at any time by using the same <code>rosrun</code> command as before:</p> <p>TERMINAL 1: <pre><code>rosrun amr31001 ex2.py\n</code></pre></p> <p>... and you can stop it at any time by entering Ctrl+C in the terminal.</p> Python Tips <p>You'll need to do a bit of maths here (see the \"Hint\" above). Here's how to implement a couple of mathematical functions in Python:</p> <ol> <li> <p>To the power of...: Use <code>**</code> to raise a number to the power of another number (i.e. \\(2^{3}\\)):</p> <pre><code>&gt;&gt;&gt; 2**3\n8\n</code></pre> </li> <li> <p>Square Root: To calculate the square root of a number (i.e. \\(\\sqrt{4}\\)):</p> <pre><code>&gt;&gt;&gt; sqrt(4)\n2.0 \n</code></pre> </li> </ol> </li> </ol> <p>Post-lab Quiz</p> <p>Keep a record of what you do here to complete this programming task, you may be asked about it in the post-lab quiz.</p> </li> </ol>"},{"location":"others/amr31001/lab2/#the-lidar-sensor","title":"The LiDAR Sensor","text":"<p>As you'll know, the black spinning device on the top of your robot is a LiDAR Sensor. As discussed previously, this sensor uses laser pulses to measure the distance to nearby objects. The sensor spins continuously so that it can fire these laser pulses through a full 360\u00b0 arc, and generate a full 2-dimensional map of the robot's surroundings.</p> <p>This data is published to a ROS Topic called <code>/scan</code>. Use the same methods that you used in Exercise 1 to find out what message type is used by this ROS Topic.</p> <p>Post-lab Quiz</p> <p>Make a note of this, there'll be a post-lab quiz question on it!</p> <p>Launch RViz, so that we can see the data coming from this sensor in real-time:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_tb3_tools rviz.launch\n</code></pre></p> <p></p> <p>The red dots illustrate the LiDAR data. Hold your hand out to the robot and see if you can see it being detected by the sensor... a cluster of red dots should form on the screen to indicate where your hand is located in relation to the robot. Move your hand around and watch the cluster of dots move accordingly. Move your hand closer and farther away from the robot and observe how the red dots also move towards or away from the robot on the screen. </p> <p>This data is really useful and (as we observed during the previous lab session) it allows us to build up 2-dimensional maps of an environment with considerable accuracy. This is, of course, a very valuable skill for a robot to have if we want it to be able to navigate autonomously, and we'll explore this further later on. For now though, we'll look at how we can use the LiDAR data ourselves to build Nodes that make the robot detect and follow walls!</p> <p>Once you're done, close down RViz by hitting Ctrl+C in TERMINAL 1. </p>"},{"location":"others/amr31001/lab2/#ex3","title":"Exercise 3: Wall following","text":"<ol> <li> <p>In VS Code, click on the <code>ex3.py</code> file in the File Explorer to display it in the editor.</p> </li> <li> <p>Have a look through the code and see if you can work out what's going on. Here's a few points to start with:</p> <ol> <li> <p>Velocity control is handled in the same way as in the previous exercise:</p> <ol> <li><code>motion.move_at_velocity(linear = x, angular = y)</code> to make the robot move at a linear velocity of <code>x</code> (m/s) and/or an angular velocity of <code>y</code> (rad/s).</li> <li><code>motion.stop()</code> to make the robot stop moving.</li> </ol> </li> <li> <p>The data from the LiDAR sensor has been preprocessed and encapsulated in an additional class from the <code>waffle</code> module. This functionality is instantiated on line 13:</p> <pre><code>lidar = waffle.Lidar()\n</code></pre> <p>This class splits up data from the LiDAR sensor into a number of different segments to focus on a number of distinct zones around the robot's body (to make the data a bit easier to deal with). For each of the segments (as shown in the figure below) a single distance value can be obtained, which represents the average distance to any object(s) within that particular angular zone:</p> <p> </p> <p>In the code, we can obtain the distance measurement (in meters) from each of the above zones as follows:</p> <ol> <li><code>lidar.distance.front</code> to obtain the average distance to any object(s) in front of the robot (within the frontal zone).</li> <li><code>lidar.distance.l1</code> to obtain the average distance to any object(s) located within LiDAR zone L1.</li> <li><code>lidar.distance.r1</code> to obtain the average distance to any object(s) located within LiDAR zone R1.     and so on...</li> </ol> </li> <li> <p>The code template has been developed to detect a wall on the robot's left-hand side.</p> <ol> <li>We use distance measurements from LiDAR zones L3 and L4 to determine the alignment of the robot to a left-hand wall.</li> <li> <p>This is determined by calculating the difference between the distance measurements reported from these two zones:</p> <pre><code>wall_rate = lidar.distance.l3 - lidar.distance.l4\n</code></pre> </li> <li> <p>If this value is close to zero, then the robot and the wall are well aligned. If not, then the robot is at an angle to the wall, and it needs to adjust its angular velocity in order to correct for this:</p> <p> </p> </li> </ol> </li> </ol> </li> <li> <p>Run the node, as it is, from TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>rosrun amr31001 ex3.py\n</code></pre></p> <p>When you do this, you'll notice that the robot doesn't move at all (yet!), but the following data appears in the terminal:</p> <ol> <li>The distance measurements from each of the LiDAR zones.</li> <li>The current value of the <code>wall_rate</code> parameter, i.e. how well aligned the robot currently is to a wall on its left-hand side.</li> <li>The decision that has been made by the <code>if</code> statement on the appropriate action that should be taken, given the current value of <code>wall_rate</code>.</li> </ol> </li> <li> <p>What you need to do:</p> <ol> <li>First, place the robot on the floor with a wall on its left-hand side</li> <li> <p>Manually vary the alignment of the robot and the wall and observe how the information that is being printed to the terminal changes as you do so.</p> <p>Question</p> <p>The node will tell you if it thinks the robot needs to turn right or left in order to improve its current alignment with the wall. Is it making the correct decision?</p> </li> <li> <p>Currently, all velocity parameters inside the <code>while()</code> loop are set to zero.</p> <ul> <li> <p>You'll need to set a constant linear velocity, so that the robot is always moving forwards. Set an appropriate value for this now, by editing the line that currently reads:</p> <pre><code>lin_vel = 0.0\n</code></pre> </li> <li> <p>The angular velocity of the robot will need to be adjusted conditionally, in order to ensure that the value of <code>wall_rate</code> is kept as low as possible at all times. Adjust the value of <code>ang_vel</code> in each of the <code>if</code> statement blocks so that this is achieved under each of the three possible scenarios.</p> <ol> <li>Hopefully, by following the steps above, you will get to the point where you can make the robot follow a wall reasonably well, as long as the wall remains reasonably straight! Consider what would happen however if the robot were faced with either of the following situations:</li> </ol> </li> </ul> <p> </p> <p>You may have already observed this during your testing... how could you adapt the code so that such situations can be achieved?</p> Hints <ol> <li>You may need to consider the distance measurements from some other LiDAR zones!</li> <li> <p>The <code>ex3.py</code> template that was provided to you uses an <code>if</code> statement with three different cases:</p> <pre><code>if ...:\n\nelif ...:\n\nelse:\n</code></pre> <p>You may need to add in some further cases to this to accommodate the additional situations discussed above, e.g.:</p> <pre><code>if ...:\n\nelif ...:\n\nelif ...:\n\nelif ...:\n\nelse:\n</code></pre> </li> </ol> </li> <li> <p>Finally, think about how you could adapt this algorithm to make the robot follow a wall on its right-hand side instead.</p> </li> </ol> <p>Post-lab Quiz</p> <p>Keep a record of what you do here to complete this programming task, you may be asked about it in the post-lab quiz.  </p> </li> </ol>"},{"location":"others/amr31001/lab2/#slam-and-autonomous-navigation","title":"SLAM and Autonomous Navigation","text":"<p>We've played around with the data from both the LiDAR sensor and the robot's Odometry System now, so hopefully you now understand what these two systems can tell us about our robot and its environment, and how this information is very valuable for robotic applications.</p> <p>To illustrate this further, we'll now have a look at two extremely useful tools that are built into ROS, and that use the data from these two sensors alone to achieve powerful results!</p> <p>Simultaneous Localisation and Mapping (SLAM)</p> <p>As you now know, the LiDAR sensor gives us a full 360\u00b0 view of our robot's environment. As the robot moves around it starts to observe different features in the environment, or perhaps the same features that it has already seen, but at a different perspective. Using the LiDAR data in combination with our robots Pose and Velocity (as provided by the Odometry System), we can actually build up a comprehensive 2-dimensional map of an environment and keep track of where our robot is actually located within that map, at the same time. This is a process called SLAM, and you may remember that we had a go at this in the previous lab session.</p> <p>Navigation</p> <p>Having built a complete map (using SLAM) our robot then knows exactly what its environment looks like, and it can then use the map to work out how to get from one place to another on its own! </p> <p>In the next exercise, we'll have a go at this: first we need to build a map, then we can use that map to implement autonomous navigation!</p>"},{"location":"others/amr31001/lab2/#ex4","title":"Exercise 4: SLAM and Navigation","text":"<p>Step 1: Mapping</p> <ol> <li> <p>Make sure none of your code from the previous exercise is still running now, TERMINAL 1 should be idle, ready to accept some new instructions!</p> </li> <li> <p>Place your robot in the enclosure that we have created in the lab. </p> </li> <li> <p>Then, in TERMINAL 1, run the following command:</p> <p>TERMINAL 1: <pre><code>roslaunch amr31001 ex4_slam.launch\n</code></pre></p> <p>An RViz screen will open up showing the robot from a top-down view, with the LiDAR data represented by green dots.</p> <p> </p> <p>SLAM is already working, and you should notice black lines forming underneath the green LiDAR dots to indicate the regions that SLAM has already determined as static and which therefore represent boundaries in the environment.</p> </li> <li> <p>In TERMINAL 2 launch the <code>turtlebot3_teleop_keyboard</code> node again, to drive the robot around:</p> <p>TERMINAL 2: <pre><code>rosrun turtlebot3_teleop turtlebot3_teleop_key\n</code></pre></p> </li> <li> <p>Drive the robot around until SLAM has built up a complete map of the entire arena.</p> <p> </p> </li> <li> <p>Once you're happy with this, stop the <code>turtlebot3_teleop_keyboard</code> node by hitting Ctrl+C in TERMINAL 2. You can also stop SLAM now too, so head back to TERMINAL 1 and enter Ctrl+C to stop this too.</p> </li> <li> <p>While you were doing the above, a map file was being constantly updated and saved to the laptop's filesystem. Have a quick look at this now to make sure that the map that you have built is indeed a good representation of the environment:</p> <p>TERMINAL 1: <pre><code>eog ~/catkin_ws/src/amr31001/maps/my_map.pgm\n</code></pre></p> <p>If the map looks like the actual arena then you're good to move on, but if not then you may need to repeat the above steps again to give it another go.</p> </li> </ol> <p>Step 2: Navigating Autonomously</p> <p>Using the map that you've just created you should now be able to make your robot navigate around the real environment!</p> <ol> <li> <p>Launch the navigation processes in TERMINAL 1 by entering the following command:</p> <p>TERMINAL 1: <pre><code>roslaunch amr31001 ex4_nav.launch\n</code></pre></p> <p>RViz should then open up again, once again with a top-down view of the robot, but this time with the map that you generated earlier displayed underneath it (in black).  </p> </li> <li> <p>To begin with, the robot needs to know roughly where it is within this map, and we can tell it this by performing a manual \"2D Pose Estimation\".  </p> <ol> <li>Press the \"2D Pose Estimate\" button at the top of the RViz screen.  </li> <li>The map should be visible (in black) in the background underneath all the live LiDAR Data (displayed in green), but the two are probably not lined up properly. Move the cursor to the approximate point on the background map at which the robot is actually located.  </li> <li>Press and hold the left mouse button and a large green arrow will appear.  </li> <li>Whilst still holding down the left mouse button, rotate the green arrow to set the robot's orientation within the map.  </li> <li> <p>Let go of the left mouse button to set the pose estimation. If done correctly, the real-time LiDAR data should nicely overlap the background map after doing this:</p> <p> </p> <p>... if not, then have another go!</p> </li> </ol> </li> <li> <p>We then need to give the robot a chance to determine its pose more accurately. The navigation processes that are running in the background are currently generating what's called a \"localisation particle cloud\", which is displayed as lots of tiny green arrows surrounding the robot in RViz. The scatter of these green arrows indicates the level of certainty the robot currently has about its true pose (position and orientation) within the environment: a large scatter indicates a high level of uncertainty, and we need to improve this before we can make the robot navigate around autonomously.</p> <p>We can improve this by simply driving the robot around the environment a bit, so that it gets the opportunity to see different walls and obstacles from different perspectives. In the background, the navigation algorithms will be comparing the real time LiDAR data with the background map that was generated earlier, and when the two things line up well, the robot becomes more certain about its pose in the environment.</p> <p>Once again, launch the <code>turtlebot_teleop_keyboard</code> node in TERMINAL 2, and drive the robot around a bit:</p> <p>TERMINAL 2: <pre><code>rosrun turtlebot3_teleop turtlebot3_teleop_key\n</code></pre></p> <p>As you're doing this, watch how the scatter in the localisation particle cloud reduces, and the small green arrows begin to converge underneath the robot.</p> <p> </p> </li> <li> <p>Now, click the \"2D Nav Goal\" button:  </p> <ol> <li>Move the cursor to the location that you want the robot to move to on the map. </li> <li>Click and hold the left mouse button and a large green arrow will appear again to indicate that the position goal has been set.  </li> <li>Whilst still holding the left mouse button, rotate the green arrow around to set the desired orientation goal.  </li> <li> <p>Release the mouse button to set the goal...</p> <p>The robot will then try its best to navigate to the destination autonomously!</p> </li> </ol> <p>Post-lab Quiz</p> <p>What does a ROS robot need in order to be able to navigate an environment autonomously?</p> </li> </ol>"},{"location":"others/amr31001/lab2/#wrapping-up","title":"Wrapping Up","text":"<p>Before you leave, please turn off your robot! Enter the following command in TERMINAL 1 to do so:</p> <p>TERMINAL 1: <pre><code>waffle NUM off\n</code></pre> ... again, replacing <code>NUM</code> with the number of the robot that you have been working with today.</p> <p>You'll need to enter <code>y</code> and then hit Enter to confirm this.</p> <p>Please then shut down the laptop, which you can do by clicking the battery icon in the top right of the desktop and selecting the \"Power Off / Log Out\" option in the drop-down menu.</p> <p></p> <p> <p>AMR31001 Lab 2 Complete! </p> <p></p> <ol> <li> <p>https://en.wikipedia.org/wiki/Odometry\u00a0\u21a9</p> </li> </ol>"},{"location":"software/","title":"Accessing ROS for this Course","text":"<p>In order to engage with the COM2009 Course you'll need access to a ROS environment. As discussed here, the course is designed around the latest (and final) version of ROS 1: Noetic. </p> <p>ROS is a bit difficult to install, and is only really available on a small selection of Linux platforms (as listed here), the most common choice being Ubuntu 20.04. It is also possible to install it on Windows 10 or 11 via the Windows Subsystem for Linux (WSL). In order to install or access ROS for this course, we recommend one of the following options. Click on the relevant link below to access the associated instructions:</p>"},{"location":"software/#wsl-ros-highly-recommended","title":"WSL-ROS (highly recommended)","text":"<p>We've created our own custom ROS (Noetic) and Ubuntu (20.04) environment for WSL specifically for this course, which we call \"WSL-ROS\". The environment contains all the tools and ROS packages that you will need for COM2009. This is our recommended option, and there are two ways to access this: </p> <ol> <li>Install it on your own Windows 10 or 11 machine via the IT Services Software Download Service.</li> <li>Access it on a range of managed desktop computers across the University campus.</li> </ol>"},{"location":"software/#manually-install-ros-and-all-required-dependencies","title":"Manually Install ROS and all Required Dependencies","text":"<p>If you have access to an Ubuntu 20.04 installation (either running natively on a personal computer or via WSL) then you can install all the necessary software manually, and we have instructions for this too:</p> <ol> <li>Windows 10 or 11: Installing ROS in an Ubuntu 20.04 WSL distro.</li> <li>Native Linux Install: Installing ROS and all necessary dependencies on Ubuntu 20.04.</li> </ol> <p>Mac Users</p> <p>Unfortunately we aren't currently aware of a reliable way to install ROS Noetic on a Mac, and we'd therefore recommend that you access WSL-ROS on a University Campus machine.</p> <p>If you do come across a workable solution though then please let us know! </p>"},{"location":"software/manual/","title":"Manually Installing ROS","text":"<p>Applicable to: More advanced users who are already familiar with Linux and/or WSL and have access to one or the other environment already.</p> <ul> <li>Installing ROS on Windows (using WSL)</li> <li>Installing ROS on Linux (Ubuntu 20.04)</li> </ul>"},{"location":"software/manual/linux/","title":"Installing ROS on Linux (Ubuntu 20.04)","text":"<p>Applicable to: Those who have access to a computer running Ubuntu 20.04 (either natively or via WSL).</p> <p>If you have Ubuntu 20.04 running on a machine then follow the steps below to install ROS and all the additional packages required for this ROS Course:</p> <ol> <li> <p>Install ROS Noetic (the instructions that follow are largely taken from the ROS.org website):</p> <ol> <li> <p>Set up your system to accept software from packages.ros.org:</p> <pre><code>sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" &gt; /etc/apt/sources.list.d/ros-latest.list'\n</code></pre> </li> <li> <p>Set up your keys (using <code>curl</code>): </p> <pre><code>curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add -\n</code></pre> </li> <li> <p>Make sure your package index is up-to-date:</p> <pre><code>sudo apt update\n</code></pre> </li> <li> <p>Install the \"Desktop-Full\" version of ROS:</p> <pre><code>sudo apt install ros-noetic-desktop-full\n</code></pre> </li> </ol> </li> <li> <p>Set up your environment.</p> <ol> <li> <p>A ROS script must be sourced in every bash terminal you use ROS in, so it's best to add a line to the end of your <code>~/.bashrc</code> so that this occurs automatically whenever you open a terminal:</p> <pre><code>source /opt/ros/noetic/setup.bash\n</code></pre> </li> <li> <p>Re-source your environment for the changes to take effect:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Install dependencies for building packages</p> <p>\"Up to now you have installed what you need to run the core ROS packages. To create and manage your own ROS workspaces, there are various tools and requirements that are distributed separately. For example, rosinstall is a frequently used command-line tool that enables you to easily download many source trees for ROS packages with one command.\"</p> <pre><code>sudo apt install python3-rosdep \\\npython3-rosinstall \\ \npython3-rosinstall-generator \\ \npython3-wstool build-essential\n</code></pre> </li> <li> <p>Initialize <code>rosdep</code>:</p> <p>\"Before you can use many ROS tools, you will need to initialize rosdep. rosdep enables you to easily install system dependencies for source you want to compile and is required to run some core components in ROS.\"</p> <p><pre><code>sudo rosdep init\n</code></pre> <pre><code>rosdep update\n</code></pre></p> </li> <li> <p>Install 'Catkin Tools'. This is optional. By default, <code>catkin_make</code> can be used to invoke CMake for building ROS packages, but we use <code>catkin build</code> instead (because it's a bit nicer!) See how to install Catkin Tools here, if you want to. </p> </li> <li> <p>Create and build a Catkin Workspace:</p> <ol> <li> <p>Make the directories:</p> <pre><code>mkdir -p ~/catkin_ws/src\n</code></pre> </li> <li> <p>Navigate into the root folder:</p> <pre><code>cd ~/catkin_ws/\n</code></pre> </li> <li> <p>Then if you installed Catkin Tools:</p> <pre><code>catkin build\n</code></pre> <p>... and if not, then its:</p> <pre><code>catkin_make\n</code></pre> </li> <li> <p>Then you need to add this to the end of your <code>~/.bashrc</code> as well:</p> <pre><code>source ~/catkin_ws/devel/setup.bash\n</code></pre> </li> <li> <p>And finally, re-source:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Install the TurtleBot3 packages, based on the Robotis instructions:</p> <ol> <li> <p>First, install dependencies:</p> <pre><code>sudo apt-get install ros-noetic-joy ros-noetic-teleop-twist-joy \\\nros-noetic-teleop-twist-keyboard ros-noetic-laser-proc \\\nros-noetic-rgbd-launch ros-noetic-rosserial-arduino \\\nros-noetic-rosserial-python ros-noetic-rosserial-client \\\nros-noetic-rosserial-msgs ros-noetic-amcl ros-noetic-map-server \\\nros-noetic-move-base ros-noetic-urdf ros-noetic-xacro \\\nros-noetic-compressed-image-transport ros-noetic-rqt* ros-noetic-rviz \\\nros-noetic-gmapping ros-noetic-navigation ros-noetic-interactive-markers\n</code></pre> </li> <li> <p>Then, install the TurtleBot3 packages themselves:</p> <p><pre><code>sudo apt install ros-noetic-dynamixel-sdk\n</code></pre> <pre><code>sudo apt install ros-noetic-turtlebot3-msgs\n</code></pre> <pre><code>sudo apt install ros-noetic-turtlebot3\n</code></pre> <pre><code>sudo apt install ros-noetic-turtlebot3-simulations\n</code></pre></p> </li> <li> <p>And then add some environment variables to your <code>~/.bashrc</code> too, in order for ROS and the TurtleBot3 packages to launch correctly:</p> <pre><code>export TURTLEBOT3_MODEL=waffle\nexport ROS_MASTER_URI=http://localhost:11311\nexport ROS_HOSTNAME=localhost\n</code></pre> </li> </ol> </li> <li> <p>Next, install some other useful Python tools:</p> <pre><code>sudo apt install python3-pip python3-scipy python3-pandas\n</code></pre> </li> <li> <p>Finally, install the Course Repo:</p> <ol> <li> <p>Navigate to your Catkin Workspace:</p> <pre><code>cd ~/catkin_ws/src/\n</code></pre> </li> <li> <p>Download the repo from GitHub:</p> <pre><code>git clone https://github.com/tom-howard/tuos_ros.git\n</code></pre> </li> <li> <p>Then, if you installed Catkin Tools:</p> <pre><code>catkin build\n</code></pre> <p>... if not, do this:</p> <pre><code>cd ~/catkin_ws/ &amp;&amp; catkin_make\n</code></pre> </li> <li> <p>And finally, re-source again:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>For convenience, we use some Bash Aliases to make it easier to call some common ROS commands. You might want to create a <code>~/.bash_aliases</code> file with the following content (or add to an existing one):</p> <pre><code>alias tb3_teleop=\"rosrun turtlebot3_teleop turtlebot3_teleop_key\"\nalias tb3_world=\"roslaunch turtlebot3_gazebo turtlebot3_world.launch\"\nalias tb3_empty_world=\"roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\"\nalias tb3_slam=\"roslaunch turtlebot3_slam turtlebot3_slam.launch\"\nalias tb3_rviz=\"roslaunch tuos_simulations rviz.launch\"\n\n# This one's quite useful too:\nalias src=\"echo 'Sourcing bashrc...' &amp;&amp; source ~/.bashrc\"\n</code></pre> </li> </ol>"},{"location":"software/manual/windows/","title":"Installing ROS on Windows (using WSL)","text":"<p>Applicable to: Those who have a Windows 10 or 11 personal machine.</p>"},{"location":"software/manual/windows/#prerequisites","title":"Prerequisites","text":"<ol> <li>Your computer must be running Windows 10 Build 19044 or higher, or Windows 11.</li> <li>Update the GPU drivers for your machine.</li> <li>Install or update WSL:<ol> <li>If you don't already have WSL installed on your machine then follow these instructions to install it.</li> <li>If you do already have WSL installed on your machine, then follow these instructions to update it.</li> </ol> </li> <li>Install the Windows Terminal.</li> <li>Install Visual Studio Code and the WSL extension.</li> </ol>"},{"location":"software/manual/windows/#install-and-configure-an-ubuntu-2004-wsl-distribution","title":"Install and configure an Ubuntu 20.04 WSL distribution","text":"<ol> <li> <p>Launch PowerShell and enter the following command to download and install an Ubuntu 20.04 distribution for WSL:</p> <pre><code>wsl --install Ubuntu-20.04 --web-download\n</code></pre> <p>The installation might take a few minutes, but once complete the distribution should automatically be launched.</p> </li> <li> <p>From within the new Ubuntu distribution, update and install a few important tools:</p> <ol> <li> <p>Update existing packages:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade\n</code></pre> </li> <li> <p>Install the latest version of Git:</p> <p><pre><code>sudo add-apt-repository ppa:git-core/ppa\n</code></pre> <pre><code>sudo apt update &amp;&amp; sudo apt install git\n</code></pre></p> </li> <li> <p>Make sure a few additional packages are also installed:</p> <pre><code>sudo apt install eog dos2unix curl wget\n</code></pre> </li> </ol> </li> </ol>"},{"location":"software/manual/windows/#installing-ros","title":"Installing ROS","text":"<p>Having installed and configured your Ubuntu 20.04 instance, continue on to the next page and follow the steps for Installing ROS (and dependencies).</p>"},{"location":"software/on-campus/getting-started/","title":"Getting Started with WSL-ROS on the University Managed Desktops","text":""},{"location":"software/on-campus/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Accessing WSL-ROS on a University Managed Desktop Computer</li> </ul>"},{"location":"software/on-campus/getting-started/#launching-wsl-ros","title":"Launching WSL-ROS","text":"<ol> <li> <p>Click the Windows Start Menu button: </p> </li> <li> <p>Then, start typing <code>wsl-ros</code> and click on the application shortcut that should appear in the list:</p> <p> </p> <p>You will be presented with the following screen:</p> <p> </p> <p>WSL-ROS is now being installed from our custom OS image, which may take a couple of minutes to complete.  Once it's done, the Windows Terminal should automatically launch:</p> <p> </p> </li> </ol> <p>This is an Ubuntu Terminal Instance running within the WSL-ROS Environment!</p>"},{"location":"software/on-campus/getting-started/#configure-vscode","title":"Configuring Visual Studio Code","text":"<p>Visual Studio Code (or \"VS Code,\" for short) should be installed on all the University of Sheffield Managed Desktop Computers that have WSL-ROS pre-installed on. This is a great Integrated Development Environment (IDE) that we'll use extensively throughout the course. You'll first need to make sure it's set up correctly though, so follow the instructions here, in preparation for later.</p> <p>Tip</p> <p>You should only ever need to do this bit once: the configurations should be saved to your user profile, and should be carried over to any other University Desktop Computer that you log into!</p>"},{"location":"software/on-campus/getting-started/#backing-up-and-restoring-your-data","title":"Backing-Up (and Restoring) your Data","text":"<p>If you're working with WSL-ROS on a university managed desktop machine, the WSL-ROS Environment will only be preserved for a limited time on the machine that you installed it on. As such, any work that you do within WSL-ROS will not be preserved between sessions or across different machines automatically! It's therefore really important that you run a backup script before you close WSL-ROS down. To do so is very easy, simply run the command below from any WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will create an archive of your Linux Home Directory and save it to your University <code>U:</code> Drive. Whenever you launch a fresh WSL-ROS Environment again on another day, or on a different machine, simply run the following command to restore your work to it:</p> <pre><code>wsl_ros restore\n</code></pre> <p>To make things a little easier, on launching WSL-ROS the system will check to see if a backup file already exists from a previous session. If it does, then you will be asked if you want to restore it straight away:</p> <p></p> <p>Enter <code>Y</code> to restore your data from this backup file, or <code>N</code> to leave the backup file alone and work from fresh (none of your previous work will be restored). </p>"},{"location":"software/on-campus/getting-started/#re-launching-the-environment","title":"Re-Launching the Environment","text":"<p>As discussed above, the WSL-ROS environment is not preserved on the university managed desktops indefinitely. If you log back in to the same machine within a few hours however, then it may still be there, and you'll be presented with the following message when you launch it:</p> <p></p> <p>Enter <code>Y</code> to continue where you left things previously, or <code>N</code> to start from a fresh installation.</p> <p>Warning</p> <p>If you select <code>N</code> then any work that you have created in the existing environment will be deleted! Always make sure you back up your work using the procedure outlined above!</p>"},{"location":"software/on-campus/linux-term/","title":"A Quick Introduction to the Linux Terminal","text":"<p>You'll work extensively with the Linux Terminal throughout this course. An idle WSL-ROS terminal instance will look like this:</p> <p></p> <p>Here, the presence of the <code>$</code> symbol indicates that the terminal is ready to accept a command. Text before the <code>$</code> symbol has two parts separated by the <code>:</code> symbol:</p> <ul> <li>Text to the left of the <code>:</code> tells us the name of the Linux user (\"student\" in this case) followed by the WSL-ROS version that you are working with.</li> <li>Text to the right of the <code>:</code> tells us where in the Linux Filesystem we are currently located (<code>~</code> means \"The Home Directory\", which is an alias for the path: <code>/home/student/</code>).</li> </ul> <p>If you don't see the <code>$</code> symbol at all, then this means that a process is currently running. To stop any running process enter Ctrl+C simultaneously on your keyboard.</p>"},{"location":"software/on-campus/rdp/","title":"Accessing WSL-ROS Remotely","text":"<p>As discussed here, WSL-ROS can also be accessed on certain managed desktop machines remotely via the University Remote Desktop Service. You'll need to use your university account credentials to access this service and will also need to have multifactor authentication (MFA) set up on your university account.</p> <ol> <li>The environment has been installed on all the machines in Virtual Classroom 1 (University of Sheffield sign-in required).</li> <li>From here, you should see a long list of PCs. Any that have a blue \"Connect\" button next to them are available to use.</li> <li>Click on the \"Connect\" button next to an available machine.  This will download a <code>.rdp</code> file to your computer.</li> <li>Once downloaded, double-click on the <code>.rdp</code> file and log in to the PC using your university account credentials. (Look out for an MFA notification on your MFA-linked device.)</li> <li>Once you're logged into the remote machine follow the steps here to launch the environment. </li> <li>When you're finished, close the RDP connection and then close the Remote Desktop app on your machine.</li> </ol>"},{"location":"software/on-campus/rdp/#its","title":"Remote Access Support","text":"<p>Note</p> <p>We (the Teaching Team) are not able to deal with issues relating to the University Remote Desktop Service!</p> <p>If you are facing any issues then you should have a look at this IT Services support page. You can also contact the IT Services Helpdesk for help and support (Monday-Friday between 8am-5pm).</p> <p>If you're having issues related to WSL-ROS however, then please let the Teaching Team know.</p>"},{"location":"software/on-campus/vscode/","title":"VS Code and WSL","text":""},{"location":"software/on-campus/vscode/#the-top","title":"Launching VS Code from the Terminal","text":"<ol> <li> <p>You can launch VS Code directly from a WSL-ROS terminal instance. Simply type <code>code .</code> and then hit Enter :</p> <p> </p> </li> <li> <p>A warning message may then pop up:</p> <p> </p> <p>Check the box to \"Permanently allow host 'wsl$'\" and then click the \"Allow\" button.</p> </li> <li> <p>VS Code should then launch, and you'll be presented with another trust dialogue:</p> <p> </p> <p>Click the blue \"Yes, I trust the authors\" button.</p> </li> </ol>"},{"location":"software/on-campus/vscode/#wsl-ext","title":"Installing the WSL Extension","text":"<ol> <li> <p>The first time you launch VS Code (as above) you should be presented with a pop-up in the bottom-right of the screen, asking if you would like to \"install the recommended 'WSL' extension from Microsoft...\"</p> <p> </p> <p>Click on the blue \"Install\" button.</p> Don't see the pop-up? <p>You can also install the 'WSL' extension manually.</p> <p>Click on the \"Extensions\" icon in the left-hand toolbar (or hit Ctrl+Shift+X ), type \"wsl\" in the search box and hit the install button on the right extension, as show below:</p> <p> </p> </li> <li> <p>Once installed, close down VS Code, go back to the WSL-ROS terminal instance and re-launch it using the <code>code .</code> command again.</p> <p>This time, you may be presented with yet another trust pop-up dialogue:</p> <p> </p> <p>Once again, check the box to \"Trust the authors\" and then click the blue \"Yes, I trust the authors\" button. </p> </li> <li> <p>You can now navigate the WSL-ROS filesystem in the explorer window on the left-hand side of the VS Code screen. You'll need to use this to locate the packages and scripts that you create throughout this course!</p> <p> </p> <p>Important</p> <p>Always make sure that the \"WSL\" extension is enabled!!</p> <p>If you don't see the WSL icon at the bottom-left of your screen (as shown below) then start again from the top or follow the alternative instructions below.</p> <p> </p> </li> </ol>"},{"location":"software/on-campus/vscode/#alt-launch","title":"Accessing WSL-ROS from VS Code (Alternate Method)","text":"<p>It's really important to make sure VS Code is launched within the WSL-ROS environment each time you work with it. Having followed the steps above, if the blue WSL icon is visible in the bottom-left corner of the VS Code screen then you're all good. If not, then try this method to load it correctly instead.</p> <ol> <li> <p>First, make sure WSL-ROS is running by following the steps outlined here.</p> </li> <li> <p>Launch VS Code from the Windows Application Menu by clicking the Windows Start Menu button: </p> </li> <li> <p>Type <code>vscode</code> (or just <code>code</code> works as well) and click on the application shortcut that should then appear in the list:</p> <p> </p> </li> <li> <p>You should have already installed the \"WSL\" extension. If so, click the blue \"remote window\" icon and then click the <code>Connect to WSL using Distro...</code> option in the menu that appears:</p> <p> </p> </li> <li> <p>Then, click on <code>WSL-ROS</code> to select it.</p> <p> </p> </li> <li> <p>The blue \"remote window\" icon in the bottom left-hand corner should change now to indicate that VS Code is now running inside WSL-ROS:</p> <p> </p> </li> <li> <p>Next, access the WSL-ROS filesystem by:</p> <ol> <li>Clicking the \"Explorer\" icon in the left-hand toolbar (or by using the Ctrl+Shift+E keyboard shortcut),</li> <li>Clicking the blue <code>\"Open Folder\"</code> button,</li> <li>Clicking <code>\"OK\"</code> to select the default <code>/home/student/</code> filesystem location.</li> </ol> <p> </p> </li> <li> <p>Finally, you should be presented with a pop-up asking if you trust the WSL-ROS home directory. Tick the checkbox and click on the blue <code>\"Yes, I trust the authors\"</code> button:</p> <p> </p> </li> <li> <p>VS Code should now be good to go...</p> <p> </p> </li> </ol>"},{"location":"software/on-campus/vscode/#verify","title":"Always make sure that the \"WSL\" extension is enabled!!","text":"<p>Check that your blue \"Remote Window\" icon in the bottom-left of the VS Code screen always looks like this:</p> <p></p> <p>If not, then go back to the top of this page and try again!</p>"},{"location":"software/wsl-ros/","title":"The WSL-ROS Simulation Environment","text":"<p>To support these courses we've created a custom ROS (Noetic) and Ubuntu (20.04) environment which runs on Windows 10 using the Windows Subsystem for Linux (WSL). We call this \"WSL-ROS,\" and you can find out more about it - and how to use it - here:</p> <ul> <li>Installing WSL-ROS on your Own Machine</li> <li>Accessing WSL-ROS from A University Managed Desktop Computer on Campus</li> <li>Further Support: Using WSL-ROS on the University Managed Desktops</li> </ul> <p></p>"},{"location":"software/wsl-ros/install/","title":"Installing WSL-ROS on your Own Machine","text":"<p>Applicable for: Windows 10 or 11 personal (unmanaged) computers</p> <p>You can install WSL-ROS (our purpose-built ROS image for this course) via the University of Sheffield Software Download Service (University login required).</p> <p>Note</p> <p>When you download WSL-ROS from the Software Download Service you will receive an email with installation instructions. We recommend that you follow the instructions provided on this page instead, as this page will be kept more up-to-date throughout the semester.</p>"},{"location":"software/wsl-ros/install/#prerequisites","title":"Prerequisites","text":"<ol> <li>Your computer must be running Windows 10 Build 19044 or higher, or Windows 11.</li> <li>Update the GPU drivers for your machine.</li> <li>Install or update WSL:<ol> <li>If you don't already have WSL installed on your machine then follow these instructions to install it.</li> <li>If you do already have WSL installed on your machine, then follow these instructions to update it.</li> </ol> </li> <li>Install the Windows Terminal.</li> <li>Install Visual Studio Code and the WSL extension.</li> </ol>"},{"location":"software/wsl-ros/install/#installing-wsl-ros","title":"Installing WSL-ROS","text":"<ol> <li>Go to the IT Services Software Downloads page (you'll need to log in with your university MUSE credentials).</li> <li>Scroll down to the bottom, where you should see WSL-ROS listed. Click on the blue \"Request WSL-ROS\" button and then wait to receive an email to your university email address. </li> <li> <p>The email will contain a link to the download location. Click this link to download the software to your machine as a <code>.zip</code> file (~2 GB).</p> <p>Remember</p> <p>The email will include some installation instructions, but we'd recommend that you follow the instructions provided below instead, just in case anything has changed recently.</p> </li> <li> <p>On your computer, create a new folder in the root of your <code>C:\\</code> drive called <code>WSL-ROS</code>.</p> </li> <li>Extract the content of the downloaded <code>.zip</code> file into to the <code>C:\\WSL-ROS\\</code> folder.</li> <li> <p>Launch PowerShell and enter the follow command to install WSL-ROS as a WSL distro on your machine:</p> <pre><code>wsl --import WSL-ROS $env:localappdata\\WSL-ROS `\nC:\\WSL-ROS\\wsl-ros-2309b-SDS.tar --version 2\n</code></pre> <p>Note</p> <p>Note that it's important to install this as a WSL 2 distro, not WSL 1 (hence the <code>--version 2</code> bit)!</p> </li> <li> <p>This may take a couple of minutes. Once it's done, you can verify that it was successful with the following command:</p> <pre><code>wsl -l -v\n</code></pre> <p>Where <code>WSL-ROS</code> should be listed.</p> </li> <li> <p>Next (optional, but recommended), copy over our recommended settings for the Windows Terminal:</p> <pre><code>Copy-Item -Path C:\\WSL-ROS\\settings.json -Destination `\n$env:localappdata\\Packages\\Microsoft.WindowsTerminal_8wekyb3d8bbwe\\LocalState\n</code></pre> </li> <li> <p>You should now be able to launch the WSL-ROS environment by launching the Windows Terminal App:</p> </li> </ol> <p></p>"},{"location":"software/wsl-ros/install/#see-also","title":"See Also","text":"<ul> <li>Setting up VS Code for WSL</li> <li>A Quick Introduction to the Linux Terminal</li> <li>A Config File for VcXsrv<sup>1</sup></li> </ul> <ol> <li> <p>https://sourceforge.net/projects/vcxsrv/ \u21a9</p> </li> </ol>"},{"location":"software/wsl-ros/on-campus/","title":"Accessing WSL-ROS from A University Managed Desktop Computer on Campus","text":"<p>Applicable for: Students who do not have a Windows 10 or 11 personal computer.</p> <p>The WSL-ROS environment is available on a range of managed desktop computers across The University of Sheffield Campus. </p>"},{"location":"software/wsl-ros/on-campus/#pre-installed","title":"Managed Desktops with WSL-ROS Pre-Installed","text":"<p>WSL-ROS is pre-installed on all machines in the following Campus Computer Rooms:</p> <ul> <li>The Diamond:  <ul> <li>Computer Rooms 1, 2 &amp; 4</li> <li>Short Term Loan Laptops in Computer Room 5</li> <li>40 dedicated WSL-ROS laptops (made available in Computer Room 5 during the COM2009 lab sessions)</li> </ul> </li> <li>North Campus PC Room</li> <li>The Information Commons:<ul> <li>Classrooms 3 and 4</li> <li>Computers on Floors 1, 2, 3, 4 (including Northlights and Pavillion), and Floor 6</li> </ul> </li> <li>Remote Desktops:<ul> <li>Virtual Classroom 1 - See here for more information</li> </ul> </li> </ul>"},{"location":"software/wsl-ros/on-campus/#managed-desktops-with-wsl-ros-available","title":"Managed Desktops with WSL-ROS Available","text":"<p>In addition to this, WSL-ROS can also be installed on all machines in the following additional Campus Computer Rooms (via Software Center):</p> <ul> <li>Sir Frederick Mappin Building:<ul> <li>Room F110</li> <li>Heartspace Room E095</li> </ul> </li> </ul>"},{"location":"software/wsl-ros/on-campus/#see-also","title":"See Also","text":"<ul> <li>Getting Started with WSL-ROS on the University Managed Desktops </li> <li>A Quick Introduction to the Linux Terminal</li> </ul>"},{"location":"waffles/","title":"Working with the Real TurtleBot3 Waffles","text":"<p>This section of the Course Site contains a range of resources to support your work with our real TurtleBot3 Waffles in the Diamond. </p> <p></p>"},{"location":"waffles/exercises/","title":"ROS &amp; Waffle Basics","text":"<p>Having completed the steps on the previous page, your robot and laptop should now be paired, and ROS should be up and running. The next thing to do is bring the robot to life! </p> <p>On this page you'll work through a series of exercises with the TurtleBot3 (aka, the Waffle) in your teams, exploring how the robot works whilst also getting an initial insight into how ROS works too. A number of the exercises here are similar to those that you'll do (or perhaps have already done) individually in simulation for Assignment #1. As you'll soon see, whether you're working with a real robot or a simulation, a lot of the principles are the same for both. </p>"},{"location":"waffles/exercises/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Making the Robot Move</li> <li>Exercise 2: Cloning Your Team's ROS Package to the Robot Laptop</li> <li>Exercise 3: Seeing the Sensors in Action</li> <li>Exercise 4: Visualising the ROS Network</li> <li>Exercise 5: Exploring ROS Topics and Messages</li> <li>Exercise 6: Creating Your First Python Node</li> <li>Exercise 7: Using SLAM to create a map of the environment</li> <li>Exercise 8: Pushing Changes to Your ROS Package Back to GitHub</li> </ul>"},{"location":"waffles/exercises/#manual-control","title":"Manual Control","text":""},{"location":"waffles/exercises/#exMove","title":"Exercise 1: Making the Robot Move","text":"<p>Throughout Lab Assignment #1 you will use a ready-made ROS application called <code>turtlebot3_teleop_keyboard</code> to drive a Waffle around a range of simulated environments. This works in exactly the same way with a real robot in a real world too:</p> <ol> <li> <p>Open up a new terminal instance on the laptop either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon, we'll refer to this as TERMINAL 1. In this terminal enter the following <code>rosrun</code> command to launch <code>turtlebot3_teleop_keyboard</code> (note that it's exactly the same command as you use in simulation too):</p> <p>TERMINAL 1: <pre><code>rosrun turtlebot3_teleop turtlebot3_teleop_key\n</code></pre></p> Robotics Laptop Tip <p>There's a bash alias for this command to make it quicker to type in future: <code>tb3_teleop</code>!</p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on the keyboard:</p> <p> </p> <p>Warning</p> <p>Take care to avoid any obstacles or other people in the lab as you do this!</p> </li> <li> <p>Once you've spent a bit of time on this, close the application down by entering Ctrl+C in TERMINAL 1.</p> </li> </ol>"},{"location":"waffles/exercises/#packages-and-nodes","title":"Packages and Nodes","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations).  </p> <p>Scripts tell the robot what to do and how to act. In ROS, these scripts are called nodes. ROS Nodes are executable programs that perform specific robot tasks and operations. These are typically written in C++ or Python, but it's possible to write ROS Nodes using other programming languages too.</p> <p>In the initial setup of the robot on the previous page (Step 3) you simultaneously established a ROS Network (\"the ROS Master\") and launched a range of different nodes on the robot with a <code>roslaunch</code> command. Then, in Exercise 1 above you launched the <code>turtlebot3_teleop_key</code> node on the laptop:</p> <p> Command Context 1 <code>roslaunch tuos_tb3_tools ros.launch</code> Robot 2 <code>rosrun turtlebot3_teleop turtlebot3_teleop_key</code> Laptop <p></p> <p>The first of the above commands was a <code>roslaunch</code> command, which has the following two parts to it (after the <code>roslaunch</code> bit):</p> <pre><code>roslaunch {[1] Package name} {[2] Launch file}\n</code></pre> <p>Part [1] specifies the name of the ROS package containing the functionality that we want to execute. Part [2] is a file within that package that tells ROS exactly what scripts ('nodes') that we want to launch. We can launch multiple nodes at the same time from a single launch file.</p> <p>The second command was a <code>rosrun</code> command, which has a structure similar to <code>roslaunch</code>:</p> <pre><code>rosrun {[1] Package name} {[2] Node name}\n</code></pre> <p>Here, Part [1] is the same as the <code>roslaunch</code> command, but Part [2] is slightly different: <code>{[2] Node name}</code>. Here we are directly specifying a single script that we want to execute. We therefore use <code>rosrun</code> if we only want to launch a single node on the ROS network (<code>turtlebot3_teleop_key</code> in this case, which is a Python script).</p> <p>The key difference between <code>roslaunch</code> and <code>rosrun</code> then is that with <code>roslaunch</code> we can execute 1 or more nodes at the same time via launch files. Another handy feature of <code>roslaunch</code> is that it will automatically launch the ROS Master if it isn't already running. As illustrated by the \"Context\" column in the table above, we ran our <code>roslaunch</code> command on the robot, which means that all the specified nodes (defined in the <code>ros.launch</code> file<sup>1</sup>) and the ROS Master were executed on the robot.</p> <p></p> <p>The ROS Master</p> <p>The ROS Master is a wireless communication network that is established between the robot and laptop (and indeed any other device that we might want to add). The benefit of this then is that it allowed us to run our <code>rosrun</code> command on the laptop, and this was able to invoke changes to the robot (i.e. making it move around) via this wireless ROS communication network.</p>"},{"location":"waffles/exercises/#exClone","title":"Exercise 2: Cloning Your Team's ROS Package to the Robot Laptop","text":"<p>In the Assignment #2 \"Getting Started\" tasks that you should have completed earlier you should have created your team's Assignment #2 ROS package and pushed it to GitHub. In this exercise you will now clone it on to the Robotics Laptop and create your first Python ROS node within it.</p> <p>WiFi</p> <p>Remember, the Robotics Laptop needs to be connected to the \"DIA-LAB\" WiFi network in order for the robot and laptop to communicate with one another, but DIA-LAB is an internal network, and you won't be able to access the internet!</p> <p>Make sure the laptop is now connected to \"eduroam\" before starting this exercise.</p> <p>You'll use SSH keys to download your team's ROS package onto the laptop now. You'll need to follow a similar procedure if you end up working on a different laptop during a different lab, or if you happen to delete your package from the laptop that you're working on now. There are some more detailed instruction on how all this works here, which you should refer to in future lab sessions.</p>"},{"location":"waffles/exercises/#ssh-keygen","title":"Step 1: Generating an SSH key (on the Laptop)","text":"<ol> <li> <p>From a terminal instance on the laptop (i.e. TERMINAL 1) navigate to the <code>~/.ssh</code> folder using the <code>cd</code> Linux Command (\"change directory\"):</p> <pre><code>cd ~/.ssh\n</code></pre> </li> <li> <p>Create a new SSH key on the laptop, using your GitHub email address:</p> <pre><code>ssh-keygen -t ed25519 -C \"your.email@sheffield.ac.uk\"\n</code></pre> <p>Replacing <code>your.email@sheffield.ac.uk</code> with your GitHub email address.</p> </li> <li> <p>You'll then be asked to \"Enter a file in which to save the key\". This needs to be unique, so enter the name of your ROS package. For the purposes of this example, let's assume yours is called <code>com2009_team999</code>.</p> </li> <li> <p>You'll then be asked to enter a passphrase. This is how you make your SSH key secure, so that no other teams using the same laptop can access and make changes to your team's package/GitHub repo. You'll be asked to enter this whenever you try to commit/push new changes back to GitHub. Decide on a passphrase and share this with ONLY YOUR TEAM. </p> </li> <li> <p>Next, start the laptop's ssh-agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> </li> <li> <p>Add your SSH private key to the laptop's ssh-agent. You'll need to enter the name of the SSH key file that you created in the earlier step (e.g.: <code>com2009_team999</code>)</p> <pre><code>ssh-add ~/.ssh/com2009_team999\n</code></pre> <p>Replacing <code>com2009_team999</code> with the name of your own SSH key file.</p> </li> <li> <p>Then, you'll need to add the SSH key to your account on GitHub...</p> </li> </ol>"},{"location":"waffles/exercises/#step-2-adding-the-ssh-key-to-your-github-account","title":"Step 2: Adding the SSH key to your GitHub account","text":"<ol> <li> <p>On the laptop, copy the SSH public key that you created in the previous steps to your clipboard.</p> <p>Do this from a terminal on the laptop, using <code>cat</code>:</p> <pre><code>cat ~/.ssh/com2009_team999.pub\n</code></pre> <p>replacing <code>com2009_team999</code> with the name of your SSH key file.</p> <p>The content of the file will then be displayed in the terminal... copy it from here.</p> <p>Tips</p> <ol> <li>To copy text from inside a terminal window use Ctrl+Shift+C</li> <li> <p>You could also open the file in VS Code and copy it from there:</p> <pre><code>code ~/.ssh/com2009_team999.pub\n</code></pre> </li> </ol> </li> <li> <p>Go to your GitHub account in a web browser. In the upper-right corner of any page, click your profile photo, then click Settings.</p> </li> <li> <p>In the \"Access\" section of the sidebar, click SSH and GPG keys.</p> </li> <li> <p>Click New SSH key.</p> </li> <li> <p>Enter a descriptive name for the key in the \"Title\" field, e.g. <code>com2009_dia-laptop1</code>.</p> </li> <li> <p>Select <code>Authentication Key</code> as the \"Key Type.\"</p> </li> <li> <p>Paste the text from your SSH Public Key file into the \"Key\" field.</p> </li> <li> <p>Finally, click the \"Add SSH Key\" button.</p> </li> </ol>"},{"location":"waffles/exercises/#ssh-clone","title":"Step 3: Cloning your ROS package onto the Laptop","text":"<p>With your SSH keys all set up, you can now clone your ROS package onto the laptop. </p> <p>There's a \"Catkin Workspace\" on each of the robot laptops and your package must reside within this workspace. (You'll learn more about Catkin Workspaces in Assignment #1.)</p> <ol> <li> <p>From a terminal on the laptop, navigate to the Catkin Workspace <code>src</code> directory:</p> <pre><code>cd ~/catkin_ws/src\n</code></pre> </li> <li> <p>Go to your ROS package on GitHub. Click the Code button and then select the SSH option to reveal the SSH address of your repo. Copy this. </p> </li> <li> <p>Head back to the terminal instance on the laptop to then clone your package into the <code>catkin_ws/src/</code> directory using <code>git</code>:</p> <pre><code>git clone {REMOTE_SSH_ADDRESS}\n</code></pre> <p>Where <code>{REMOTE_SSH_ADDRESS}</code> is the SSH address that you have just copied from GitHub.</p> <p>Tip</p> <p>To paste text into the Linux terminal window use Ctrl+Shift+V</p> </li> <li> <p>Run Catkin Build to make sure that any resources within your package that need to be compiled (custom ROS messages, etc.) are compiled onto the laptop so that they can be used locally:</p> <pre><code>catkin build com2009_team999\n</code></pre> <p>...again, replacing <code>com2009_team999</code> with your team's package name.</p> </li> <li> <p>Then, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Note</p> <p>This will all become very familiar with you once you've worked through Assignment #1!</p> </li> </ol> <p>You should then be able to commit and push any updates that you make to your ROS package while working on the laptop back to your remote repository using the secret passphrase that you defined earlier!</p>"},{"location":"waffles/exercises/#sensors-visualisation-tools","title":"Sensors &amp; Visualisation Tools","text":"<p>WiFi</p> <p>Make sure the laptop is now connected back to \"DIA-LAB\" in order to continue with the rest of the exercises.</p> <p>Our Waffles have some pretty sophisticated sensors on them, allowing them to \"see\" the world around them. Let's now see what our robot sees, using some handy ROS tools.</p>"},{"location":"waffles/exercises/#exViz","title":"Exercise 3: Seeing the Sensors in Action","text":""},{"location":"waffles/exercises/#part-1-the-camera","title":"Part 1: The Camera","text":"<ol> <li> <p>There shouldn't be anything running in TERMINAL 1 now, after you closed down the Teleop node (using Ctrl+C) at the end of the previous exercise. Return to this terminal and launch the <code>rqt_image_view</code> node:</p> <p>TERMINAL 1: <pre><code>rosrun rqt_image_view rqt_image_view\n</code></pre></p> <p>Questions</p> <ol> <li>We're using <code>rosrun</code> here again, what does this mean?</li> <li>Why do we have to type <code>rqt_image_view</code> twice?</li> </ol> </li> <li> <p>A new window should open. Maximise this (if it isn't already) and then select <code>/camera/color/image_raw</code> from the dropdown menu at the top-left of the application window.</p> </li> <li>Live images from the robot's camera should now be visible! Stick your face in front of the camera and see yourself appear on the laptop screen!</li> <li> <p>Close down the window once you've had enough. This should release TERMINAL 1 so that you can enter commands in it again.</p> <p>The camera on the robot is quite a clever device. Inside the unit is two separate image sensors, giving it - effectively - both a left and right eye. The device then combines the data from both of these sensors and uses the combined information to infer depth from the images as well. Let's have a look at that in action now...</p> </li> <li> <p>In TERMINAL 1 enter the following command:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_tb3_tools rviz.launch\n</code></pre></p> <p>This will launch an application called RViz, which is a handy tool that allows us to visualise the data from all the sensors on-board our robots. When RViz opens, you should see something similar to the following:</p> <p> </p> <p>The strange wobbly sheet of colours in front of the robot is the live image stream from the camera with depth applied to it at the same time. The camera is able to determine how far away each image pixel is from the camera lens, and then uses that to generate this 3-dimensional representation. Nice eh!</p> </li> <li> <p>Again, place your hand or your face in front of the camera and hold steady for a few seconds (there may be a bit of a lag as all of this data is transmitted over the WiFi network). You should see yourself rendered in 3D in front of the robot! </p> </li> </ol>"},{"location":"waffles/exercises/#part-2-the-lidar-sensor","title":"Part 2: The LiDAR Sensor","text":"<p>In RViz you may have also noticed a lot of red dots scattered around the robot. This is a representation of the laser displacement data coming from the LiDAR sensor (the black device on the top of the robot). The LiDAR sensor spins continuously, sending out laser pulses into the environment as it does so. When a pulse hits an object it is reflected back to the sensor, and the time it takes for this to happen is used to calculate how far away the object is.</p> <p>The LiDAR sensor spins and performs this process continuously, so a full 360\u00b0 scan of the environment can be generated. This data is therefore really useful for things like obstacle avoidance and mapping. We'll explore this in more detail later.</p> <ol> <li> <p>For now, move your hand around the robot and see if you can see it being detected by the LiDAR sensor. Move your hand up and down and consider at what height the LiDAR sensor is able to detect it.</p> </li> <li> <p>Then, move your hand closer and further away and watch how the red dots move to match this. </p> </li> <li> <p>Open up a new terminal instance (TERMINAL 2) and launch the <code>turtlebot3_teleop_keyboard</code> node as you did in Exercise 1. Watch how the data in the RViz screen changes as you drive the robot around a bit.</p> </li> <li> <p>Once you've had enough, close down RViz (click the \"Close without saving\" button, if asked) and stop the Keyboard Teleop node by entering Ctrl+C in TERMINAL 2.</p> </li> </ol> <p>Using <code>rosrun</code> and <code>roslaunch</code>, as we have done so far, it's easy to end up with a lot of different processes or ROS Nodes running on the network, some of which we will interact with, but others may just be running in the background. It is often useful to know exactly what is running on the ROS network, and there are a number of ways to do this.</p>"},{"location":"waffles/exercises/#exNet","title":"Exercise 4: Visualising the ROS Network","text":"<ol> <li> <p>There shouldn't be anything running in Terminals 1 or 2 now, so return to TERMINAL 1 and use the <code>rosnode</code> command to list the nodes that are currently running on the robot:</p> <p>TERMINAL 1: <pre><code>rosnode list\n</code></pre></p> <p>You should see a list of at least 7 items.</p> </li> <li> <p>We can visualise the connections between the active nodes by using a ROS node called <code>rqt_graph</code>. Launch this as follows:</p> <p>TERMINAL 1: <pre><code>rosrun rqt_graph rqt_graph\n</code></pre></p> </li> <li> <p>In the window that opens, select <code>Nodes/Topics (active)</code> from the dropdown menu in the top left. </p> <p>What you should then see is a map of all the nodes in the list from above (as ovals), and arrows to illustrate the flow of information between them. This is a visual representation of the ROS network!</p> <p>Items that have a rectangular border are ROS Topics. ROS Topics are essentially communication channels, and ROS nodes can read (subscribe) or write (publish) to these topics to access sensor data, pass information around the network and make things happen.</p> </li> </ol> <p>A ROS Robot could have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to communicate and share data with the other nodes on the ROS Network.</p>"},{"location":"waffles/exercises/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics using ROS Messages. We were actually publishing messages to a topic when we made the robot move using the Teleop node in the previous exercises.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"waffles/exercises/#exTopicMsg","title":"Exercise 5: Exploring ROS Topics and Messages","text":"<p>Much like the <code>rosnode list</code> command, we can use <code>rostopic list</code> to list all the topics that are currently active on the ROS network.</p> <ol> <li> <p>Close down the <code>rqt_graph</code> window if you haven't done so already. This will release TERMINAL 1 so that we can enter commands in it again. Return to this terminal window and enter the following:</p> <p>TERMINAL 1: <pre><code>rostopic list\n</code></pre></p> <p>A much larger list of items should be printed to the terminal now. See if you can spot the <code>/cmd_vel</code> item in the list.</p> <p>This topic is used to control the velocity of the robot ('command velocity').</p> </li> <li> <p>Let's find out more about this using the <code>rostopic info</code> command.</p> <p>TERMINAL 1: <pre><code>rostopic info /cmd_vel\n</code></pre></p> <p>This should provide an output similar to the following: </p> <pre><code>Type: geometry_msgs/Twist\n\nPublishers: None\n\nSubscribers:\n * /turtlebot3_core (http://dia-waffleX:#####/)\n</code></pre> <p>This tells us a few things: </p> <ol> <li>The <code>/cmd_vel</code> topic currently has no publishers (i.e. no other nodes are currently writing data to this topic).</li> <li>The <code>/turtlebot3_core</code> node is subscribing to the topic. The <code>/turtlebot3_core</code> node turns motor commands into actual wheel motion, so it monitors the topic (i.e. subscribes to it) to see when a velocity command is published to it.</li> <li> <p>The type of message used by the <code>/cmd_vel</code> topic is called: <code>geometry_msgs/Twist</code>. </p> <p>The message type has two parts: <code>geometry_msgs</code> and <code>Twist</code>. <code>geometry_msgs</code> is the name of the ROS package that this message belongs to and <code>Twist</code> is the actual message type. </p> <p>We have just learnt then, that if we want to make the robot move we need to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic. </p> </li> </ol> </li> <li> <p>We can use the <code>rosmsg</code> command to find out more about the <code>Twist</code> message:</p> <p>TERMINAL 1: <pre><code>rosmsg info geometry_msgs/Twist\n</code></pre></p> <p>From this, we should obtain the following:</p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x\n  float64 y\n  float64 z\ngeometry_msgs/Vector3 angular\n  float64 x\n  float64 y\n  float64 z\n</code></pre> <p>Let's find out what it all means...</p> </li> </ol>"},{"location":"waffles/exercises/#velocity-control","title":"Velocity Control","text":"<p>The motion of any mobile robot can be defined in terms of its three principal axes: <code>X</code>, <code>Y</code> and <code>Z</code>. In the context of our TurtleBot3 Waffle, these axes (and the motion about them) are defined as follows:</p> <p></p> <p>In theory then, a robot can move linearly or angularly about any of these three axes, as shown by the arrows in the figure. That's six Degrees of Freedom (DOFs) in total, achieved based on a robot's design and the actuators it is equipped with. Take a look back at the <code>rosmsg info</code> output in TERMINAL 1. Hopefully it's a bit clearer now that these topic messages are formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x  &lt;-- Forwards (or Backwards)\n  float64 y  &lt;-- Left (or Right)\n  float64 z  &lt;-- Up (or Down)\ngeometry_msgs/Vector3 angular\n  float64 x  &lt;-- \"Roll\"\n  float64 y  &lt;-- \"Pitch\"\n  float64 z  &lt;-- \"Yaw\"\n</code></pre> <p>Our TurtleBot3 robot only has two motors, so it doesn't actually have six DOFs! The two motors can be controlled independently, which gives it what is called a \"differential drive\" configuration, but this still only allows it to move with two degrees of freedom in total, as illustrated below.</p> <p></p> <p>It can therefore only move linearly in the x-axis (Forwards/Backwards) and angularly in the z-axis (Yaw). </p>"},{"location":"waffles/exercises/#exSimpleVelCtrl","title":"Exercise 6: Creating Your First Python Node","text":"<p>Making a robot move with ROS is simply a case of publishing the right ROS Message (<code>Twist</code>) to the right ROS Topic (<code>/cmd_vel</code>). In some of the previous exercises above you used the Keyboard Teleop node to drive the robot around, a bit like a remote control car. In the background here all that was really happening was that the Teleop node was converting our keyboard button presses into velocity commands and publishing these to the <code>/cmd_vel</code> topic.</p> <p>In reality, robots need to be able to navigate complex environments autonomously, which is quite a difficult task, and requires us to build bespoke applications. We can build these applications using Python, and we'll look at the core concepts behind this now by building a simple node that will allow us to make our robot a bit more \"autonomous\". What we will do here forms the basis of the more complex applications that you will learn about in Assignment #1 and implement in Assignment #2 to bring a real robot to life!</p> <ol> <li> <p>You will create your first ROS node inside your team's <code>com2009_team999</code> ROS package, which you should have cloned to the laptop earlier on. This package should now correctly reside within the Catkin Workspace on the laptop's filesystem. Navigate to this from TERMINAL 1 using the <code>roscd</code> command:</p> <p>TERMINAL 1: <pre><code>roscd com2009_team999/ \n</code></pre> Replacing <code>com2009_team999</code> accordingly.</p> </li> <li> <p>Then, use the <code>cd</code> command to move into the <code>src</code> directory that should already exist within your package:</p> <p>TERMINAL 1: <pre><code>cd src/ \n</code></pre></p> </li> <li> <p>In here, create a Python file called <code>simple_move_square.py</code> using the <code>touch</code> command:</p> <p>TERMINAL 1: <pre><code>touch simple_move_square.py\n</code></pre></p> </li> <li> <p>You'll need to change the execution permissions for this file in order to be able to run it later on. You'll learn more about this in Assignment #1 but, for now, simply run the following command:</p> <p>TERMINAL 1: <pre><code>chmod +x simple_move_square.py\n</code></pre></p> </li> <li> <p>Now we want to edit this file, and we'll do that using Visual Studio Code (VS Code):</p> <p>TERMINAL 1: <pre><code>code .\n</code></pre></p> <p>Note</p> <p>Don't forget to include the <code>.</code> at the end there, it's important!!</p> </li> <li> <p>Once VS Code launches, open up the <code>simple_move_square.py</code> file, which should be visible in the file explorer on the left-hand side of the VS Code window. Paste the following content into it:</p> simple_move_square.py<pre><code>#!/usr/bin/env python3\n\nimport rospy # (1)!\nfrom geometry_msgs.msg import Twist # (2)!\nfrom math import sqrt, pow, pi # (15)!\n\nmovement = \"state1\" # \"state2, state3 etc...\"\ntransition = True\n\nrospy.init_node(\"move_waffle\", anonymous=True) # (3)!\nrate = rospy.Rate(10) # (4)!\n\npub = rospy.Publisher('/cmd_vel', Twist, queue_size=10) # (5)!\nvel = Twist() # (6)!\n\nrospy.loginfo(f\"The node has been initialised...\")\ntimestamp = rospy.get_time() # (7)!\n\nwhile not rospy.is_shutdown(): # (8)!\n    elapsed_time = rospy.get_time() - timestamp # (9)!\n    if transition: # (10)!\n        timestamp = rospy.get_time()\n        transition = False\n        vel.linear.x = 0.0\n        vel.angular.z = 0.0\n        print(f\"Moving to state: {movement}\")\n    elif movement == \"state1\": # (11)!\n        if elapsed_time &gt; 2:\n            movement = \"state2\"\n            transition = True\n        else:\n            vel.linear.x = 0.05\n            vel.angular.z = 0.0\n    elif movement == \"state2\": # (12)!\n        if elapsed_time &gt; 4:\n            movement = \"state1\"\n            transition = True\n        else:\n            vel.angular.z = 0.2\n            vel.linear.x = 0.0\n    pub.publish(vel) # (13)!\n    rate.sleep() # (14)!\n</code></pre> <ol> <li><code>rospy</code> is the ROS client library for Python. We need this so that our Python node can interact with ROS.</li> <li>We know from earlier that in order to make a robot move we need to publish messages to the <code>/cmd_vel</code> topic, and that this topic uses <code>Twist</code> messages from the <code>geometry_msgs</code> package. This is how we import that message, from that package, in order to create velocity commands in Python (which we'll get to shortly...)</li> <li>Before we do anything we need to initialise our node to register it on the ROS network with a name. We're calling it \"move_waffle\" in this case, and we're using <code>anonymous=True</code> to ensure that there are no other nodes of the same name already registered on the network.</li> <li>We want our main <code>while</code> loop (when we get to that bit) to execute 10 times per second (10 Hz), so we create a <code>rate</code> object here which will be used to control the rate of the main loop later...</li> <li>Here we are setting up a publisher to the <code>/cmd_vel</code> topic so that the node can write <code>Twist</code> messages to make the robot move.</li> <li>We're instantiating a <code>Twist</code> message here and calling it <code>vel</code> (we'll assign velocity values to this in the <code>while</code> loop later on). A <code>Twist</code> message contains six different components that we can assign values to. Any idea what these six values might represent?  </li> <li>What time is it right now? (This will be useful to compare against in the while loop.)</li> <li>We're entering the main <code>while</code> loop now. This <code>rospy.is_shutdown()</code> function will read <code>False</code> unless we request for the node to be stopped (by pressing Ctrl+C in the terminal). Once it turns <code>True</code> the <code>while</code> loop stops.</li> <li>Here we're comparing the time now to the time the last time we checked, to tell us how much time has elapsed (in seconds) since then. We'll use that information to decide what to do...  </li> <li>The \"transition\" state is used to stop the robot (if necessary), and check the time again.</li> <li>In \"state1\" we set velocities that will make the robot move forwards (linear-X velocity only). If the elapsed time is greater than 2 seconds however, we move on to \"state2\".</li> <li>In \"state2\" we set velocities that will make the robot turn on the spot (angular-Z velocity only). In this case, if the elapsed time is greater than 4 seconds, we move back to \"state1\".</li> <li>Regardless of what happens in the <code>if</code> statements above, we always publish a velocity command to the <code>/cmd_vel</code> topic here (i.e. every loop iteration).</li> <li>We created a <code>rate</code> object earlier, and we use this now to make sure that each iteration of this <code>while</code> loop takes exactly the right amount of time to maintain the rate of execution that we specified earlier (10 Hz).</li> <li> <p>Here we're importing some mathematical operators that might be useful... </p> Mathematical Operation Python Implementation \\(\\sqrt{a+b}\\) <code>sqrt(a+b)</code> \\(a^{2}+(bc)^{3}\\) <code>pow(a, 2) + pow(b*c, 3)</code> \\(\\pi r^2\\) <code>pi * pow(r, 2)</code> </li> </ol> <p>Click on the  icons above to expand the code annotations. Read these carefully to ensure that you understand what's going on and how this code works.</p> </li> <li> <p>Now, go back to TERMINAL 1 and run the code.</p> <p>Note</p> <p>Make sure the robot is on the floor and has enough room to roam around before you do this!</p> <p>TERMINAL 1: <pre><code>rosrun com2009_team999 simple_move_square.py\n</code></pre></p> <p>Observe what the robot does. When you've seen enough, enter Ctrl+C in TERMINAL 1 to stop the node from running, which should also stop the robot from moving.</p> </li> <li> <p>As the name may suggest, the aim here is to make the robot follow a square motion path. What you may have observed when you actually ran the code is that the robot doesn't actually do that! We're using a time-based approach to make the robot switch between two different states continuously:</p> <ol> <li>Moving forwards</li> <li>Turning on the spot</li> </ol> <p>Have a look at the code to work out how much time the robot will currently spend in each state.</p> </li> <li> <p>The aim here is to make the robot follow a 0.5m x 0.5m square motion path.  In order to properly achieve this you'll need to adjust the timings, or the robot's velocity, or both. Edit the code so that the robot actually follows a 0.5m x 0.5m square motion path!</p> </li> </ol>"},{"location":"waffles/exercises/#slam","title":"SLAM","text":"<p>Simultaneous Localisation and Mapping (SLAM) is a sophisticated tool that is built into ROS. Using data from the robot's LiDAR sensor, plus knowledge of how far the robot has moved<sup>2</sup> the robot is able to create a map of its environment and keep track of its location within that environment at the same time. IN the exercise that follows you'll see easy it is to implement SLAM on the real robot.  </p>"},{"location":"waffles/exercises/#exSlam","title":"Exercise 7: Using SLAM to create a map of the environment","text":"<ol> <li> <p>In TERMINAL 1 enter the following command to launch all the necessary SLAM nodes on the laptop:</p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch\n</code></pre></p> Robotics Laptop Tip <p>This command is also available as an alias: <code>tb3_slam</code>!</p> <p>This will launch RViz once again, where you should now be able to see a model of the Waffle from a top-down view surrounded by green dots representing the real-time LiDAR data. The SLAM tools will already have begun processing this data to start building a map of the boundaries that are currently visible to the Waffle based on its location in the environment.</p> <p>Note</p> <p>To begin with your robot may just appear as a white shadow (similar to the left-hand image below). It may take some time for the robot to render correctly (like the right-hand image) as the SLAM processes and data communications catch up with one another. </p> <p> </p> <p>This can sometimes take up to a minute or so, so please be patient! If (after a minute) nothing has happened, then speak to a member of the teaching team.</p> </li> <li> <p>Return to TERMINAL 2 and launch the <code>turtlebot3_teleop_keyboard</code> node again. Start to drive the robot around slowly and carefully to build up a complete map of the area.</p> <p>Tip</p> <p>It's best to do this slowly and perform multiple circuits of the whole area to build up a more accurate map.</p> </li> <li> <p>Once you're happy that your robot has built up a good map of its environment, you can save this map using a node called <code>map_saver</code> from a package called <code>map_server</code>:</p> <ol> <li> <p>First, create a new directory within your team's ROS package on the laptop. We'll use this to save maps in. Open up a new terminal instance (TERMINAL 3) and navigate to the root of your team's ROS package with <code>roscd</code> again:</p> <p>TERMINAL 3: <pre><code>roscd com2009_team999\n</code></pre></p> </li> <li> <p>Create a directory in here called <code>maps</code>: </p> <p>TERMINAL 3: <pre><code>mkdir maps/\n</code></pre></p> </li> <li> <p>Navigate into this directory:</p> <p>TERMINAL 3: <pre><code>cd maps/\n</code></pre></p> </li> <li> <p>Then, use <code>rosrun</code> to run the <code>map_saver</code> node from the <code>map_server</code> package to save a copy of your map:</p> <p>TERMINAL 3: <pre><code>rosrun map_server map_saver -f {map_name}\n</code></pre></p> <p>Replacing <code>{map_name}</code> with an appropriate name for your map. This will create two files: </p> <ol> <li>a <code>{map_name}.pgm</code> </li> <li>a <code>{map_name}.yaml</code> file</li> </ol> <p>...both of which contain data related to the map that you have just created.</p> </li> <li> <p>The <code>.pgm</code> file can be opened using an application called <code>eog</code> on the laptop: </p> <p>TERMINAL 3: <pre><code>eog {map_name}.pgm\n</code></pre></p> </li> </ol> </li> <li> <p>Return to TERMINAL 1 and close down SLAM by pressing Ctrl+C. The process should stop and RViz should close down.</p> </li> <li> <p>Close down the Keyboard Teleop node in TERMINAL 2 as well if that's still running.</p> </li> </ol>"},{"location":"waffles/exercises/#wrapping-up","title":"Wrapping Up","text":""},{"location":"waffles/exercises/#exGitPush","title":"Exercise 8: Pushing Changes to Your ROS Package Back to GitHub","text":"<p>Having completed the above exercises your team's ROS package should now contain a new ROS Node called <code>simple_move_square.py</code>, a new directory called <code>maps</code>, and a couple of map files within this. You can commit these changes to your repo now using Git, and then push these to GitHub. While the files themselves aren't particularly important for Assignment #2, this will at least illustrate the process for pushing changes from the laptop in future.</p> <p>WiFi</p> <p>Once again, make sure the laptop is now connected to \"eduroam\" in order to be able to access the internet.</p> <ol> <li> <p>Head back to TERMINAL 3 and make sure that you are located in the root of your team's package:</p> <pre><code>roscd com2009_team999/\n</code></pre> </li> <li> <p>Check the status of your Git repo to identify the changes that have been made:</p> <pre><code>git status\n</code></pre> </li> <li> <p>Stage all the changes that have been made:</p> <pre><code>git add .\n</code></pre> </li> <li> <p>Then commit them:</p> <pre><code>git commit -m \"Getting started in the lab with the Waffles\"\n</code></pre> </li> <li> <p>Finally, push these to GitHub:</p> <pre><code>git push origin main\n</code></pre> </li> </ol> <p>        Getting Started Exercises complete!      </p> <p>Continue onto the next page now for the shutdown procedures that you need to follow at the end of each lab session...</p> <ol> <li> <p>Source code available here (if you're interested)\u00a0\u21a9</p> </li> <li> <p>You'll learn much more about \"Robot Odometry\" in Assignment #1 Part 2, and in the COM2009 Lectures.\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/fact-finding/","title":"Fact-Finding Missions!","text":"<p>For the most part, everything that you do with ROS in simulation is directly applicable to the real Waffles too. There are a few subtle differences though, and you should make sure that you are aware of these whilst working on Assignment #2.</p> <p>Complete the following fact-finding missions, which will help you to explore and identify the key differences between the way our TurtleBot3 Waffles work in the real world, compared to how they work in simulation.</p> <p>Each mission is linked to a particular part of the Assignment #1 course and, ideally, you should have completed the Assignment #1 tasks before completing the corresponding fact finding mission below. </p> <p>Be mindful of the differences that we are trying to highlight here and the implications that they will have on the applications that you develop for Assignment #2. </p>"},{"location":"waffles/fact-finding/#mission-1-publishing-velocity-commands","title":"Mission 1: Publishing Velocity Commands","text":"<p>Assignment #1 Checkpoint</p> <p>You'll need to have completed Part 2 Exercise 3 before starting on this mission.</p> <p>In the above Assignment #1 exercise you learnt how to publish velocity commands from the command-line to make the robot move in simulation. Repeat this in simulation if you need a reminder on how it all worked.</p> <p>Then, follow exactly the same steps but with the real Waffle this time... What do you notice about how the real robot moves when you issue a correctly formulated <code>rostopic pub</code> command, e.g.:</p> <pre><code>rostopic pub /cmd_vel geometry_msgs/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre> <p>(replacing some <code>0.0</code>s above with applicable values)</p> <p>Next, look at the usage information for the <code>rostopic pub</code> command by entering:</p> <pre><code>rostopic pub -h\n</code></pre> <p>Try to work out how to control the publishing rate by adding a command-line argument to the end of the <code>rostopic pub</code> commands. Use this to specify a publishing rate of 1 hz and see what happens (in terms of how the robot moves). Next, specify a rate of 10 hz and see how this changes things.</p> <p>When you stop the <code>rostopic pub</code> command what happens to the robot, and how does this differ to what happens when you do the same thing in simulation?</p> <p>Question</p> <p>Having answered the questions above, what implications might this have for any ROS nodes that you create to control the velocity of a real robot? </p>"},{"location":"waffles/fact-finding/#mission2","title":"Mission 2: The Camera Image Topic","text":"<p>Assignment #1 Checkpoint</p> <p>You'll need to have completed the whole of Part 6 before starting on this mission.</p> <p>In Part 6 you worked extensively with the robot's camera and its images, which were published to the <code>/camera/rgb/image_raw</code> topic.</p> <p>Warning</p> <p>The name of the camera image topic is not the same on the real robots!</p> <p>On the laptop, use ROS command-line tools such as <code>rostopic list</code> and <code>rostopic info</code> to interrogate the real robot ROS Network and identify the name of the alternative camera image topic that is used here.</p>"},{"location":"waffles/fact-finding/#mission-3-camera-image-resolution","title":"Mission 3: Camera Image Resolution","text":"<p>Assignment #1 Checkpoint</p> <p>You'll need to have completed the questions before Part 6 Exercise 1 before starting on this mission.</p> <p>At the start of Part 6 we explore the messages published to the robot's camera image topic. Here you need to work out which part of these messages indicate the resolution of the camera images (i.e.: the <code>height</code> and <code>width</code> of the images, in pixels). You may recall what this was, but if not, go back and interrogate this again to find out what resolution the simulated robot's camera images are transmitted at (you'll need to use <code>rostopic echo</code>). </p> <p>Warning</p> <p>The real robot's camera captures images at a different image resolution! </p> <p>On the Robotics Laptop use <code>rostopic echo</code> again, to interrogate the real robot ROS network and identify the <code>height</code> and <code>width</code> of the camera images that are captured by our real robot's camera.</p> <p>Note</p> <p>This will have implications when you come to apply image cropping techniques... the same cropping procedures may not work the same way for nodes run in simulation, compared to those running on a real robot!</p>"},{"location":"waffles/fact-finding/#mission-4-out-of-range-lidar-data","title":"Mission 4: Out of Range LiDAR Data","text":"<p>Assignment #1 Checkpoint</p> <p>You'll need to have completed the section on \"Interpreting <code>/LaserScan</code> Data\" (Part 3) before starting on this mission.</p> <p>The robot's LiDAR sensor can only obtain measurements from objects within a certain distance range. In Part 3 we look at how to work out what this range is, using the <code>rostopic echo</code> command. Apply the same techniques to the real robot now to discover the maximum and minimum distances that the real robot's LiDAR sensor can measure.</p> <p>If the LiDAR sensor detects an object that falls within this range then it will report the exact distance to this object (in meters). Conversely, if it doesn't detect anything within this range then it will report a default out-of-range value instead. In simulation, the out-of-range value is <code>inf</code>.</p> <p>Warning</p> <p>The out-of-range value reported by the real robot's LiDAR sensor is not <code>inf</code>!</p> <p>Use the <code>rostopic echo</code> command to interrogate the ROS network running between your real Waffle and the robotics laptop, and find out what out-of-range value is used here.</p>"},{"location":"waffles/fact-finding/#mission-5-object-detection","title":"Mission 5: Object Detection","text":"<p>Assignment #1 Checkpoint</p> <p>You'll need to have completed Part 6 Exercise 3 before starting on this mission.</p> <p>In general, image detection gets a little more challenging in the real-world, where the same object might appear (to a robot's camera) to have slightly different colour tones under different light conditions, from different angles, in different levels of shade, etc. In simulation, you may build an extremely effective <code>colour_search.py</code> node to detect each of the four coloured pillars in the <code>tuos_simulations/coloured_pillars</code> world. See how well this now works in the real world now by running the same code on your real Waffle.</p> <p>Questions</p> <ol> <li>Without changing any of your code, is the robot able to detect any of our real coloured pillars in the robot arena?</li> <li>Can you make any changes to adapt this and make it work more reliably in a real-world environment?</li> </ol>"},{"location":"waffles/fact-finding/#summary","title":"Summary","text":"<p>When working on Assignment #2 you will naturally do a fair bit of the development work in simulation, where it's easier to test things out and less disastrous if things go wrong! Overall, you'll be able to develop things much faster this way, and you can do this outside of your weekly lab sessions too. Whilst you're doing this though, keep in mind all the differences that you have identified during the above fact-finding missions, so that there are less nasty surprises when you come to deploy your ROS applications on the real Waffles. </p> <p>Throughout the design phase, think about how your applications could be developed more flexibly to accommodate these variations, or how you could design things so that only small/quick changes/switches need to be made when you transition from testing in simulation, to deploying in the real world. </p>"},{"location":"waffles/intro/","title":"Introduction","text":""},{"location":"waffles/intro/#handling-the-robots","title":"Handling the Robots","text":"<p>Health and Safety</p> <p>You must have completed a health and safety quiz before working with the robots for the first time. This quiz is available on Blackboard.</p> <p></p> <p>As you can see from the figure above, the robots have lots of exposed sensors and electronics and so you must take great care when handling them to avoid the robots becoming damaged in any way.  When handling a robot, always hold it by either the black Waffle Layers, or the vertical Support Pillars (as highlighted in the figure above).</p> <p>Important</p> <p>Do not pick the robot up or carry it by the camera or LiDAR sensor! These are delicate devices that could be easily damaged!</p> <p>A lot of the electronics are housed on the middle waffle layer. Try not to touch any of the circuit boards, and take care not to pull on any of the cabling or try to remove or rehouse any of the connections. If you have any concerns with any of the electronics or cabling, if something has come loose, or if your robot doesn't seem to be working properly then ask a member of the teaching team to have a look for you.</p> <p>The robots will be provided to you with a battery already installed and ready to go. Don't try to disconnect or remove the battery yourselves! The robot will beep when the battery is low, and if this happens ask a member of the team to get you a replacement (we have plenty).</p>"},{"location":"waffles/intro/#laptops","title":"The Robotics Laptops","text":"<p>You'll be provided with one of our pre-configured Robotics Laptops in the lab when working with the real Waffles. These Laptops (and the Robots) run Ubuntu 20.04 with ROS Noetic. </p> <p>On the laptops there is a \"student\" user account that you'll use when working in the lab. The laptop should log you into this user account automatically on startup, but we'll provide you with the account password as well, during the lab sessions, should you need it.</p>"},{"location":"waffles/intro/#dialab","title":"WiFi","text":"<p>The Robots and Laptops connect to a dedicated wireless network running in the Diamond called 'DIA-LAB'. There are a few things that you need to know about this:</p> <ul> <li>Laptops must be connected to the DIA-LAB network in order to establish a ROS network between them and the robots.</li> <li>Laptops do not have internet access when connected to DIA-LAB.</li> <li>You'll need to connect the laptop to eduroam (or use another computer) to access any external resources (such as the instructions on this site).</li> </ul> <p>Credentials for DIA-LAB and eduroam have already been set on the laptops, allowing you to connect to either network straight away, but speak to a member of the teaching team if you are having any issues.</p>"},{"location":"waffles/intro/#ide-vs-code","title":"IDE: VS Code","text":"<p>Visual Studio Code is installed on the laptops for you to use when working on your ROS applications for the assignment tasks. Launch VS Code from any terminal by simply typing <code>code</code>. You can also launch it by clicking the icon in the favourites bar on the left-hand side of the screen:</p> <p></p>"},{"location":"waffles/launching-ros/","title":"Launching ROS","text":"<p>The first step is to launch ROS on the Waffle.</p> <p>Important</p> <p>Launching ROS on the Waffle enables the ROS Master<sup>1</sup>. The ROS Master always runs on the Waffle. It's therefore important to complete the steps on this page in full before you do anything else, otherwise the ROS Master will not be running, the robot's core functionality won't be active, and you won't be able to do anything with it! </p>"},{"location":"waffles/launching-ros/#step-1-identify-your-waffle","title":"Step 1: Identify your Waffle","text":"<p>Robots are named as follows:</p> <pre><code>dia-waffleX\n</code></pre> <p>... where <code>X</code> indicates the 'Robot Number' (a number between 1 and 50). Make sure you know which robot you are working with, or check the label printed on top of it!</p>"},{"location":"waffles/launching-ros/#step-2-pairing-your-waffle-to-a-laptop","title":"Step 2: Pairing your Waffle to a Laptop","text":"<p>As discussed earlier, you'll be provided with one of our Robotics Laptops to work with in the lab, and the robot needs to be paired with this in order for the two to work together.  </p> <ol> <li> <p>Open up a terminal instance on the laptop, either by using the Ctrl+Alt+T keyboard shortcut, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p> </p> </li> <li> <p>We'll use our purpose-built <code>waffle</code> CLI to handle the pairing process. Run this in the terminal by entering the following command to pair the laptop and robot:</p> <p><pre><code>waffle X pair\n</code></pre> Replacing <code>X</code> with the number of the robot that you are working with.</p> </li> <li> <p>You may see a message like this early on in the pairing process:</p> <p> </p> <p>If so, just type <code>yes</code> and then hit Enter to confirm that you want to continue.</p> </li> <li> <p>Enter the password for the robot when requested (we'll tell you what this is in the lab!)</p> <p>Note</p> <p>You won't see anything change on the screen when you are entering the password. This is normal, just keep typing!!</p> </li> <li> <p>Once the pairing process is finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>Then, in the same terminal, enter the following command:</p> <p><pre><code>waffle X term\n</code></pre> (again, replacing <code>X</code> with the number of your robot).</p> <p>A green banner should appear across the bottom of the terminal window:</p> <p> </p> <p>This is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> </ol>"},{"location":"waffles/launching-ros/#step-3-launching-ros","title":"Step 3: Launching ROS","text":"<p>Launch ROS (and the ROS Master) on the robot by entering the following command:</p> <pre><code>roslaunch tuos_tb3_tools ros.launch\n</code></pre> Tip <p>To save you typing this command out in full all the time, we've created a handy bash alias for it! You can therefore use <code>tb3_bringup</code> instead.</p> <p>After a short while, you should see a message like this:</p> <pre><code>[INFO] [#####] --------------------------\n[INFO] [#####] dia-waffleX is up and running!\n[INFO] [#####] -------------------------- \n</code></pre> <p> <p>ROS is now up and running on the robot, and you're ready to go!</p> <p></p> <p>You shouldn't need to interact with this terminal instance any more now, but after about 20 seconds the screen will clear, and you'll be presented with some real-time info related to the status of the robot. Keep this terminal instance open in the background and keep an eye on the <code>Voltage</code> indicator in particular:</p> <pre><code>Voltage: 12.40V [100%]\n</code></pre> <p>Low Battery </p> <p>The robot's battery won't last a full 2-hour lab session!!</p> <p>When the capacity indicator reaches around 15% then let a member of the teaching team know and we'll replace the battery for you. (It's easier to do it at this point, rather than waiting until it runs completely flat.)</p>"},{"location":"waffles/launching-ros/#at-the-end-of-each-lab-session","title":"At the End of Each Lab Session","text":"<p>When you've finished working with a robot it's really important to shut it down properly before turning off the power switch. Please refer to the safe shutdown procedures for more info.</p> <ol> <li> <p>What is the ROS Master!? We'll talk about that on the next page (here)\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/shutdown/","title":"Shutdown Procedures","text":""},{"location":"waffles/shutdown/#deleting-your-ros-package","title":"Deleting your ROS Package","text":"<p>Remember that the laptops use an account that everyone in the class has access to. You therefore might want to delete your team's ROS package from the laptop at the end of each lab session, and clone it back on again at the start of the following lab session. </p> <p>Warning</p> <p>Make sure you've pushed any changes to GitHub before deleting your package!</p> <p>Deleting your package (by following the instructions below) won't delete your SSH key from the laptop, so you won't need to do all the SSH key-gen processes again, and your SSH key will still be protected with the secret passphrase that you set up when generating the SSH Key to begin with (assuming that you are working on the same laptop, of course!) </p> <p>Info</p> <p>Not working with the same laptop? You can find all the instructions on how to clone your package onto a laptop using SSH here.(TODO)</p> <p>Delete your package by simply running the following command from any terminal on the laptop:</p> <pre><code>rm -rf ~/catkin_ws/src/com2009_team999\n</code></pre> <p>... replacing <code>999</code> with your own team's number!</p>"},{"location":"waffles/shutdown/#shutdown-procedures","title":"Shutdown Procedures","text":"<p>As you should know, the Waffles are powered by a Single Board Computer (SBC), which runs a full-blown operating system (Ubuntu 20.04). As with any operating system, it's important to shut it down properly, rather than simply disconnecting the power, to avoid any data loss or other issues. </p> <p>Therefore, once you've finished working with a robot during a lab session, follow the steps below to shut it down.</p> <ol> <li> <p>Open a new terminal instance on the laptop (Ctrl+Alt+T), and enter the following:</p> <p><pre><code>waffle X off\n</code></pre> ... replacing <code>X</code> with the number of the robot that you have been working with.</p> </li> <li> <p>You'll be asked to confirm that you want to shut the robot down: </p> <pre><code>[INPUT] Are you sure you want to shutdown dia-waffleX? [y/n] &gt;&gt;\n</code></pre> <p>Enter <code>y</code> and hit Enter and the robot's SBC will be shut down. </p> </li> <li> <p>Once the blue light on the corner of the SBC goes out, it's then safe to slide the power button to the left to completely turn off the device. </p> <p> </p> </li> <li> <p>Once you've turned off the robot, remember to shut down the laptop too! Do this by clicking the battery icon in the top right of the desktop and selecting the \"Power Off / Log Out\" option in the drop-down menu.</p> <p> </p> </li> </ol> <p>      Hand your robot and laptop back to a member of the teaching team who will put it away for you!    </p>"},{"location":"waffles/tips/bash-aliases/","title":"Bash Aliases","text":"<p>Bash aliases are abbreviations for long terminal commands. Some common ROS commands that we use when working with the robots are pretty long, so we've created a few aliases to make it quicker to launch certain things. Most of these are for commands that you'll run on the laptop, but there are a couple that apply to the robot too. See the table below for the full list of bash aliases that are available to you when working with the robots and the laptops:</p> <p> Alias Full Command Context <code>tb3_teleop</code> <code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch</code> Laptop <code>tb3_bringup</code> <code>roslaunch turtlebot3_bringup turtlebot3_remote.launch</code> Laptop <code>tb3_slam</code> <code>roslaunch turtlebot3_slam turtlebot3_slam.launch</code> Laptop <code>tb3_rviz</code> <code>roslaunch tuos_tb3_tools rviz.launch</code> Laptop <code>tb3_world</code> <code>roslaunch turtlebot3_gazebo turtlebot3_world.launch</code> Laptop <code>tb3_sim</code> <code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch</code> Laptop <code>tb3_bringup</code> <code>roslaunch tuos_tb3_tools ros.launch</code> Robot <code>src</code> <code>source ~/.bashrc</code> Both <p></p>"},{"location":"waffles/tips/laptops/","title":"Working with your ROS Packages on the Robotics Laptops (with GitHub and SSH)","text":"<p>You'll need to transfer your ROS package(s) to a robot laptop whenever you want to work on a real robot in the real robot arena during the labs. </p> <p>Having your ROS package on GitHub makes it much easier to transfer between simulation (e.g. WSL-ROS, for example) and the real robots. Using SSH keys, you can clone your team's ROS package to the robot laptops, make commits and push these back up to GitHub without needing to provide your GitHub username and a personal access token every time. This makes life a lot easier! The following steps describe the process you should follow (adapted from GitHub Docs).</p> <p>WiFi</p> <p>You'll need to connect the Robotics Laptop to \"eduroam\" when doing this!</p>"},{"location":"waffles/tips/laptops/#check-if-you-already-have-an-ssh-key-on-the-laptop","title":"Check if you already have an SSH Key on the Laptop","text":"<p>These instructions are adapted from this GitHub Docs page.</p> <p>If you're generating an SSH key for the first time, then you can skip this step and go straight to the next section: \"Generating an SSH key (on the Laptop)\". If you've already generated an SSH Key on the laptop previously then check it's still there before you go any further by following these steps...</p> <ol> <li> <p>Open a terminal on the laptop.</p> Tip <p>You can use the Ctrl+Alt+T keyboard combination to open a terminal!</p> </li> <li> <p>Use the <code>ls</code> command as follows, to see if your SSH key already exists on the laptop:</p> <pre><code>ls -al ~/.ssh\n</code></pre> <p>This will provide you with a list of all the SSH keys on the laptop. Your team's key should have the same name as your ROS package (if you followed the steps correctly when you created the key previously), and so you should see your key in the list, i.e.: <code>com2009_team{}.pub</code>.</p> </li> <li> <p>If your key is there, then you're good to go... you may need to Clone your ROS package onto the Laptop again if you deleted it the last time you were working on the laptop.</p> </li> <li> <p>If you can't see your key in the list, then you'll need to follow all the steps on this page, starting with Generating an SSH key (on the Laptop).</p> </li> </ol>"},{"location":"waffles/tips/laptops/#ssh-keygen","title":"Generating an SSH key (on the Laptop)","text":"<ol> <li> <p>From a terminal instance on the laptop navigate to the <code>~/.ssh</code> folder:</p> <pre><code>cd ~/.ssh\n</code></pre> </li> <li> <p>Create a new SSH key on the laptop, using your GitHub email address:</p> <pre><code>ssh-keygen -t ed25519 -C \"your.email@sheffield.ac.uk\"\n</code></pre> <p>Replacing <code>your.email@sheffield.ac.uk</code> with your GitHub email address.</p> <p></p> </li> <li> <p>You'll then be asked to \"Enter a file in which to save the key\". This needs to be unique, so enter the name of your ROS package. For the purposes of this example, let's assume yours is called <code>com2009_team999</code>.</p> </li> <li> <p>You'll then be asked to enter a passphrase. This is how you make your SSH key secure, so that no other teams using the same laptop can access and make changes to your team's package/GitHub repo. You'll be asked to enter this whenever you try to commit/push new changes to your ROS package on GitHub. Decide on a passphrase and share this with your team ONLY. </p> </li> <li> <p>Next, start the laptop's ssh-agent:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> </li> <li> <p>Add your SSH private key to the laptop's ssh-agent. You'll need to enter the name of the SSH key file that you created in the earlier step (e.g.: <code>com2009_team999</code>)</p> <pre><code>ssh-add ~/.ssh/com2009_team999\n</code></pre> <p>Replacing <code>com2009_team999</code> with the name of your own SSH key file, of course!</p> </li> <li> <p>Then, you'll need to add the SSH key to your account on GitHub...</p> </li> </ol>"},{"location":"waffles/tips/laptops/#adding-an-ssh-key-to-your-github-account","title":"Adding an SSH key to your GitHub account","text":"<p>These instructions are replicated from this GitHub Docs page.</p> <ol> <li> <p>On the laptop, copy the SSH public key that you created in the previous steps to your clipboard.</p> <p>Do this from a terminal on the laptop, using <code>cat</code>:</p> <pre><code>cat ~/.ssh/com2009_team999.pub\n</code></pre> <p>replacing <code>com2009_team999</code> with the name of your SSH key file.</p> <p>The content of the file will then be displayed in the terminal... copy it from here.</p> <p>Tips</p> <ol> <li>To copy text from inside a terminal window use Ctrl+Shift+C</li> <li> <p>You could also open the file in VS Code and copy it from there:</p> <pre><code>code ~/.ssh/com2009_team999.pub\n</code></pre> </li> </ol> </li> <li> <p>Go to your GitHub account in a web browser. In the upper-right corner of any page, click your profile photo, then click Settings.</p> </li> <li> <p>In the \"Access\" section of the sidebar, click SSH and GPG keys.</p> </li> <li> <p>Click New SSH key.</p> </li> <li> <p>Enter a descriptive name for the key in the \"Title\" field, e.g. <code>com2009_dia-laptop1</code>.</p> </li> <li> <p>Select <code>Authentication Key</code> as the \"Key Type.\"</p> </li> <li> <p>Paste the text from your SSH Public Key file into the \"Key\" field.</p> </li> <li> <p>Finally, click the \"Add SSH Key\" button.</p> </li> </ol>"},{"location":"waffles/tips/laptops/#ssh-clone","title":"Cloning your ROS package onto the Laptop","text":"<p>With your SSH keys all set up, you can now clone your ROS package onto the laptop. </p> <p>There's a Catkin Workspace on each of the robot laptops and (much the same as in your own local ROS environment) your package must reside within this workspace!</p> <ol> <li> <p>From a terminal on the laptop, navigate to the Catkin Workspace <code>src</code> directory:</p> <pre><code>cd ~/catkin_ws/src\n</code></pre> </li> <li> <p>Go to your ROS package on GitHub. Click the Code button and then select the SSH option to reveal the SSH address of your repo. Copy this. </p> </li> <li> <p>Head back to the terminal instance on the laptop to then clone your package into the <code>catkin_ws/src/</code> directory using <code>git</code>:</p> <pre><code>git clone {REMOTE_SSH_ADDRESS}\n</code></pre> <p>Where <code>{REMOTE_SSH_ADDRESS}</code> is the SSH address that you have just copied from GitHub.</p> </li> <li> <p>Run Catkin Build to make sure that any resources within your package that need to be compiled (custom ROS messages, etc.) are compiled onto the laptop so that they can be used locally:</p> <pre><code>catkin build com2009_team999\n</code></pre> <p>...again, replacing <code>com2009_team999</code> with your team's package name.</p> </li> <li> <p>Then, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>Navigate into your package and run the following commands to set your identity, to allow you to make commits to your package repo:</p> <p><pre><code>cd com2009_team999/\n</code></pre> <pre><code>git config user.name \"your name\"\n</code></pre> <pre><code>git config user.email \"your email address\"\n</code></pre></p> </li> </ol> <p>You should then be able to commit and push any updates that you make to your ROS package while working on the laptop, back to your remote repository using the secret passphrase that you defined earlier!</p>"},{"location":"waffles/tips/laptops/#deleting-your-ros-package-after-a-lab-session","title":"Deleting your ROS package after a lab session","text":"<p>Remember that the Robotics Laptops use an account that everyone in the class has access to. You might therefore want to delete your package from the laptop at the end of each lab session. It's very easy to clone it back onto the laptop again by following the steps above at the start of each lab session. Deleting your package (by following the instructions below) won't delete your SSH key from the laptop though, so you won't need to do all that again, and your SSH key will still be protected with the secret passphrase that you set up when generating the SSH Key to begin with (assuming that you are working on the same laptop, of course!) </p> <p>Warning</p> <p>Make sure you've pushed any changes to GitHub before deleting your package!</p> <p>Delete your package by simply running the following command from any terminal on the laptop:</p> <pre><code>rm -rf ~/catkin_ws/src/com2009_team{}\n</code></pre> <p>... replacing <code>{}</code> with your own team's number!</p>"},{"location":"waffles/tips/laptops/#returning-in-a-subsequent-lab-session","title":"Returning in a Subsequent Lab Session","text":"<p>Your team will be provided with the same Robotics Laptop for each lab session. Having completed all the steps above in a previous lab session, you should be able to return to the laptop, re-clone your package and continue working with relative ease...</p> <ol> <li> <p>The private SSH key that you created in a previous lab session (and secured with a passphrase) should still be saved on the laptop. Check that this is the case by first running the following command:</p> <p><pre><code>ls -al ~/.ssh\n</code></pre> If you can see your team's ssh key in the list then you're good to go. If not you'll need to go back here and follow the steps to create it again.</p> </li> <li> <p>Next, start the laptop's ssh-agent and re-add your team's private key:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> <pre><code>ssh-add ~/.ssh/com2009_team{}\n</code></pre> <p>Replacing <code>{}</code> with your team number.</p> </li> <li> <p>Then, navigate to the Catkin Workspace <code>src</code> directory:</p> <pre><code>cd ~/catkin_ws/src\n</code></pre> </li> <li> <p>Clone your package into here, using the SSH address of your package on GitHub:</p> <pre><code>git clone {REMOTE_SSH_ADDRESS}\n</code></pre> <p>You'll be asked for your secret passphrase, hopefully you remember it!</p> </li> </ol>"},{"location":"waffles/tips/sim-mode/","title":"Running the Laptops in Simulation Mode","text":"<p>By default, the laptops are set up to work with the real robots, but it is possible to switch them into \"Simulation Mode\" in order to work with ROS and the Waffle in simulation instead. All the simulations that you work with in Assignment #1 are available to launch on the laptops, once you're in simulation mode. </p> <p>To switch into Simulation Mode, enter the following command:</p> <pre><code>robot_mode sim\n</code></pre> <p>... which should present you with the following message:</p> <pre><code>Switching into 'Simulation Mode' ...\n</code></pre> <p>Note</p> <p>When you're ready to switch back to a real robot, the <code>waffle</code> CLI tool will switch you back into \"Real Robot Mode\" automatically! Just follow the steps here.</p>"}]}