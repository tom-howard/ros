{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#ros-the-robot-operating-system","title":"ROS: The Robot Operating System","text":"<p>Robotics in the Diamond @ The University of Sheffield</p> <p> </p> <p>By Tom Howard Department of Multidisciplinary Engineering Education  </p> <p>(Image courtesy of Andy Brown)</p> <p>Find out more...</p> <p></p>"},{"location":"about/","title":"Welcome","text":"<p>This is the home of some practical lab courses developed by Dr Tom Howard, a University Teacher at the University of Sheffield. The material here is designed for teaching ROS (the Robot Operating System), and has been developed to support a range of practical teaching activities that take place in The Diamond for a range of different undergraduate degree courses.</p> <p>This material was primarily designed, by Tom, for COM2009 \"Robotics\": a second-year Computer Science module led by the Department of Computer Science. It's grown a bit over the last couple of years though, and we're now also using this site to support the teaching of a masters-level module for the department of Automatic Control and Systems Engineering (ACS6121), and some labs for an AMRC Training Centre module (AMR31001).</p> <p>The courses here are designed to teach you how to use ROS to program robots, using a mix of simulation-based learning and real robot hardware. Typically, we start in simulation first, and we've got a WSL-based simulation environment that we use for this. Everything you'll learn in simulation is directly applicable to real robots too though, and - on all of these courses - you'll be able to apply your new-found ROS knowledge to the real TurtleBot3 Waffle Robots that we have in The Diamond.</p>"},{"location":"about/acknowledgements/","title":"Acknowledgements","text":"<p>These course materials have been informed by a range of other (mostly free and open-access) resources. We recommend you check out these as well, if you're doing one of our courses and want to dig a little deeper:</p> <ul> <li>The Official ROS Tutorials.</li> <li>The Gaitech Online ROS Tutorials.</li> <li>\"What Is ROS?\" and other blogs from the Robotics Back-End.</li> <li>Another excellent (and free) eBook: A Gentle Introduction to ROS by Jason M. O'Kane.</li> <li>The huge range of online ROS courses provided by The Construct.</li> </ul>"},{"location":"about/changelog/","title":"Version History","text":"<p> Iteration Academic Year Details 3 2022-23 Moved everything (from the Wiki) across to this new site and made lots of tweaks and improvements along the way. Two new labs for AMR31001 have been added. 2 2021-22 Updated for a new release of the WSL-ROS Environment: now running Ubuntu 20.04 and ROS Noetic. All code templates now updated for Python 3, including nice things like f-strings for all string formatting. Week 3 Exercise 4 (Autonomous Navigation) has been revised to use command-line calls to the <code>/move_base_simple</code> action server to make the robot move to navigation goals (rather than using the GUI tools in RViz), in the hope that this will make it easier to see how the same thing could be achieved programmatically instead. Removed an exercise in Week 5 to make it shorter (because it was a bit of a long one originally), but introduced a couple of optional ones instead for those who wish to delve further. 1 2020-21 Initial release of the COM2009 practical ROS course and the COM2009 Wiki. Based on the brand new WSL-ROS environment (running Ubuntu 18.04 and ROS Melodic). <p></p>"},{"location":"about/license/","title":"License","text":"<p> This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License. </p> <p>You are free to distribute, remix, adapt, and build upon this work (for non-commercial purposes only) as long as credit is given to the original author.</p>"},{"location":"about/robots/","title":"Introducing the Robots","text":""},{"location":"about/robots/#the-turtlebot3-waffle","title":"The TurtleBot3 Waffle","text":""},{"location":"about/robots/#turtlebot-what","title":"Turtlebot what?!","text":"<p>To teach ROS here we use the TurtleBot3 Waffle robot, made by Robotis. This is the 3rd Generation Robot in the TurtleBot family (which has been the reference hardware platform for ROS since 2010). The TurtleBot Robot family exists to provide accessible and relatively low-cost hardware and open-source software on a robot platform, to encourage people to learn robotics and ROS and make it as easy as possible to do so.</p>"},{"location":"about/robots/#ebook","title":"The (Free) TurtleBot3 eBook","text":"<p>The TurtleBot3 Waffle developers (Robotis) have written a book on programming robots with ROS. This is available as a free eBook, which you can download here, and we recommend that you do so! This is a great resource which provides a detailed introduction to what ROS is and how it works, as well as a comprehensive \"Beginners Guide\" to ROS programming. The other great thing about this is that it is tailored to the TurtleBot3 Robot specifically, providing examples of how to use a range of TurtleBot3 packages along with a detailed description of how they work.</p> <p>We recommend that you have a look at this book to learn more about the concepts that you are exploring in these courses.</p>"},{"location":"about/robots/#tb3","title":"Our Waffles","text":"<p>Here in the Diamond we have a number of customised TurtleBot3 Waffles specifically for teaching this course:</p> <p> </p> <p>Whether we're working in simulation or with the real thing, the ROS applications that we develop as part of the courses here are directly transferable between the two (mostly!) </p> <p>The robots that we have are slightly different to the standard TurtleBot3 WafflePi that you can buy from Robotis. We've made a few adjustments, and the full specifications are as follows:</p> <p></p> <p>The robots have the following core hardware elements:</p> <ul> <li>An OpenCR Micro-Controller Board to power and control the wheel motors, distribute power to other hardware elements and provide an interface for additional sensors.</li> <li>An UP Squared Single-Board Computer (SBC) with an Intel Processor and 32GB of on-board eMMC storage. This board acts as the \"brain\" of the robot.</li> <li>Independent left and right wheel motors (DYNAMIXEL XM430\u2019s) to drive the robot using a differential drive configuration.</li> </ul> <p>This drive configuration allows the robots to move with the following maximum velocities: </p> <p> Velocity Component Upper Limit Units Linear 0.26 m/s Angular 1.82 rad/s <p></p> <p>In addition to this, the robots are equipped with the following sensors:</p> <ul> <li>A Light Detection and Ranging (or LiDAR) sensor, which spins continuously when the robot is in operation. This uses light in the form of laser pulses to allow the robot to measure the distance to surrounding objects, providing it with a 360\u00b0 view of its environment.</li> <li>An Intel RealSense D435 Camera with left and right imaging sensors, allowing depth sensing as well as standard image capture.</li> <li>A 9-Axis Inertial Measurement Unit (or IMU) on-board the OpenCR Micro Controller board, which uses an accelerometer, gyroscope and magnetometer to measure the robot's specific force, acceleration and orientation. </li> <li>Encoders in each of the DYNAMIXEL wheel motors, allowing measurement of speed and rotation count for each of the wheels.</li> </ul>"},{"location":"about/robots/#ros-version","title":"ROS Version","text":"<p>Our robots run on the most up-to-date version of ROS1: ROS Noetic Ninjemys (or \"ROS Noetic\" for short). Our courses here are therefore based around this version of ROS. ROS1 is best installed on the Ubuntu Operating System for stability, reliability and ease, and ROS Noetic runs on Ubuntu 20.04 (Focal Fossa).</p>"},{"location":"about/robots/#other-tech-in-the-diamond","title":"Other Tech in the Diamond","text":""},{"location":"about/robots/#laptops","title":"Laptops","text":"<p>In the Diamond, we have dedicated Robot Laptops running the same OS &amp; ROS version as above (Ubuntu 20.04 and ROS Noetic). We use these when working with the robots in the lab. See here for more details. </p>"},{"location":"about/robots/#simulation-environment","title":"Simulation Environment","text":"<p>To deliver the simulation-based parts of this course, we've created a custom simulation environment using the Windows Subsystem for Linux (WSL). This has been developed primarily to run on University of Sheffield Managed Desktop Computers, which run Windows 10, but it's also possible to run this on other machines too. We call this simulation environment \"WSL-ROS\".</p>"},{"location":"com2009/","title":"COM2009","text":"<p>"},{"location":"com2009/#com2009-robotics","title":"COM2009: Robotics","text":"<p>The COM2009 (&amp; COM3009) Robotics lab course is a 12-week course which takes place in the Spring Semester. The course is split into two lab assignments:</p> <p></p> <ul> <li>Lab Assignment #1: \"An Introduction to the Robot Operating System (ROS)\" (Weeks 1-6).</li> <li>Lab Assignment #2: \"A Team Robotics Challenge\" (Weeks 7-12).</li> </ul>"},{"location":"com2009/la1/","title":"COM2009 Lab Assignment #1: An Introduction to ROS","text":""},{"location":"com2009/la1/#an-introduction-to-ros","title":"An Introduction to ROS","text":"<ul> <li> <p>Week 1: ROS &amp; Linux Basics</p> <p>In this first week you will learn the basics of ROS and become familiar with some key tools and principles of this framework, allowing you to program robots and work with ROS applications effectively.</p> </li> <li> <p>Week 2: Odometry &amp; Basic Navigation</p> <p>In this session you'll learn how to control a ROS robot's velocity (and thus its position), how to interpret Odometry data and implement some open-loop control nodes.</p> </li> <li> <p>Week 3: Advanced Navigation &amp; SLAM</p> <p>Implement odometry-based velocity control to make a robot follow a pre-defined motion path. Explore the LiDAR sensor, how the data form this device can be of huge benefit for robotics applications, and see this in practice by leveraging the autonomous navigation (and mapping) tools within ROS.</p> </li> <li> <p>Week 4: ROS Services</p> <p>Here, you'll learn how ROS Services can be used in combination with the standard publisher/subscriber principles that you already know about, to control a robot more effectively for certain operations.</p> </li> <li> <p>Week 5: ROS Actions</p> <p>Building on what you learnt about ROS Services last week, we'll now look at ROS Actions, which work similarly, but with key differences.</p> </li> <li> <p>Week 6: Cameras, Machine Vision &amp; OpenCV</p> <p>Here we'll look at how to build ROS nodes that work with images from on-board camera, we'll look at techniques to detect features within these images that can then be used to inform robot decision-making.</p> </li> </ul>"},{"location":"com2009/la1/week1/","title":"Week 1: ROS & Linux Basics","text":"<p>Info</p> <p>You should be able to complete all the exercises on this page within a two-hour lab session.</p>"},{"location":"com2009/la1/week1/#introduction","title":"Introduction","text":""},{"location":"com2009/la1/week1/#aims","title":"Aims","text":"<p>In this first week you will learn the basics of ROS and become familiar with some key tools and principles of this framework, which will allow you to program robots and work with ROS applications effectively.  For the most part, you will interact with ROS using the Linux command line and so you will also become familiar with some key Linux command line tools that will help you.  Finally, you will learn how to create some basic ROS Nodes using Python and get a taste of how ROS topics and messages work.</p>"},{"location":"com2009/la1/week1/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:  </p> <ol> <li>Control a TurtleBot3 Robot, in simulation, using ROS.</li> <li>Launch ROS applications using <code>roslaunch</code> and <code>rosrun</code>.</li> <li>Interrogate running ROS applications using key ROS command line tools.</li> <li>Create a ROS package comprised of multiple nodes and program these nodes (in Python) to communicate with one another using ROS Communication Methods.</li> <li>Navigate a Linux filesystem and learn how to do various filesystem operations from within a Linux Terminal.</li> </ol>"},{"location":"com2009/la1/week1/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Launching a simulation and making a ROS robot move</li> <li>Exercise 2: Exploring a ROS Package</li> <li>Exercise 3: Visualising the ROS Network</li> <li>Exercise 4: Exploring ROS Topics and Messages</li> <li>Exercise 5: Creating your own ROS Package</li> <li>Exercise 6: Creating a publisher node</li> <li>Exercise 7: Creating a subscriber node</li> <li>Exercise 8: Creating a launch file</li> </ul>"},{"location":"com2009/la1/week1/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Publisher Code (for Exercise 6)</li> <li>The Subscriber Code (for Exercise 7)</li> </ul>"},{"location":"com2009/la1/week1/#first-steps","title":"First Steps","text":""},{"location":"com2009/la1/week1/#ex1","title":"Exercise 1: Launching a simulation and making a ROS robot move","text":"<ol> <li>If you haven't done so already, launch your WSL-ROS environment by running the WSL-ROS shortcut in the Windows Start Menu (see here for detailed instructions). This should open up a terminal application and an Ubuntu terminal instance.  We'll refer to this terminal instance as TERMINAL 1.</li> <li> <p>In the terminal enter the following command to launch a simulation of a TurtleBot3 Waffle in an empty world:  </p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre></p> </li> <li> <p>A Gazebo simulation window should open and within this you should see a TurtleBot3 Waffle (similar to our real robots that you'll work with later):</p> <p> </p> </li> <li> <p>With your Gazebo Simulation up and running, return to the terminal application and open up a new Ubuntu terminal instance (TERMINAL 2) by pressing the New Tab button: </p> <p> </p> <p>(or, alternatively, press the <code>Ctrl+Shift+T</code> keyboard shortcut).</p> </li> <li> <p>In the new terminal instance enter the following command:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n</code></pre></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around in its simulated environment.</p> </li> </ol>"},{"location":"com2009/la1/week1/#summary","title":"Summary","text":"<p>You have so far launched two separate ROS applications using the <code>roslaunch</code> command. <code>roslaunch</code> is one way to launch ROS programs. As you should have observed from the above examples, we use this command in the following way:</p> <pre><code>roslaunch {package name} {launch file}\n</code></pre> <p>The command takes two parameters as inputs: <code>{package name}</code> is the name of the ROS package that contains the functionality that we want to execute and <code>{launch file}</code> is a file within that package that tells ROS exactly what functionality within the package we want to launch.</p>"},{"location":"com2009/la1/week1/#ros-packages","title":"ROS Packages","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations), all of which relate to some common robot functionality. ROS uses packages as a way to organise all the programs running on a robot. </p> <p>Info</p> <p>The package system is a fundamental concept in ROS and all ROS programs are organised in this way.</p>"},{"location":"com2009/la1/week1/#ex2","title":"Exercise 2: Exploring a ROS Package","text":"<p><code>roscd</code> is a ROS command that allows us to navigate to the directory of any ROS package installed on our system, without us needing to know the path to the package beforehand.</p> <ol> <li> <p>Open up a new terminal instance (TERMINAL 3) and use the <code>roscd</code> command to navigate to the <code>turtlebot3_teleop</code> package directory on the Linux filesystem:</p> <p>TERMINAL 3: <pre><code>roscd turtlebot3_teleop\n</code></pre></p> <p>The terminal prompt should have changed to illustrate where on the filesystem the <code>roscd</code> command has just taken you:</p> <p> </p> </li> <li> <p><code>pwd</code> is a Linux command which tells us the current filesystem location of our terminal.  Enter this command to confirm what the terminal prompt has told us.</p> <p>So, now we know where the <code>turtlebot3_teleop</code> package is located on our machine, and we can then use more Linux commands to explore this further:</p> </li> <li> <p><code>ls</code> is a Linux command which lists the contents of the current directory.  Use this to list the contents of the <code>turtlebot3_teleop</code> package directory.</p> </li> <li> <p><code>ls</code> on its own will simply list the items in the current directory, try this first.</p> </li> <li> <p>Then, use the <code>-F</code> option to find out a little more:</p> <p>TERMINAL 3: <pre><code>ls -F\n</code></pre></p> <p>You will notice that the output has now changed slightly: items followed by a <code>/</code> are folders (aka \"directories\") and items without the <code>/</code> are files (files will often have a file extension too).</p> <p>Questions</p> <ol> <li>How many items were there in the <code>turtlebot3_teleop</code> package directory?</li> <li>How many of these were directories and how many were files? </li> </ol> <p>Launch files for a package are typically located in a launch folder within the package directory. Did you notice a <code>launch</code> folder in the output of the <code>ls</code> command above?</p> </li> <li> <p><code>cd</code> is a Linux command that allows us to Change the Directory that the terminal is currently located in. Use this to navigate into the <code>turtlebot3_teleop</code> package <code>launch</code> folder and then use <code>ls</code> again to see what's in there. </p> <p>In this folder you should see the <code>turtlebot3_teleop_key.launch</code> file that we executed with the <code>roslaunch</code> command in Exercise 1.  We will now have a look at the contents of this file...</p> </li> <li> <p><code>cat</code> is a Linux command that we can use to display the contents of a file in the terminal.  Use this to display the contents of the <code>turtlebot3_teleop_key.launch</code> file.</p> <p>TERMINAL 3: <pre><code>cat turtlebot3_teleop_key.launch\n</code></pre></p> </li> </ol>"},{"location":"com2009/la1/week1/#package_attributes","title":"Summary","text":"<p>From the output of <code>cat</code> in the step above you should have noticed that the contents of a launch file are contained within a <code>&lt;launch&gt;</code> tag:</p> <pre><code>&lt;launch&gt;\n... &lt;/launch&gt;\n</code></pre> <p>Within that, we also have (amongst other things) a <code>&lt;node&gt;</code> tag which tells ROS exactly what scripts (\"executables\") to launch and how to launch them:</p> <pre><code>&lt;node pkg=\"turtlebot3_teleop\" type=\"turtlebot3_teleop_key\" name=\"turtlebot3_teleop_keyboard\" output=\"screen\"&gt;\n&lt;/node&gt;\n</code></pre> <p>The attributes here have the following meaning:</p> <ul> <li><code>pkg</code>: The name of the ROS package containing the functionality that we want to launch.</li> <li><code>type</code>: The full name of the script (i.e. ROS Node) that we want to execute within that package (including the file extension, if it has one).</li> <li><code>name</code>: A descriptive name that we want to give to the ROS node, which will be used to register it on the ROS Network.</li> <li><code>output</code>: The place where any output from the node will be printed (either screen where the output will be printed to our terminal window, or log where the output will be printed to a log file).</li> </ul>"},{"location":"com2009/la1/week1/#nodes","title":"ROS Nodes","text":"<p>ROS Nodes are executable programs that perform specific robot tasks and operations, such as remote (or \"teleoperational\") control, as we saw in the earlier example. </p> <p>The packages that we will create throughout this course will contain nodes, launch files and other things too, and we'll explore this a little more shortly.</p> <p>The <code>turtlebot3_teleop</code> package that we have just interrogated here however is fairly minimal and only contains launch files... the nodes are actually located elsewhere, in a directory called <code>/opt/ros/noetic/lib/turtlebot3_teleop/</code> (this is just the way things are organised for pre-installed packages).</p> <p>Questions</p> <ol> <li>What is the name of the node that is launched by the <code>turtlebot3_teleop_key.launch</code> file?</li> <li>See if you can find this in <code>/opt/ros/noetic/lib/turtlebot3_teleop/</code>.</li> <li>Use the Linux/ROS command line tools that you have learnt about so far to interrogate the file...</li> </ol> <p>A ROS Robot might have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to communicate and share data with the other nodes on the ROS Network.</p>"},{"location":"com2009/la1/week1/#the-ros-network","title":"The ROS Network","text":"<p>You can use the <code>rosnode</code> ROS command to view all the nodes that are currently active on a ROS Network.</p>"},{"location":"com2009/la1/week1/#ex3","title":"Exercise 3: Visualising the ROS Network","text":"<p>You should currently have three terminal windows active: the first in which you launched the Gazebo simulation (TERMINAL 1), the second with your <code>turtlebot3_teleop_key</code> node active (TERMINAL 2), and the third where you explored the contents of the <code>turtlebot3_teleop</code> package directory (TERMINAL 3).  TERMINAL 3 should now be idle.</p> <ol> <li>In TERMINAL 3 enter <code>cd ~</code> to go back to your home directory (remember that <code>~</code> is an alias for your home directory).</li> <li> <p>Use the following command to have a look at which nodes are currently active on the network:</p> <p>TERMINAL 3: <pre><code>rosnode list\n</code></pre></p> <p>Only a handful of nodes should be listed:</p> <pre><code>/gazebo\n/gazebo_gui\n/rosout\n/turtlebot3_teleop_keyboard\n</code></pre> </li> <li> <p>We can visualise the connections between the active nodes by using the <code>rqt_graph</code> node within the <code>rqt_graph</code> package. We can use <code>rosrun</code> to launch this node directly (you might get some error messages, but don't worry about them):</p> <p>TERMINAL 3: <pre><code>rosrun rqt_graph rqt_graph\n</code></pre></p> <p>A new window should then open, displaying something similar to the following (hover over the diagram to enable colour highlighting):</p> <p> </p> <p>This tool shows us that the <code>/turtlebot3_teleop_keyboard</code> and <code>/gazebo</code> nodes are communicating with one another. The direction of the arrow tells us that the <code>/turtlebot3_teleop_keyboard</code> node is a Publisher and the <code>/gazebo</code> node is a Subscriber. The two nodes communicate via a ROS Topic, in this case the <code>/cmd_vel</code> topic, and on this topic the <code>/turtlebot3_teleop_keyboard</code> node publishes messages.</p> </li> </ol>"},{"location":"com2009/la1/week1/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics using ROS Messages. We were actually publishing messages to a topic when we made the robot move using the Teleop node in the previous exercises.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"com2009/la1/week1/#ex4","title":"Exercise 4: Exploring ROS Topics and Messages","text":"<p>We can find out more about the <code>/cmd_vel</code> topic by using the <code>rostopic</code> ROS command.</p> <ol> <li> <p>In a new terminal instance (TERMINAL 4) type the following:</p> <p>TERMINAL 4: <pre><code>rostopic info /cmd_vel\n</code></pre></p> <p>This should provide an output similar to the following:</p> <pre><code>Type: geometry_msgs/Twist\n\nPublishers:\n    * /turtlebot3_teleop_keyboard (http://localhost:#####/)\nSubscribers:\n    * /gazebo (http://localhost:#####/)\n</code></pre> <p>This confirms what we discovered earlier about the publisher(s) and subscriber(s) to the <code>/cmd_vel</code> topic.  In addition, this also tells us the topic type, or the type of message that is being published on this topic.</p> <p>This tells us a few things:</p> <ol> <li>The <code>/turtlebot3_teleop_keyboard</code> node is currently publishing (i.e. writing data) to the <code>/cmd_vel</code> topic, confirming what we saw from the <code>rqt_graph</code> node before.</li> <li>The <code>/gazebo</code> node is subscribing to the topic. This node is the Gazebo application that's running the simulation of the robot. The node therefore monitors (i.e. subscribes to) the <code>/cmd_vel</code> topic and makes the robot move in the simulator whenever a velocity command is published.</li> <li> <p>The type of message used by the <code>/cmd_vel</code> topic is called: <code>geometry_msgs/Twist</code>. </p> <p>The message type has two parts: <code>geometry_msgs</code> and <code>Twist</code>. <code>geometry_msgs</code> is the name of the ROS package that this message belongs to and <code>Twist</code> is the actual message type. </p> <p>We have just learnt then, that if we want to make the robot move we need to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic.</p> </li> </ol> </li> <li> <p>We can use the <code>rosmsg</code> ROS command to provide further information about this message, or any other message that we may be interested in:</p> <p>TERMINAL 4: <pre><code>rosmsg info geometry_msgs/Twist\n</code></pre></p> <p>From this, we should obtain the following:</p> <pre><code>geometry_msgs/Vector3 linear\n    float64 x\n    float64 y\n    float64 z\ngeometry_msgs/Vector3 angular\n    float64 x\n    float64 y\n    float64 z\n</code></pre> <p>We'll learn more about what this means next week.</p> </li> <li> <p>To finish, shut down any active terminal processes by entering <code>Ctrl+C</code> in any that still have processes running (Terminals 1, 2 and 3). The associated Gazebo and rqt_graph windows should close as a result of this too.</p> </li> </ol>"},{"location":"com2009/la1/week1/#ex5","title":"Exercise 5: Creating your own ROS Package","text":"<p>In a minute or two you will create some simple publisher and subscriber nodes in Python and send messages between them. As we learnt earlier though, ROS applications must be created within packages, and so we need to create a package first in order to start creating our own ROS nodes. </p> <p>ROS provides a tool to create a new ROS package and ensure that all the essential elements are present: <code>catkin_create_pkg</code>.</p> <p>It's important to work in a specific filesystem location when we create and work on our own ROS packages, so that ROS can access and build everything appropriately. These spaces are called \"Catkin Workspaces\" and one has already been created in the WSL-ROS environment, called <code>catkin_ws</code>1:</p> <p><pre><code>/home/student/catkin_ws/\n</code></pre> Or: <pre><code>~/catkin_ws/\n</code></pre></p> <ol> <li> <p>Navigate to the <code>catkin_ws</code> folder by using the Linux <code>cd</code> command. In TERMINAL 1 enter the following:</p> <p>TERMINAL 1: <pre><code>cd ~/catkin_ws/\n</code></pre></p> </li> <li> <p>Inside the catkin workspace there is a directory called <code>src</code> (use the <code>ls</code> command to confirm this). All new packages need to be located in the <code>src</code> folder, so we need to be here when we use the <code>catkin_create_pkg</code> tool to create a new package. So, use the <code>cd</code> command again to navigate to the <code>catkin_ws/src</code> folder:</p> <p>TERMINAL 1: <pre><code>cd src\n</code></pre></p> </li> <li> <p>Now, use the <code>catkin_create_pkg</code> script to create a new package called <code>week1_pubsub</code>, and define <code>std_msgs</code> and <code>rospy</code> as dependencies:</p> <p>TERMINAL 1: <pre><code>catkin_create_pkg week1_pubsub std_msgs rospy\n</code></pre></p> <p>Question</p> <p>What did the <code>catkin_create_pkg</code> tool just do? (Hint: there were four things, and it will have told you about them!)</p> </li> <li> <p>Navigate into this new package directory and use <code>ls</code> to list the content that has been created by the <code>catkin_create_pkg</code> tool.</p> <p>Catkin packages are typically organised in the following way, and have a few essential features that must be present in order for the package to be valid:</p> <pre><code>package_folder/    -- All packages must be self-contained within their own root folder [essential]\n|-launch/          -- A folder for launch files (optional)\n|-src/             -- A folder for source files (python scripts etc)\n|-CMakeLists.txt   -- Rules for compiling the package [essential]\n`-package.xml      -- Information about the package [essential]\n</code></pre> <p>You will have noticed that the <code>catkin_create_pkg</code> tool made sure that the essential features of a Catkin Package were created when we asked it to build the <code>week1_pubsub</code> package above.</p> </li> <li> <p>Before we do anything else, it's good practice to now run <code>CMake</code> on the package (using <code>catkin build</code>) to register it on our ROS system and make sure there are no errors with its definition so far:</p> <p>TERMINAL 1: <pre><code>catkin build week1_pubsub\n</code></pre> Finally, \"re-source\" your environment2 using the following command: <pre><code>source ~/.bashrc\n</code></pre></p> <p>... and you're good to go.</p> <p>Warning</p> <p>You will need run <code>source ~/.bashrc</code> in any other terminals that you have open too, in order for the changes to propagate through to these as well!</p> </li> </ol>"},{"location":"com2009/la1/week1/#ex6","title":"Exercise 6: Creating a publisher node","text":"<ol> <li>Within your <code>week1_pubsub</code> package directory, navigate to the <code>src</code> folder using the <code>cd</code> command.</li> <li> <p><code>touch</code> is a Linux command that we can use to create an empty file. Use this to create an empty file called <code>publisher.py</code>, which we will add content to shortly:</p> <p>TERMINAL 1: <pre><code>touch publisher.py\n</code></pre></p> </li> <li> <p>Use <code>ls</code> to verify that the file has been created, but use the <code>-l</code> option with this, so that the command provides its output in \"a long listing format\":</p> <p>TERMINAL 1: <pre><code>ls -l\n</code></pre></p> <p>This should output something similar to the following:</p> <pre><code>total 0\n-rw-r--r-- 1 student student 0 MMM DD HH:MM publisher.py\n</code></pre> <p>This confirms that the file exists, and the <code>0</code> in the middle of the bottom line there indicates that the file is empty (i.e. its current size is 0 bytes), which is what we'd expect.</p> </li> <li> <p>We therefore now need to open the file and add content to it. As discussed in the WSL-ROS Section, we'll be using Visual Studio Code as our IDE for this work. It's important to launch this in a very specific way in order for it to work properly with the WSL-ROS environment, so follow the instructions here to get this up and running now!</p> <p>Warning</p> <p>Make sure that the \"Remote - WSL\" VS Code extension is enabled within the WSL-ROS environment!!</p> </li> <li> <p>Using the VS Code File Explorer, navigate to your <code>week1_pubsub</code> package directory (<code>~/catkin_ws/src/week1_pubsub/</code>), locate the <code>publisher.py</code> file that you have just created in the <code>/week1_pubsub/src/</code> folder and click on the file to open it. </p> </li> <li> <p>Once opened, copy the code provided here into the empty file and save it. </p> <p>Note</p> <p>It's important that you understand how this code works, so make sure that you read the annotations!</p> </li> <li> <p>Make sure that you've saved the <code>publisher.py</code> file (in VS Code) before trying to run it!</p> <p>Do this by using the <code>Ctrl+S</code> keyboard shortcut, or going to <code>File &gt; Save</code> from the menu at the top of the VS Code screen.</p> </li> <li> <p>We can now run this node using the <code>rosrun</code> ROS command. However, because we closed everything down earlier on, the ROS Master is no longer active. First then, we need to re-launch it manually using <code>roscore</code>:</p> <p>TERMINAL 1: <pre><code>roscore\n</code></pre></p> </li> <li> <p>Then, in TERMINAL 2, use <code>rosrun</code> to execute the <code>publisher.py</code> script that you have just created by providing the relevant information to the <code>rosrun</code> command as follows: <code>rosrun {package name} {script name}</code>, i.e.:</p> <p>TERMINAL 2: <pre><code>rosrun week1_pubsub publisher.py\n</code></pre></p> <p>... Hmmm, something not quite right? If you typed the command exactly as above and then tried to run it, you probably just received the following error:</p> <pre><code>[rosrun] Couldn't find executable named publisher.py below /home/student/catkin_ws/src/week1_pubsub\n[rosrun] Found the following, but they're either not files,\n[rosrun] or not executable:\n[rosrun]   /home/student/catkin_ws/src/week1_pubsub/src/publisher.py\n</code></pre> <p>The clue there is the word \"executable\". When we create a file, using <code>touch</code> it is given certain permissions. Run <code>ls -l</code> again (making sure that your terminal is in the right location: <code>~/catkin_ws/src/week1_pubsub/src/</code>).</p> <p>The first bit tells us about the permissions that are currently set: <code>-rw-r--r--</code>. This tells us who has permission to do what with this file and (currently) the first bit: <code>-rw-</code>, tells us that we (as the user <code>student</code>) have permission to Read or Write to it. There is a third option we can set too though, which is the execute permission, and we can set this using the <code>chmod</code> Linux command...</p> </li> <li> <p>Run the <code>chmod</code> command as follows:</p> <p>TERMINAL 2: <pre><code>chmod +x publisher.py\n</code></pre></p> </li> <li> <p>Now, run <code>ls -l</code> again to see what has changed:</p> <p>TERMINAL 2: <pre><code>ls -l\n</code></pre></p> <p>We have now granted permission for the file to be eXecuted too:</p> <pre><code>-rwxr-xr-x 1 student student 1557 MMM DD HH:MM publisher.py\n</code></pre> </li> <li> <p>OK, now use <code>rosrun</code> again to (hopefully!) run the <code>publisher.py</code> node (remember: <code>rosrun {package name} {script name}</code>).</p> <p>If you see a message in the terminal similar to the following then the node has been launched successfully:</p> <pre><code>[INFO] [#####]: The 'simple_publisher' node is active...\n</code></pre> <p>Phew!</p> </li> <li> <p>We can further verify that our publisher node is running using a number of different tools. Try running the following commands in TERMINAL 3:</p> <ol> <li><code>rosnode list</code>: This will provide a list of all the nodes that are currently active on the system. Verify that the name of our publisher node is visible in this list.</li> <li><code>rostopic list</code>: This will provide a list of the topics that are currently being used by nodes on the system. Verify that the name of the topic that our publisher is publishing messages to is present within this list.</li> </ol> </li> </ol>"},{"location":"com2009/la1/week1/#rostopic","title":"Using the <code>rostopic</code> command","text":"<p>So far we have used the <code>rostopic</code> ROS command with two additional arguments:</p> <ul> <li><code>list</code>: to provide us with a list of all the topics that are active on our ROS system, and</li> <li><code>info</code>: to provide us with information on a particular topic of interest.</li> </ul> <p>We can use the autocomplete functionality of the Linux terminal to provide us with a list of all the available options that we can use with the <code>rostopic</code> command.  To do this you can type <code>rostopic</code> followed by a <code>Space</code> and then press the <code>Tab</code> key twice:</p> <pre><code>rostopic[SPACE][TAB][TAB]\n</code></pre> <p>You should then be presented with a list of the available arguments for the <code>rostopic</code> command:</p> <p></p> <ul> <li> <p><code>rostopic hz {topic name}</code> provides information on the frequency (in Hz) at which messages are being published to a topic:</p> <pre><code>rostopic hz /chatter\n</code></pre> <p>This should tell us that our publisher node is publishing messages to the <code>/chatter</code> topic at (or close to) 10 Hz, which is exactly what we ask for in the <code>publisher.py</code> file (in the <code>__init__</code> part of our <code>Publisher</code> class). Press <code>Ctrl+C</code> to stop this command.</p> </li> <li> <p><code>rostopic echo {topic name}</code> shows the messages being published to a topic:</p> <pre><code>rostopic echo /chatter\n</code></pre> <p>This will provide a live stream of the messages that our <code>publisher.py</code> node is publishing to the <code>/chatter</code> topic. Press <code>Ctrl+C</code> to stop this.</p> </li> <li> <p>We can see some additional options for this command by viewing the help documentation:</p> <pre><code>rostopic echo -h\n</code></pre> <p>From here, for instance, we can learn that if we just wanted the echo command to display a set number of messages from the <code>/chatter</code> topic we could use the <code>-n</code> option. To display the most recent two messages only, for example:</p> <pre><code>rostopic echo /chatter -n2\n</code></pre> </li> </ul>"},{"location":"com2009/la1/week1/#ex7","title":"Exercise 7: Creating a subscriber node","text":"<p>You will now create another node to subscribe to the topic that our publisher node is broadcasting messages to, to illustrate how information can be passed from one node to another, via topic messages.</p> <ol> <li>In TERMINAL 3 use the filesystem commands that were introduced earlier (<code>cd</code>, <code>ls</code> and <code>roscd</code>) to navigate to the <code>src</code> folder of your <code>week1_pubsub</code> package.</li> <li>Use the same procedure as before to create a new empty Python file called <code>subscriber.py</code> and remember to make it executable! </li> <li> <p>Then, open the newly created <code>subscriber.py</code> file in VS Code, paste in the code here and save it. Once again, it's important that you understand how this code works, so make sure you read the code annotations! </p> </li> <li> <p>Use <code>rosrun</code> to execute your newly created <code>subscriber.py</code> node (remember: <code>rosrun {package name} {script name}</code>). If your publisher and subscriber nodes are working correctly you should see an output like this:</p> <p> </p> </li> <li> <p>As before, we can find out what nodes are running on our system by using the <code>rosnode list</code> command. Open a new terminal window (TERMINAL 4), run this and see if you can identify the nodes that you have just launched.</p> </li> <li> <p>Finally, close down your publisher and subscriber nodes and the ROS Master by entering <code>Ctrl+C</code> in Terminals 1, 2 and 3.</p> </li> </ol>"},{"location":"com2009/la1/week1/#launch-files","title":"Launch Files","text":"<p>At the beginning of this session we launched our Gazebo Simulation and the <code>turtlebot3_teleop_keyboard</code> node using launch files and the <code>roslaunch</code> command. This provides a means to launch multiple ROS nodes simultaneously, and we will demonstrate this by building a launch file for the publisher and subscriber nodes that we created in the previous exercises.</p>"},{"location":"com2009/la1/week1/#ex8","title":"Exercise 8: Creating a launch file","text":"<ol> <li>In TERMINAL 1, use <code>roscd</code> to navigate to the root of your <code>week1_pubsub</code> package directory.</li> <li> <p>Use the Linux <code>mkdir</code> command to make a new directory in the package root folder called <code>launch</code>:</p> <p>TERMINAL 1: <pre><code>mkdir launch\n</code></pre></p> </li> <li> <p>Use the <code>cd</code> command to enter the <code>launch</code> folder that you just created, then use the <code>touch</code> command (as before) to create a new empty file called <code>pubsub.launch</code>.</p> </li> <li> <p>Open this launch file in VS Code and enter the following text:</p> <pre><code>&lt;launch&gt;\n&lt;node pkg={BLANK} type={BLANK} name=\"pub_node\" output=\"screen\"&gt;\n&lt;/node&gt;\n&lt;/launch&gt;\n</code></pre> <p>Fill in the Blanks!</p> <p>Referring to what we learned about the format of launch files earlier, replace each <code>{BLANK}</code> above with the correct text to launch the publisher node that you created in Exercise 6.</p> </li> <li> <p>Use <code>roslaunch</code> to launch this file and test it out as it is (remember: <code>roslaunch {package name} {launch file}</code>). If everything looks OK then carry on to the next step.</p> </li> <li>The code that we've given you above will launch the <code>publisher.py</code> node, but not the <code>subscriber.py</code> node.  Add another <code>&lt;node&gt;</code> tag to your <code>pubsub.launch</code> file to launch the subscriber node as well.</li> <li>The publisher and subscriber nodes and the ROS Master can now all be launched with the <code>roslaunch</code> command and the <code>pubsub.launch</code> file that you have now created.  </li> <li>Launch this in TERMINAL 1 and then use <code>rosnode list</code> in TERMINAL 2 to check that it all works correctly.</li> </ol>"},{"location":"com2009/la1/week1/#roslaunch","title":"Summary","text":"<ul> <li><code>roslaunch</code> can be used to launch multiple nodes on a robot from one single command.</li> <li>It will also automatically launch the ROS Master (equivalent to running the <code>roscore</code> command manually) if it isn't already running (did you notice that we didn't have to do this manually in Exercise 8, but we did when we launched our nodes individually, using <code>rosrun</code>, in Exercises 6 &amp; 7?)</li> <li>In the <code>rospy.init(...)</code> functions of our <code>publisher.py</code> and <code>subscriber.py</code> Python scripts, we defined a node name and set <code>anonymous=True</code>. As a result, when we launched our nodes manually using <code>rosrun</code>, the names we defined were honoured, but were appended with a unique combination of numbers.</li> <li>When we launched our nodes using <code>roslaunch</code> however, the node names were set according to what we had defined in the <code>name</code> field of the <code>&lt;node&gt;</code> tag within the launch file, and anything specified within the <code>rospy.init(...)</code> functions of our Python scripts were overwritten as a result.</li> </ul>"},{"location":"com2009/la1/week1/#wrapping-up","title":"Wrapping Up","text":"<p>In this session we've learnt about some key concepts in ROS, such as Packages; Launch files; Nodes and the Publisher-Subscriber Communication Method using Topics and Messages.</p> <p>We've learnt how to use some key ROS commands:  </p> <ul> <li><code>roslaunch</code>: to launch multiple ROS Nodes via launch files.</li> <li><code>roscd</code>: to navigate to installed ROS packages using a package name alone.</li> <li><code>rosnode</code>: to display information about active ROS Nodes.</li> <li><code>rosrun</code>: to run executables within a ROS package.</li> <li><code>rostopic</code>: to display information about active ROS topics.</li> <li><code>rosmsg</code>: to display information about all ROS messages that are available to use in a ROS application.</li> <li><code>roscore</code>: to launch the ROS Master: The baseline nodes and programs that are required for ROS to function.</li> </ul> <p>In addition to this we've also learnt how to use <code>catkin_create_pkg</code>, which is a helper script for creating ROS package templates.</p> <p>We have also learnt how to work in the Linux Terminal and navigate a Linux filesystem using key commands such as:</p> <ul> <li><code>pwd</code>: prints the path of the current working directory to show you which directory you're currently located in.</li> <li><code>ls</code>: lists the files in the current directory.</li> <li><code>cd</code>: change directory to move around the file system.</li> <li><code>mkdir</code>: make a new directory (<code>mkdir {new_folder}</code>).</li> <li><code>cat</code>: show the contents of a file.</li> <li><code>chmod</code>: modify file permissions (i.e. to add execute permissions to a file for all users: <code>chmod +x {file}</code>).</li> <li><code>touch</code>: create a file without any content.</li> </ul> <p>Finally, we have learnt how to create basic ROS nodes in Python to both publish and subscribe to ROS topics using standard ROS messages.</p>"},{"location":"com2009/la1/week1/#backup","title":"Saving your work","text":"<p>Remember, the work you have done in the WSL-ROS environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University U: Drive, allowing you to restore it at the start of the next session.  </p> <ol> <li> <p>\"The name catkin comes from the tail-shaped flower cluster found on willow trees -- a reference to Willow Garage where catkin was created.\" (According to ROS.org)\u00a0\u21a9</p> </li> <li> <p>What does <code>source ~/.bashrc</code> do? See here for an explanation.\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/la1/week2/","title":"Week 2: Odometry & Basic Navigation","text":"<p>Info</p> <p>You should be able to complete the exercises on this page within a two-hour lab session.</p>"},{"location":"com2009/la1/week2/#introduction","title":"Introduction","text":""},{"location":"com2009/la1/week2/#aims","title":"Aims","text":"<p>This week you will learn how to control a ROS robot's position and velocity from both the command line and through ROS Nodes. You will also learn how to interpret the data that allows us to monitor a robot's position in its physical environment.  The things you will learn here form the basis for all robot navigation in ROS, from simple open-loop methods to more advanced closed-loop control (which you will learn more about next week).</p>"},{"location":"com2009/la1/week2/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Interpret the Odometry data published by a ROS Robot and identify the parts of these messages that are relevant to a 2-wheeled differential drive robot (such as the TurtleBot3).</li> <li>Develop Python nodes to obtain Odometry messages from an active ROS network and translate them to provide useful information about a robot's pose in a convenient, human-readable way.</li> <li>Implement open-loop velocity control of a robot using ROS command-line tools.</li> <li>Develop Python nodes that use open-loop velocity control methods to make a robot follow a pre-defined motion path.</li> </ol>"},{"location":"com2009/la1/week2/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Exploring Odometry Data</li> <li>Exercise 2: Creating a Python node to process Odometry data</li> <li>Exercise 3: Moving a Robot with <code>rostopic</code> in the Terminal</li> <li>Exercise 4: Creating a Python node to make the robot move</li> </ul>"},{"location":"com2009/la1/week2/#getting-started","title":"Getting Started","text":""},{"location":"com2009/la1/week2/#step-1-launch-wsl-ros","title":"Step 1: Launch WSL-ROS","text":"<p>If you haven't done so already, launch your WSL-ROS environment by running the WSL-ROS shortcut in the Windows Start Menu. As you will now know, this may take a couple of minutes, but once it's ready this will open up the Windows Terminal and an Ubuntu terminal instance (which we'll refer to as TERMINAL 1).</p>"},{"location":"com2009/la1/week2/#step-2-restore-your-work","title":"Step 2: Restore your work","text":"<p>Remember that any work that you do within the WSL-ROS Environment will not be preserved between sessions or across different University computers.  At the end of the previous session you should have run the <code>wsl_ros</code> tool to back up your home directory to your University U: Drive. Once WSL-ROS is up and running, you should be prompted to restore this:</p> <p></p> <p>Enter <code>Y</code> to restore your work from last time. You can also restore your work at any time using the following command:</p> <pre><code>wsl_ros restore\n</code></pre>"},{"location":"com2009/la1/week2/#step-3-launch-vs-code","title":"Step 3: Launch VS Code","text":"<p>It's also worth launching VS Code now, so that it's ready to go for when you need it later on. Follow the steps here to launch it correctly.</p>"},{"location":"com2009/la1/week2/#course-repo","title":"Step 4: Download The Course Repo","text":"<p>We've put together a few ROS packages of our own that you'll use throughout this course. These all live inside the COM2009 Course Repo on GitHub, and you'll need to download this into the WSL-ROS environment now, before going any further.</p> <ol> <li> <p>In TERMINAL 1, navigate to the Catkin Workspace <code>src</code> directory using the <code>cd</code> command:</p> <p>TERMINAL 1: <pre><code>cd ~/catkin_ws/src/\n</code></pre></p> </li> <li> <p>Then, clone the Course Repo from GitHub:</p> <p>TERMINAL 1: <pre><code>git clone https://github.com/tom-howard/COM2009.git\n</code></pre></p> </li> <li> <p>Once this is done, we need to run <code>catkin build</code> to compile everything:</p> <p>TERMINAL 1: <pre><code>catkin build\n</code></pre></p> </li> <li> <p>And finally, we need to re-source our <code>.bashrc</code> file:</p> <p>TERMINAL 1: <pre><code>source ~/.bashrc\n</code></pre></p> <p>Remember</p> <p>If you have any other terminal instances open, then you'll need run <code>source ~/.bashrc</code> in these too, in order for the changes made by <code>catkin build</code> to propagate through to these as well!</p> </li> </ol> <p>That's it for now, we'll start using some of the packages that we've just installed a bit later on...</p>"},{"location":"com2009/la1/week2/#step-5-launch-the-robot-simulation","title":"Step 5: Launch the Robot Simulation","text":"<p>In TERMINAL 1 enter the following command to launch a simulation of a TurtleBot3 Waffle in an empty world:  </p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre></p> <p>A Gazebo simulation window should open and within this you should see a TurtleBot3 Waffle in empty space:</p> <p></p> <p>You're all set up and ready to go!</p>"},{"location":"com2009/la1/week2/#position-and-velocity","title":"Position and Velocity","text":"<p>Two types of Velocity Command can be issued to any ROS Robot to make it move (and thus change its position):</p> <ul> <li>Linear Velocity: The velocity at which the robot moves forwards or backwards in one of its principal axes.</li> <li>Angular Velocity: The velocity at which the robot rotates about one of its principal axes.</li> </ul>"},{"location":"com2009/la1/week2/#principal-axes","title":"Principal Axes","text":"<p>The motion (i.e. the velocity) of any mobile robot can be defined in terms of three principal axes: <code>X</code>, <code>Y</code> and <code>Z</code>. In the context of our TurtleBot3 Waffle, these axes (and the motion about them) are as follows:</p> <p></p> <p>In theory then, a robot can move linearly or angularly about any of these three axes, as shown by the arrows in the figure. That's six Degrees of Freedom (DOFs) in total, achieved based on a robot's design and the actuators it is equipped with. </p> <p>You should hopefully recall from the \"Introducing the Robots\" page that our TurtleBot3 Waffles only have two motors though, so they don't actually have six DOFs! These two motors can be controlled independently, which is known as a \"differential drive\" configuration, and ultimately provides it with a total of two degrees of freedom in total, as illustrated below.</p> <p></p> <p>It can therefore only move linearly in the x-axis (Forwards/Backwards) and angularly in the z-axis (Yaw).</p> <p>It's also worth noting (while we're on the subject of motion) that our TurtleBot3 Waffles have maximum velocity limits, which were also defined on the \"Robots\" page.</p> <p>Question</p> <p>What are the maximum velocity limits of our robots?</p>"},{"location":"com2009/la1/week2/#ros-velocity-commands","title":"ROS Velocity Commands","text":"<p>In the previous session you learnt how to list all the topics that are currently active on a ROS system. Open up a new terminal instance now (TERMINAL 2) and use what you learnt previously to list all the topics that are active on the ROS network now, as a result of launching the Gazebo simulation earlier.</p> <p>Questions</p> <ol> <li>Which topic in the list do you think could be used to control the velocity of the robot?</li> <li>Use the <code>rostopic info</code> command on the topic to find out more about it.</li> </ol> <p>The topic you identified1 should use a message of the <code>geometry_msgs/Twist</code> type. You'll have to send messages of this type to this topic in order to make the robot move. Use the <code>rosmsg</code> command (as you did in Exercise 4 last week) to find out more about the format of this message2.</p> <p>You should now be looking at a message format that looks like this: </p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x\n  float64 y\n  float64 z\ngeometry_msgs/Vector3 angular\n  float64 x\n  float64 y\n  float64 z\n</code></pre> <p>There are six parameters that we can assign values to here: <code>linear.x</code>, <code>linear.y</code>, <code>linear.z</code>; and <code>angular.x</code>, <code>angular.y</code>, <code>angular.z</code>. These relate to a robot's six degrees of freedom (about its three principal axes), as we discussed above. These topic messages are therefore formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x  &lt;-- Forwards (or Backwards)\n  float64 y  &lt;-- Left (or Right)\n  float64 z  &lt;-- Up (or Down)\ngeometry_msgs/Vector3 angular\n  float64 x  &lt;-- Roll\n  float64 y  &lt;-- Pitch\n  float64 z  &lt;-- Yaw\n</code></pre> <p>As we also learnt above though, our TurtleBots can only actually move with linear velocity in the x-axis and angular velocity in the z-axis. As a result then, only velocity commands issued to the <code>linear.x</code> (Forwards/Backwards) or <code>angular.z</code> (Yaw) parts of this message will have any effect.</p>"},{"location":"com2009/la1/week2/#robot-odometry","title":"Robot Odometry","text":"<p>Another topic that should have appeared when you ran <code>rostopic list</code> earlier is <code>/odom</code>. This topic contains Odometry data, which is also essential for robot navigation and is a basic feedback signal, allowing a robot to approximate its location.</p>"},{"location":"com2009/la1/week2/#ex1","title":"Exercise 1: Exploring Odometry Data","text":"<ol> <li> <p>In TERMINAL 2 use the <code>rostopic echo</code> command to display the odometry data currently being published by our simulated robot:</p> <p>TERMINAL 2: <pre><code>rostopic echo -c /odom\n</code></pre></p> <p>Expand the terminal window as necessary so that you can see the whole topic message (it starts with <code>header</code> and ends with <code>---</code>).</p> <p>Question</p> <p>What does the <code>-c</code> option in the command above actually do?</p> </li> <li> <p>Now, you need to launch a new Windows Terminal instance so that you can view it side-by-side with TERMINAL 2. To do this, press the \"New Tab\" button whilst pressing the <code>Shift</code> key. We'll call this one TERMINAL 3. Arrange both windows side-by-side, so you can see what's happening in both, simultaneously.</p> </li> <li> <p>In TERMINAL 3 launch the <code>turtlebot3_teleop_keyboard</code> node as you did last week: </p> <p>TERMINAL 3: <pre><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n</code></pre></p> </li> <li> <p>In TERMINAL 3 enter <code>A</code> a couple of times to make the robot rotate on the spot.  Observe how the odometry data changes in TERMINAL 2.  Is there anything in the <code>twist</code> part of the <code>/odom</code> message that corresponds to the <code>angular vel</code> that you are setting in TERMINAL 3? </p> </li> <li>Now press the <code>S</code> key to halt the robot, then press <code>W</code> a couple of times to make the robot drive forwards.  How does the <code>twist</code> part of the message now correspond to the <code>linear vel</code> setting in TERMINAL 3?</li> <li> <p>Now press <code>D</code> a couple of times and your robot should start to move in a circle.  What linear and angular velocities are you requesting in TERMINAL 3, and how are these represented in the <code>twist</code> part of the <code>/odom</code> message?  What about the <code>pose</code> part of the message?  How is this data changing as your robot moves in a circular path.</p> <p>Question</p> <p>What do you think <code>twist</code> and <code>pose</code> are actually telling us?</p> </li> <li> <p>Press <code>S</code> in TERMINAL 3 to stop the robot (but leave the <code>turtlebot3_teleop_keyboard</code> node running).  Then, press <code>Ctrl+C</code> in TERMINAL 2 to shut down the <code>rostopic echo</code> process. </p> </li> <li> <p>Let's look at the <code>pose</code> part of the <code>Odometry</code> message in more detail now. With the robot stationary, use <code>rosrun</code> to run a Python node that we have created to help illustrate how this relates to the robot's position and orientation in its environment: </p> <p>TERMINAL 2: <pre><code>rosrun tuos_ros_examples robot_pose.py\n</code></pre></p> </li> <li> <p>Now (using the <code>turtlebot3_teleop_key</code> node in TERMINAL 3), drive your robot around again, keeping an eye on the outputs that are being printed by the <code>robot_pose.py</code> node in TERMINAL 2 as you do so.</p> <p>The output of the <code>robot_pose.py</code> node shows you how the robot's position and orientation (i.e. \"pose\") are changing in real-time as you move the robot around. The <code>\"initial\"</code> column tells us the robot's pose when the node was first launched, and the <code>\"current\"</code> column show us what its pose currently is. The <code>\"delta\"</code> column then shows the difference between the two.</p> <p>Question</p> <p>Which pose parameters haven't changed, and is this what you would expect (considering the robot's principal axes, as illustrated above)?</p> </li> <li> <p>Press <code>Ctrl+C</code> in TERMINAL 2 and TERMINAL 3, to stop the <code>robot_pose.py</code> and <code>turtlebot3_teleop</code> nodes.  Then, close down TERMINAL 3 so that only one Windows Terminal application remains open with 2 active tabs: TERMINAL 1 and TERMINAL 2.</p> </li> </ol>"},{"location":"com2009/la1/week2/#odometry","title":"What is Odometry?","text":"<p>We can learn more about Odometry data by using the <code>rostopic info</code> command:</p> <p>TERMINAL 2: <pre><code>rostopic info /odom\n</code></pre></p> <p>This provides information about the type of message used by this topic:</p> <pre><code>Type: nav_msgs/Odometry\n</code></pre> <p>We can find out more about this using the <code>rosmsg info</code> command:</p> <p>TERMINAL 2: <pre><code>rosmsg info nav_msgs/Odometry\n</code></pre></p> <p>Which tells us that the <code>nav_msgs/Odometry</code> message contains four base elements:</p> <ol> <li>header</li> <li>child_frame_id</li> <li>pose</li> <li>twist</li> </ol>"},{"location":"com2009/la1/week2/#pose","title":"Pose","text":"<p>Pose tells us the position and orientation of the robot relative to an arbitrary reference point (typically where the robot was when it was turned on). The pose is determined from:</p> <ul> <li>Data from the Inertial Measurement Unit (IMU) on the OpenCR board,</li> <li>Data from both the left and right wheel encoders,</li> <li>An estimation of the distance travelled by the robot from its pre-defined reference point (using dead-reckoning).</li> </ul> <p>Position data is important for determining the movement of our robot, and from this we can estimate its location in 3-dimensional space.</p> <p>Orientation is expressed in units of Quaternions, and needs to be converted into Euler angles (in radians) about the principal axes. Fortunately, there are functions within the ROS <code>tf</code> library to do that for us, which we can use in any Python node as follows:</p> <pre><code>from tf.transformations import euler_from_quaternion\n(roll, pitch, yaw) = euler_from_quaternion([orientation.x, \norientation.y, orientation.z, orientation.w], \n'sxyz')\n</code></pre> <p>Our TurtleBot3 can only move in a 2D plane and so, actually, its pose can be fully represented by 3 parameters: <code>(x,y,\u03b8z)</code>, where <code>x</code> and <code>y</code> are the 2D coordinates of the robot in the <code>X-Y</code> plane, and <code>\u03b8z</code> is the angle of the robot about the <code>z</code> (yaw) axis.</p> <p>Question</p> <p>In the previous exercise, did you notice how the <code>linear_z</code>, <code>theta_x</code> and <code>theta_y</code> values in the <code>delta</code> column all remained at <code>0.000</code>, even when the robot was moving around?</p>"},{"location":"com2009/la1/week2/#twist","title":"Twist","text":"<p>Twist tells us the current linear and angular velocities of the robot, and this data comes directly from the wheel encoders.</p> <p>Once again, all of this data is defined in terms of the principal axes, as illustrated in the figure above.</p>"},{"location":"com2009/la1/week2/#ex2","title":"Exercise 2: Creating a Python node to process Odometry data","text":"<p>In the previous session you learnt how to create a package and build simple nodes in Python to publish and subscribe to messages on a topic. In this exercise you will build a new subscriber node, much like you did in the previous session, but this one will subscribe to the <code>/odom</code> topic that we've been talking about above. You'll also create a new package called <code>week2_navigation</code> for this node to live in!</p> <ol> <li> <p>Create a package in the same way as last week, this time called <code>week2_navigation</code>, which depends on the <code>rospy</code>, <code>nav_msgs</code> and <code>geometry_msgs</code> libraries. Use the <code>catkin_create_pkg</code> tool as you did last week. Remember to ensure that you are located in the <code>~/catkin_ws/src/</code> directory before you do this though:</p> <p>TERMINAL 2: <pre><code>cd ~/catkin_ws/src/\n</code></pre> Then: <pre><code>catkin_create_pkg {BLANK}\n</code></pre></p> <p>Fill in the Blank!</p> <p>Recall how we used the <code>catkin_create_pkg</code> tool last week, but adapt this now for the <code>week2_navigation</code> package, as detailed above.</p> </li> <li> <p>Run <code>catkin build</code> on this:</p> <p>TERMINAL 2: <pre><code>catkin build week2_navigation\n</code></pre> and then re-source your environment: <pre><code>source ~/.bashrc\n</code></pre></p> </li> <li> <p>The subscriber that we will build here will be structured in much the same way as the subscriber that we built last time. The difference now though is that this one will subscribe to the <code>/odom</code> topic (instead of <code>\"chatter\"</code>), and its callback function will therefore receive <code>Odometry</code> type messages (instead of <code>String</code>), so we'll have to deal with those a bit differently. We've created a template for this to help you to get started. Download this into the <code>src</code> directory of your new <code>week2_navigation</code> package now:</p> <p>TERMINAL 2:</p> <ol> <li>Step 1: navigate to the <code>src</code> directory of your <code>week2_navigation</code> package:     <pre><code>cd ~/catkin_ws/src/week2_navigation/src/\n</code></pre></li> <li>Then download the template code from GitHub:     <pre><code>wget -O odom_subscriber.py https://raw.githubusercontent.com/tom-howard/COM2009/main/tuos_ros_examples/src/odom_subscriber_template.py\n</code></pre></li> <li>Finally, make this executable using <code>chmod</code>:     <pre><code>chmod +x odom_subscriber.py\n</code></pre></li> </ol> </li> <li> <p>Run this as it is to see what happens to begin with:</p> <p>TERMINAL 2: <pre><code>rosrun week2_navigation odom_subscriber.py\n</code></pre></p> <p>... Hmmm, something's wrong here isn't it!? You may have seen the following error:</p> <pre><code>/usr/bin/env: \u2018python3\\r\u2019: Permission denied\n</code></pre> <p>The clue here is the <code>python3\\r</code> (specifically the <code>\\r</code> bit). This is a Windows line ending... </p> <p>Text files (including things like Python scripts) created on Windows use different line endings (i.e. the characters that signify the end of each line of text) to those created on Linux. Windows uses a \"carriage return\" and a \"line feed\" (<code>\\r\\n</code>) at the end of each line, but Linux uses just a \"line feed\" (<code>\\n</code>)3. Because we're working within a Linux environment here (Ubuntu), we must make sure we're using Linux line endings at all times! We can change this easily from inside VS Code... </p> <ol> <li>In the VS Code File Explorer navigate to the <code>~/catkin_ws/src/week2_navigation/src</code> folder and open the <code>odom_subscriber.py</code> file.</li> <li>In the blue bar along the bottom of the VS Code screen (towards the bottom right-hand corner) you should see the text <code>CRLF</code>. Click on this and a menu should then appear at the top of the screen with the text <code>\"Select End of Line Sequence\"</code>.</li> <li>Select the <code>LF</code> option in this menu, then save the file.</li> </ol> <p> </p> </li> <li> <p>OK, the file should run now, so launch it (using <code>rosrun</code> again) and see what it does.</p> </li> <li> <p>Have a think about what's different between this and the subscriber from last time...</p> <p>In the Week 1 Subscriber we were working with a <code>String</code> type message from the <code>std_msgs</code> package, whereas this time we're using an <code>Odometry</code> message from the <code>nav_msgs</code> package instead - notice how the imports and the callback function have changed as a result of this.</p> </li> <li> <p>You need to add some additional code to the callback function now: </p> <ol> <li>The node needs to print the robot's real-time odometry data to the terminal in the form: <code>(x,y,\u03b8z)</code>.</li> <li>The format of the message has already been structured for you, but you need to add in the relevant variables that represent the correct elements of the robot's real-time pose.</li> <li>You'll need to use the <code>euler_from_quaternion</code> function from the <code>tf.transformations</code> library to convert the raw orientation values from Quaternions into Radians. If you need a hint, why not have a look back at this bit from earlier, or at the source code for the <code>robot_pose.py</code> node that we launched from the <code>tuos_ros_examples</code> package in the previous exercise. </li> </ol> </li> <li> <p>Launch your node using <code>rosrun</code> and observe how the output (the formatted odometry data) changes whilst you move the robot around again using the <code>turtlebot3_teleop</code> node in a new terminal instance (TERMINAL 3).</p> </li> <li>Stop your <code>odom_subscriber.py</code> node in TERMINAL 2 and the <code>turtlebot3_teleop</code> node in TERMINAL 3 by entering <code>Ctrl+C</code> in each of the terminals.</li> </ol>"},{"location":"com2009/la1/week2/#basic-navigation-open-loop-velocity-control","title":"Basic Navigation: Open-loop Velocity Control","text":""},{"location":"com2009/la1/week2/#ex3","title":"Exercise 3: Moving a Robot with <code>rostopic</code> in the Terminal","text":"<p>Warning</p> <p>Make sure that you've stopped the <code>turtlebot3_teleop</code> node running in TERMINAL 3 (by entering <code>Ctrl+C</code>) before starting this exercise.</p> <p>We can use the <code>rostopic pub</code> command to publish data to a topic from a terminal by using the command in the following way:</p> <pre><code>rostopic pub {topic_name} {message_type} {data}\n</code></pre> <p>As we discovered earlier, the <code>/cmd_vel</code> topic is expecting linear and angular data, each with an <code>x</code>, <code>y</code> and <code>z</code> component. We can get further help with formatting this message by using the autocomplete functionality within the terminal. Type the following into TERMINAL 3 hitting the <code>Space</code> and <code>Tab</code> keys on your keyboard where indicated:</p> <p>TERMINAL 3: <pre><code>rostopic pub /cmd_vel geometry_msgs/Twist[SPACE][TAB]\n</code></pre></p> <p>The full message should then be presented to us:</p> <pre><code>rostopic pub /cmd_vel geometry_msgs/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre> <ol> <li>Scroll back through the message using the \u2190 key on your keyboard and then edit the values of the various parameters, as appropriate. First, define some values that would make the robot rotate on the spot.  Make a note of the command that you used.</li> <li>Enter <code>Ctrl+C</code> in TERMINAL 3 to stop the message from being published.</li> <li>Next, enter a command in TERMINAL 3 to make the robot move in a circle.  Again, make a note of the command that you used.</li> <li>Enter <code>Ctrl+C</code> in TERMINAL 3 to again stop the message from being published.</li> <li>Finally, enter a command to stop the TurtleBot3 and make a note of this too.</li> <li>Enter <code>Ctrl+C</code> in TERMINAL 3 to stop this final message from being published.</li> </ol>"},{"location":"com2009/la1/week2/#ex4","title":"Exercise 4: Creating a Python node to make the robot move","text":"<p>You will now create another node to control the motion of your TurtleBot3 by publishing messages to the <code>/cmd_vel</code> topic. You created a publisher node in Week 1, and you can use this as a starting point.</p> <ol> <li> <p>In TERMINAL 2, ensure that you are still located within the <code>src</code> folder of your <code>week2_navigation</code> package. You could use <code>pwd</code> to check your current working directory, where the output should look like this:</p> <pre><code>/home/student/catkin_ws/src/week2_navigation/src\n</code></pre> <p>If you aren't located here then navigate to this directory using <code>cd</code>.</p> </li> <li> <p>Create a new file called <code>move_circle.py</code>:</p> <p>TERMINAL 2: <pre><code>touch move_circle.py\n</code></pre> ... and make this file executable using the <code>chmod</code> command.</p> </li> <li> <p>Open up this file in VS Code, then copy and paste the contents of the publisher node from last week into the new <code>move_circle.py</code> file to get you started. Then edit the code to achieve the following:</p> <ul> <li>Make your TurtleBot3 move in a circle with a path radius of approximately 0.5m.</li> <li>Your Python node needs to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic in order to make the TurtleBot3 move. See below for further detail on this.</li> <li>Remember that our robots have a maximum linear velocity (<code>linear.x</code>) of 0.26 m/s, and a maximum angular velocity (<code>angular.z</code>) of 1.82 rad/s. </li> <li>Make sure that you code your <code>shutdownhook()</code> correctly so that the robot stops moving when the node is shutdown (via <code>Ctrl+C</code> in the terminal that launched it).</li> </ul> </li> </ol> <p>Advanced feature:</p> <ol> <li>Create a launch file to launch this and your <code>odom_subscriber.py</code> node simultaneously with a single <code>roslaunch</code> command. Refer to the launch file that you created last week for a reminder on how to do this.</li> </ol>"},{"location":"com2009/la1/week2/#twist-py","title":"Publishing <code>Twist</code> messages in Python","text":"<p>From last week's publisher exercise, we know how to publish a <code>String</code> type message to a topic in Python, but how do we apply the same principles to a <code>Twist</code> message (on the <code>/cmd_vel</code> topic)? Let's have a look at this... </p> <p>First, you need to import the <code>rospy</code> library, as well as the <code>Twist</code> message type from the <code>geometry_msgs</code> library:</p> <pre><code>import rospy\nfrom geometry_msgs.msg import Twist\n</code></pre> <p>Then, create an instance of a <code>rospy.Publisher()</code> and assign it to an object called <code>pub</code>. When we create the object we tell the <code>Publisher()</code> method which topic we want to publish this message to (via the first input argument), and also that we will be publishing a message of the <code>Twist</code> type (the second input argument):</p> <pre><code>pub = rospy.Publisher({topic name}, Twist, queue_size=10) # a queue size of 10 usually works!\n</code></pre> <p>Then we need to create a <code>Twist()</code> message instance and assign it to an object (which we'll call <code>vel_cmd</code>):</p> <pre><code>vel_cmd = Twist()\n</code></pre> <p>We know from earlier that the <code>geometry_msgs/Twist</code> message has the format:</p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x\n  float64 y\n  float64 z\ngeometry_msgs/Vector3 angular\n  float64 x\n  float64 y\n  float64 z\n</code></pre> <p>We also know, that only velocity commands issued to the following two parameters will actually have any effect on the velocity of our robot:</p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x\n</code></pre> <p>...and:</p> <pre><code>geometry_msgs/Vector3 angular\n  float64 z\n</code></pre> <p>As such, we set appropriate velocity values to these attributes of the <code>Twist()</code> message (assigned to <code>vel_cmd</code>):</p> <pre><code>vel_cmd.linear.x = 0.0 # m/s\nvel_cmd.angular.z = 0.0 # rad/s\n</code></pre> <p>We can then publish this to the relevant topic on the ROS network by supplying it to the <code>rospy.Publisher().publish()</code> method (which we instantiated as <code>pub</code> earlier):</p> <pre><code>pub.publish(vel_cmd)\n</code></pre> <p>Use these pointers when working on your <code>move_circle.py</code> node!</p>"},{"location":"com2009/la1/week2/#wrapping-up","title":"Wrapping Up","text":"<p>In this session you have learnt how to control the velocity and position of a robot from both the command-line (using ROS command-line tools) and from ROS Nodes by publishing correctly formatted messages to the <code>/cmd_vel</code> topic.  </p> <p>You have also learnt about Odometry, which is published by our robot to the <code>/odom</code> topic.  The odometry data tells us the current linear and angular velocities of our robot in relation to its 3 principal axes.  In addition to this though, it also tells us where in physical space our robot is located and oriented, which is determined based on dead-reckoning.  We'll talk more about dead-reckoning later on in the COM2009 lecture course, but for now though consider the following (based on what we've covered in this lab session): </p> <ul> <li>If odometry is derived from dead-reckoning, what information (sensor/actuator data) is used to do this?</li> <li>Do you see any potential limitations of this?</li> <li>Can a control method that uses odometry as a feedback signal be considered closed-loop control? </li> </ul> <p>We'll explore this a little more next week, but you might want to consider reading Chapter 11.1.3 (\"Pose of Robot\") in the ROS Robot Programming eBook that we mentioned here.</p>"},{"location":"com2009/la1/week2/#backup","title":"Saving your work","text":"<p>Remember, the work you have done in the WSL-ROS environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University U: Drive, allowing you to restore it at the start of the next session.  </p> <ol> <li> <p>The topic is <code>/cmd_vel</code> (i.e. command velocity).\u00a0\u21a9</p> </li> <li> <p>Answer: <code>rosmsg info geometry_msgs/Twist</code>.\u00a0\u21a9</p> </li> <li> <p>Adapted from: https://www.cs.toronto.edu/~krueger/csc209h/tut/line-endings.html\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/la1/week3/","title":"Week 3: Advanced Navigation & SLAM","text":"<p>Info</p> <p>You should be able to complete Exercises 2, 3, &amp; 4 on this page within a two-hour lab session, but you may want to spend a bit more time on Exercise 1.</p>"},{"location":"com2009/la1/week3/#introduction","title":"Introduction","text":""},{"location":"com2009/la1/week3/#aims","title":"Aims","text":"<p>This week you will implement closed-loop velocity control and create a ROS node that can control a robot's motion path by using odometry data as a feedback signal. In doing this however, you will start to appreciate the limitations of odometry as a feedback signal, which will lead us on to exploring some other data-streams that could be used to aid navigation further. Finally, you will leverage some existing ROS libraries and TurtleBot3 packages to explore some of the autonomous navigation methods that are available within ROS.</p>"},{"location":"com2009/la1/week3/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Combine both publisher &amp; subscriber communication methods (that you have so far dealt with in isolation) into a single Python node to implement closed-loop (odometry-based) velocity control of a robot.</li> <li>Explain the limitations of Odometry-based motion control methods. </li> <li>Interpret the data that is published to the <code>/scan</code> topic and use existing ROS tools to visualise this.</li> <li>Use existing ROS tools to implement SLAM and build a map of an environment. </li> <li>Leverage existing ROS libraries to make a robot navigate an environment autonomously, using the map that you have generated.</li> <li>Explain how these SLAM and Navigation tools are implemented and what information is required in order to make them work.</li> </ol>"},{"location":"com2009/la1/week3/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Make your robot follow a Square motion path</li> <li>Exercise 2: Using RViz to Visualise Robot Data</li> <li>Exercise 3: Building a map of an environment with SLAM</li> <li>Exercise 4: Navigating an Environment Autonomously</li> </ul>"},{"location":"com2009/la1/week3/#additional-resources","title":"Additional Resources","text":"<ul> <li>The <code>move_square</code> Template (for Exercise 1)</li> </ul>"},{"location":"com2009/la1/week3/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch WSL-ROS Launch your WSL-ROS environment by running the WSL-ROS shortcut in the Windows Start Menu (if you haven't already done so). Once installed, the Windows Terminal app should launch with an Ubuntu terminal instance ready to go (TERMINAL 1).</p> <p>Step 2: Restore your work Remember that any work that you do in WSL-ROS will not be preserved between sessions or across different University computers. You should have used the <code>wsl_ros</code> tool at the end of the previous session to back up your home directory to your University U: Drive. If so, then you should now be prompted to restore it:</p> <p></p> <p>Enter <code>Y</code> to restore your work now.</p> Tip <p>You can also use the <code>wsl_ros restore</code> command to restore your work at any other time.</p> <p>Step 3: Launch VS Code Also launch VS Code now by following the steps here to launch it correctly within the WSL-ROS environment.</p> <p>Step 4: Launch the Robot Simulation  You should know exactly how to do this now but, just to re-iterate, enter the following into TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre></p> <p>...which will launch a Gazebo simulation of a TurtleBot3 Waffle in an empty world:</p> <p></p> Tip <p>Getting bored with entering that long command to launch the simulation? You could use a command alias instead:</p> <pre><code>tb3_empty_world\n</code></pre>"},{"location":"com2009/la1/week3/#odometry-based-navigation","title":"Odometry-based Navigation","text":"<p>In the previous session you created a Python node to make your robot move using open-loop control. To achieve this you published velocity commands to the <code>/cmd_vel</code> topic to make the robot follow a circular motion path.</p> <p>Questions</p> <ul> <li>How do you know if your robot actually achieved the motion path that you were hoping for?</li> <li>In a real-world environment, what external factors might result in your robot not achieving its desired trajectory?</li> </ul> <p>Last week you also learnt about Robot Odometry, which is used by the robot to keep track of its position and orientation (aka Pose) in the environment.  This is determined by a process called \"dead-reckoning,\" which is only really an approximation, but it's a fairly good one in any case, and we can use this as a feedback signal to understand if our robot is moving in the way that we expect it to.  We can therefore build on the techniques that we used in the <code>move_circle.py</code> node from last time, and now also build in the ability to subscribe to a topic too. In this case, we'll be subscribing to the <code>/odom</code> topic that we worked with a bit (in isolation) last time, and use this to provide us with a feedback signal to allow us to implement some basic closed-loop control.</p>"},{"location":"com2009/la1/week3/#ex1","title":"Exercise 1: Make your robot follow a Square motion path","text":"<ol> <li> <p>Launch a new terminal instance (TERMINAL 2) and, from there, navigate to the <code>week2_navigation</code> package that you created last time1.</p> </li> <li> <p>Navigate to the package <code>src</code> directory and use the Linux <code>touch</code> command to create a new file called <code>move_square.py</code>:</p> <p>TERMINAL 2: <pre><code>touch move_square.py\n</code></pre></p> </li> <li> <p>Then make this file executable using <code>chmod</code>:</p> <p>TERMINAL 2: <pre><code>chmod +x move_square.py\n</code></pre></p> </li> <li> <p>Use the VS Code File Explorer to navigate to this <code>move_square.py</code> file and open it up, ready for editing.</p> </li> <li>There's a template here to help you with this exercise. Copy and paste the template code into your new <code>move_square.py</code> file to get you started. </li> <li> <p>Run the code as it is to see what happens...</p> <p>Fill in the Blank!</p> <p>Something not quite working as expected? We may have missed out something very crucial on the very first line of the code template, can you work out what it is?!</p> </li> <li> <p>Fill in the blank as required and then adapt the code to make your robot follow a square motion path of 1m x 1m dimensions:</p> <ul> <li>The robot's odometry will tell you how much the robot has moved and/or rotated, and so you should use this information to achieve the desired motion path. </li> <li>Your Python node will therefore need to subscribe to the <code>/odom</code> topic as well as publish to <code>/cmd_vel</code>.</li> </ul> </li> </ol> <p>Advanced features:</p> <ol> <li>Adapt the node further to make the robot automatically stop once it has performed two complete loops.</li> <li>Create a launch file to launch this and the <code>odom_subscriber.py</code> node from last time simultaneously!</li> </ol> <p>After following a square motion path a few times, your robot should return to the same location that it started from.</p>"},{"location":"com2009/la1/week3/#lidar","title":"Laser Displacement Data and The LiDAR Sensor","text":"<p>Odometry is really important for robot navigation, but it can be subject to drift and accumulated error over time. You may have observed this in the previous exercise, but you would most certainly notice it if you were to do the same on a real robot. Fortunately, we have another sensor on-board our robot which provides even richer information about the environment, and we can use this to supplement the odometry information and enhance the robot's navigation capabilities.</p>"},{"location":"com2009/la1/week3/#ex2","title":"Exercise 2: Using RViz to Visualise Robot Data","text":"<p>We're going to place the robot in a more interesting environment now, so you'll need to make sure that you close down the Gazebo simulation that is currently running.  The best way to do this is to go to TERMINAL 1 and enter <code>Ctrl+C</code>.  It may take a bit of time, but the Gazebo window will close down after 30 seconds or so. You should also stop your <code>move_square.py</code> node, if that's still running too. </p> <ol> <li> <p>Return to TERMINAL 1 and enter the following to launch a new simulation:</p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_world.launch\n</code></pre></p> <p>A new Gazebo simulation should now be launched with a TurtleBot3 Waffle in a new environment:</p> <p> </p> </li> <li> <p>In TERMINAL 2, enter the following:</p> <p>TERMINAL 2: <pre><code>roslaunch tuos_ros_simulations rviz.launch\n</code></pre></p> <p>On running the command a new window should open:</p> <p> </p> <p>This is RViz, which is a ROS tool that allows us to visualise the data being measured by a robot in real-time. The red dots scattered around the robot represent laser displacement data which is measured by the LiDAR sensor located on the top of the robot.  This data allows the robot to measure the distance to any obstacles in its immediate surroundings. The LiDAR sensor spins continuously, sending out laser pulses as it does so. These laser pulses then bounce off any objects and are reflected back to the sensor. Distance can then be determined based on the time it takes for the pulses to complete the full journey (from the sensor, to the object, and back again), by a process called \"time of flight\". Because the LiDAR sensor spins and performs this process continuously, a full 360\u00b0 scan of the environment can be generated.  In this case (because we are working in simulation here) the data represents the objects surrounding the robot in its simulated environment, so you should notice that the red dots produce an outline that resembles the objects in the world that is being simulated in Gazebo (or partially at least).</p> </li> <li> <p>Next, open up a new terminal instance (TERMINAL 3). Laser displacement data from the LiDAR sensor is published by the robot to the <code>/scan</code> topic. We can use the <code>rostopic info</code> command to find out more about the nodes that are publishing and subscribing to this topic, as well as the message type:</p> <p>TERMINAL 3: <pre><code>rostopic info /scan\n</code></pre> <pre><code>Type: sensor_msgs/LaserScan\n\nPublishers:\n    * /gazebo (http://localhost:#####/)\n\nSubscribers:\n    * /rviz_#### (http://localhost:#####/) \n</code></pre></p> </li> <li> <p>As we can see from above, <code>/scan</code> messages are of the <code>sensor_msgs/LaserScan</code> type, and we can find out more about this message type using the <code>rosmsg info</code> command:</p> <p>TERMINAL 3: <pre><code>rosmsg info sensor_msgs/LaserScan\n</code></pre> <pre><code>std_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\nfloat32 angle_min\nfloat32 angle_max\nfloat32 angle_increment\nfloat32 time_increment\nfloat32 scan_time\nfloat32 range_min\nfloat32 range_max\nfloat32[] ranges\nfloat32[] intensities \n</code></pre></p> </li> </ol>"},{"location":"com2009/la1/week3/#interpreting-laserscan-data","title":"Interpreting <code>/LaserScan</code> Data","text":"<p>The <code>LaserScan</code> message is a standardised ROS message (from the <code>sensor_msgs</code> package) that any ROS Robot can use to publish data that it obtains from a Laser Displacement Sensor such as the LiDAR on the TurtleBot3.  You can find the full definition of the message here. Have a look at this to find out more.</p> <p><code>ranges</code> is an array of <code>float32</code> values (we know it's an array of values because of the <code>[]</code> after the data-type). This is the part of the message containing all the actual distance measurements that are being obtained by the LiDAR sensor (in meters).</p> <p>Consider a simplified example here, taken from a TurtleBot3 robot in a much smaller, fully enclosed environment.  In this case, the displacement data from the <code>ranges</code> array is represented by green squares:</p> <p></p> <p>As illustrated in the figure, we can associate each data-point within the <code>ranges</code> array to an angular position by using the <code>angle_min</code>, <code>angle_max</code> and <code>angle_increment</code> values that are also provided within the <code>LaserScan</code> message.  We can use the <code>rostopic echo</code> command to drill down into these elements of the message specifically and find out what their values are:</p> <p><pre><code>$ rostopic echo /scan/angle_min -n1\n0.0\n</code></pre> <pre><code>$ rostopic echo /scan/angle_max -n1\n6.28318977356\n</code></pre> <pre><code>$ rostopic echo /scan/angle_increment -n1\n0.0175019223243\n</code></pre></p> <p>Notice how we were able to access specific variables within the <code>/scan</code> message using <code>rostopic echo</code> here, rather than simply printing the whole thing?</p> <p>Questions</p> <ul> <li>What does the <code>-n1</code> option do, and why is it appropriate to use this here?</li> <li>What do these values represent? (Compare them with the figure above)</li> </ul> <p>The <code>ranges</code> array contains 360 values in total, i.e. a distance measurement at every 1\u00b0 (an <code>angle_increment</code> of 0.0175 radians) around the robot. The first value in the <code>ranges</code> array (<code>ranges[0]</code>) is the distance to the nearest object directly in front of the robot (i.e. at \u03b8 = 0 radians, or <code>angle_min</code>). The last value in the <code>ranges</code> array (<code>ranges[359]</code>) is the distance to the nearest object at 359\u00b0 (i.e. \u03b8 = 6.283 radians, or <code>angle_max</code>) from the front of the robot. If, for example, we were to obtain the 65th value in the <code>ranges</code> array, that is: <code>ranges[65]</code>, we know that this would represent the distance to the nearest object at an angle of 65\u00b0 (1.138 radians) from the front of the robot (anti-clockwise), as shown in the figure.</p> <p>The <code>LaserScan</code> message also contains the parameters <code>range_min</code> and <code>range_max</code>, which represent the minimum and maximum distance (in meters) that the LiDAR sensor can detect, respectively. You can use the <code>rostopic echo</code> command to report these directly too.  </p> <p>Question</p> <p>What is the maximum and minimum range of the LiDAR sensor? Use the same technique as we used above to find out.</p> <p>Finally, use the <code>rostopic echo</code> command again to display the <code>ranges</code> portion of the <code>LaserScan</code> topic message.  Don't use the <code>-n1</code> option now, so that you can see the data changing, in the terminal, in real-time, but use the <code>-c</code> option to clear the screen after every message to make things a bit clearer.  You might also need to maximise the terminal window so that you can see the full content of the array (all 360 values!) The array is quite big, but is bound by square brackets <code>[]</code> to denote the start and end, and there should be a <code>---</code> at the end of each message too, to help you confirm that you are viewing the entire thing.</p> <p>The main thing you'll notice here is that there's way too much information, updating far too quickly for it to be of any real use! As you have already seen though, it is the numbers that are flying by here that are represented by red dots in RViz.  Head back to the RViz screen to have another look at this now. As you'll no doubt agree, this is a much more useful way to visualise the <code>ranges</code> data, and illustrates how useful RViz can be for interpreting what your robot can see in real-time.</p> <p>What you may also notice is several <code>inf</code> values scattered around the array.  This represents sensor readings that were greater than the distance specified by <code>range_max</code>, so the sensor couldn't report a distance measurement in these cases. </p> <p>Stop the <code>rostopic echo</code> command from running in the terminal window by entering <code>Ctrl+C</code>.</p>"},{"location":"com2009/la1/week3/#slam","title":"Simultaneous Localisation and Mapping (SLAM)","text":"<p>In combination, the data from the LiDAR sensor and the robot's odometry (the robot pose specifically) are really powerful, and allow some very useful conclusions to be made about the its environment.  One of the key applications of this data is \"Simultaneous Localisation and Mapping\", or SLAM.  This is a tool that is built into ROS, allowing a robot to build up a map of its environment and locate itself within that map at the same time!  You will now learn how easy it is to leverage this in ROS.</p>"},{"location":"com2009/la1/week3/#ex3","title":"Exercise 3: Building a map of an environment with SLAM","text":"<ol> <li> <p>Close down all ROS processes that are running now by entering <code>Ctrl+C</code> in each terminal:</p> <ol> <li>The Gazebo processes in TERMINAL 1.</li> <li>The RViz processes running in TERMINAL 2.</li> </ol> </li> <li> <p>We're going to launch our robot into another new simulated environment now, which we'll be creating a map of using SLAM! To launch the simulation enter the following command in TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_ros_simulations nav_world.launch\n</code></pre></p> <p>The environment that launches should look like this:</p> <p> </p> </li> <li> <p>Now we will launch SLAM to start building a map of this environment. In TERMINAL 2, launch SLAM as follows:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch\n</code></pre></p> <p>This will launch RViz again, and you should be able to see a model of your TurtleBot3 from a top-down view, this time with green dots representing the real-time LiDAR data. The SLAM tools will already have begun processing this data to start building a map of the boundaries that are currently visible to your robot based on its position in the environment.</p> </li> <li> <p>In TERMINAL 3 launch the <code>turtlebot3_teleop</code> node (you should know how to do this by now).  Re-arrange and re-size your windows so that you can see Gazebo, RViz and the <code>turtlebot3_teleop</code> terminal instance all at the same time:</p> <p> </p> </li> <li> <p>Drive the robot around the arena slowly, using the <code>turtlebot3_teleop</code> node, and observe the map being updated in the RViz window as you do so. Drive the robot around until a full map of the environment has been generated.</p> <p> </p> </li> <li> <p>As you're doing this you need to also determine the centre coordinates of the four circles (A, B, C &amp; D) that are printed on the arena floor. Drive your robot into each of these circular zones and stop the robot inside them. As you should remember from last time, we can determine the position (and orientation) of a robot in its environment from its odometery, as published to the <code>/odom</code> topic. In Exercise 2 last time you built an odometry subscriber node, so you could launch this now, in TERMINAL 4, and use this to inform you of your robot's <code>x</code> and <code>y</code> position in the environment when located within each of the zone markers:</p> <p>TERMINAL 4: <pre><code>rosrun week2_navigation odom_subscriber.py\n</code></pre></p> <p>Record the zone marker coordinates in a table such as the one below (you'll need this information for the next exercise).</p> <p> Zone X Position (m) Y Position (m) START 0.5 -0.04 A B C D <p></p> <li> <p>Once you have obtained all this data, and you're happy that your robot has built a complete map of the environment, you then need to save this map for later use. We do this using a ROS <code>map_server</code> package.  First, stop the robot by pressing <code>S</code> in TERMINAL 3 and then enter <code>Ctrl+C</code> to shut down the <code>turtlebot3_teleop</code> node.</p> </li> <li> <p>Then, remaining in TERMINAL 3, navigate to the root of your <code>week2_navigation</code> package directory and create a new folder in it called <code>maps</code>:</p> <p>TERMINAL 3: <pre><code>roscd week2_navigation\n</code></pre> <pre><code>mkdir maps\n</code></pre></p> </li> <li> <p>Navigate into this new directory:</p> <p>TERMINAL 3: <pre><code>cd maps/\n</code></pre></p> </li> <li> <p>Then, run the <code>map_saver</code> node from the <code>map_server</code> package to save a copy of your map:</p> <p>TERMINAL 3: <pre><code>rosrun map_server map_saver -f {map name}\n</code></pre> Replacing <code>{map name}</code> with a name of your choosing. </p> <p>This will create two files: a <code>{map name}.pgm</code> and a <code>{map name}.yaml</code> file, both of which contain data related to the map that you have just created.  The <code>.pgm</code> file contains an Occupancy Grid Map (OGM), which is used for autonomous navigation in ROS.  Have a look at the map by launching it in an Image Viewer Application called <code>eog</code>:</p> <p>TERMINAL 3: <pre><code>eog {map name}.pgm\n</code></pre></p> <p>A new window should launch containing the map you have just created with SLAM and the <code>map_saver</code> node: </p> <p> </p> <p>White regions represent the area that your robot has determined is open space and that it can freely move within.  Black regions, on the other hand, represent boundaries or objects that have been detected.  Any grey area on the map represents regions that remain unexplored, or that were inaccessible to the robot.</p> </li> <li> <p>Compare the map generated by SLAM to the real simulated environment. In a simulated environment this process should be pretty accurate, and the map should represent the simulated environment very well (unless you didn't allow your robot to travel around and see the whole thing!)  In a real environment this is often not the case.  </p> <p>Questions</p> <ul> <li>How accurately did your robot map the environment?</li> <li>What might impact this when working in a real-world environment?</li> </ul> </li> <li> <p>Close the image using the <code>x</code> button on the right-hand-side of the eog window.</p> </li> <p>Summary of SLAM:</p> <p>See how easy it was to map an environment in the previous exercise? This works just as well on a real robot in a real environment too (see this video demonstration that we put together a while back). </p> <p>This illustrates the power of ROS: having access to tools such as SLAM, which are built into the ROS framework, makes it really quick and easy for a robotics engineer to start developing robotic applications on top of this. Our job was made even easier here since we used some packages that had been pre-made by the manufacturers of our TurtleBot3 Robots to help us launch SLAM with the right configurations for our exact robot.  If you were developing a robot yourself, or working with a different type of robot, then you might need to do a bit more work in setting up and tuning the SLAM tools to make it work for your own application.</p>"},{"location":"com2009/la1/week3/#advanced-navigation-methods","title":"Advanced Navigation Methods","text":"<p>As mentioned above, the map that you created in the previous exercise can now be used by ROS to autonomously navigate the mapped area.  We'll explore this now.</p>"},{"location":"com2009/la1/week3/#ex4","title":"Exercise 4: Navigating an Environment Autonomously","text":"<ol> <li>Close down all ROS processes again now so that nothing is running (but leave all three terminal windows open).</li> <li>In order to perform autonomous navigation we now need to activate a number of ROS libraries, our simulated environment and also specify some custom parameters, such as the location of our map file. The easiest way to do all of this in one go is to create a launch file. </li> <li> <p>You may have already created a launch directory in your <code>week2_navigation</code> package, but if you haven't then do this now:</p> <p>TERMINAL 1: <pre><code>roscd week2_navigation\n</code></pre> <pre><code>mkdir launch\n</code></pre></p> </li> <li> <p>Next, navigate into this directory and create a new file called <code>navigation.launch</code>:</p> <p>TERMINAL 1: <pre><code>cd launch/\n</code></pre> <pre><code>touch navigation.launch\n</code></pre></p> </li> <li> <p>Open up this file in VS Code and copy and paste the following content: </p> <pre><code>&lt;!-- Adapted from the Robotis \"turtlebot3_navigation\" package: \nhttps://github.com/ROBOTIS-GIT/turtlebot3/blob/master/turtlebot3_navigation/launch/turtlebot3_navigation.launch\n--&gt;\n&lt;launch&gt;\n&lt;include file=\"$(find tuos_ros_simulations)/launch/nav_world.launch\" /&gt;\n&lt;!-- To be modified --&gt;\n&lt;arg name=\"map_file\" default=\" $(find week2_navigation)/maps/{map name}.yaml\"/&gt;\n&lt;arg name=\"initial_pose_x\" default=\"0.0\"/&gt;\n&lt;arg name=\"initial_pose_y\" default=\"0.0\"/&gt;\n&lt;!-- Other arguments --&gt;\n&lt;arg name=\"initial_pose_a\" default=\"0.0\"/&gt;\n&lt;arg name=\"model\" default=\"$(env TURTLEBOT3_MODEL)\" doc=\"model type [burger, waffle, waffle_pi]\"/&gt;\n&lt;arg name=\"move_forward_only\" default=\"false\"/&gt;\n&lt;!-- Turtlebot3 Bringup --&gt;\n&lt;include file=\"$(find turtlebot3_bringup)/launch/turtlebot3_remote.launch\"&gt;\n&lt;arg name=\"model\" value=\"$(arg model)\" /&gt;\n&lt;/include&gt;\n&lt;!-- Map server --&gt;\n&lt;node pkg=\"map_server\" name=\"map_server\" type=\"map_server\" args=\"$(arg map_file)\"/&gt;\n&lt;!-- AMCL --&gt;\n&lt;include file=\"$(find turtlebot3_navigation)/launch/amcl.launch\"&gt;\n&lt;arg name=\"initial_pose_x\" value=\"$(arg initial_pose_x)\"/&gt;\n&lt;arg name=\"initial_pose_y\" value=\"$(arg initial_pose_y)\"/&gt;\n&lt;arg name=\"initial_pose_a\" value=\"$(arg initial_pose_a)\"/&gt;\n&lt;/include&gt;\n&lt;!-- move_base --&gt;\n&lt;include file=\"$(find turtlebot3_navigation)/launch/move_base.launch\"&gt;\n&lt;arg name=\"model\" value=\"$(arg model)\" /&gt;\n&lt;arg name=\"move_forward_only\" value=\"$(arg move_forward_only)\"/&gt;\n&lt;/include&gt;\n&lt;!-- rviz --&gt;\n&lt;node pkg=\"rviz\" type=\"rviz\" name=\"rviz\" required=\"true\"\nargs=\"-d $(find turtlebot3_navigation)/rviz/turtlebot3_navigation.rviz\"/&gt;\n&lt;/launch&gt;\n</code></pre> </li> <li> <p>Edit the default values in the <code>To be modified</code> section:</p> <pre><code>&lt;!-- To be modified --&gt;\n&lt;arg name=\"map_file\" default=\" $(find week2_navigation)/maps/{map name}.yaml\"/&gt;\n&lt;arg name=\"initial_pose_x\" default=\"0.0\"/&gt;\n&lt;arg name=\"initial_pose_y\" default=\"0.0\"/&gt;\n</code></pre> <ol> <li>Change <code>{map name}</code> to the name of your map file as created in the previous exercise (remove the <code>{}</code>s!).</li> <li>Change the <code>initial_pose_x</code> and <code>initial_pose_y</code> default values. Current these are both set to <code>\"0.0\"</code>, but they need to be set to match the coordinates of the start zone of the <code>tuos_ros_simulations/nav_world</code> environment (we may have given you a clue about these in the table earlier!) </li> </ol> </li> <li> <p>Once you've made these changes, save the file and then launch it:</p> <p>TERMINAL 1: <pre><code>{BLANK} week2_navigation navigation.launch\n</code></pre></p> <p>Fill in the Blank!</p> <p>Which ROS command do we use to execute launch files?</p> </li> <li> <p>RViz and Gazebo should be launched, both windows looking something like this:</p> <p> </p> <p>Question</p> <p>How many nodes were actually launched on our ROS Network by executing this launch file?</p> <p>As shown in the figure, in RViz you should see the map that you generated with SLAM earlier.</p> <ul> <li>There should be a \"heatmap\" surrounding your robot and a lot of green arrows scattered all over the place. </li> <li> <p>The green arrows represent the localisation particle cloud, and the fact that these are all scattered across quite a wide area at the moment indicates that there is currently a great deal of uncertainty about the actual robot pose within the environment. Once we start moving around, this will improve and the arrows will start to converge more closely around the robot. </p> <p>This is actually called a \"costmap\", and it illustrates what the robot perceives of its environment: blue regions representing safe space that it can move around in; red regions representing areas where it could collide with an obstacle.</p> </li> <li> <p>Finally, the green dots illustrate the real-time <code>LaserScan</code> data coming from the LiDAR sensor, as we saw earlier. This should be nicely overlaid on top of the boundaries in our map.</p> </li> </ul> </li> <li> <p>To send a navigation goal to our robot we need to issue a request to the move_base action server. We will cover ROS Actions later in this course, but for now, all you really need to know is that we can send a navigation goal by publishing a message to a topic on the ROS network. In TERMINAL 2 run <code>rostopic list</code> and filter this to show only topics related to <code>/move_base</code>:</p> <p>TERMINAL 2: <pre><code>rostopic list | grep /move_base\n</code></pre> This will provide quite a long list, but right at the bottom you should see the following item: <pre><code>/move_base_simple/goal\n</code></pre></p> <p>We will use this to publish navigation goals to our robot to make it move autonomously using the ROS Navigation Stack.</p> </li> <li> <p>Running <code>rostopic info</code> on this topic will allow us to find out more about it:</p> <p>TERMINAL 2: <pre><code>rostopic info move_base_simple/goal\n</code></pre> <pre><code>Type: geometry_msgs/PoseStamped\n\nPublishers:\n  * /rviz (http://localhost:#####/)\n\nSubscribers:\n  * /move_base (http://localhost:#####/)\n  * /rviz (http://localhost:#####/)\n</code></pre></p> <p>Question</p> <p>What type of message does this topic use, and which ROS package does it live within?</p> </li> <li> <p>Run another command now to find out what the structure of this message is (you did this earlier for the <code>LaserScan</code> messages published to the <code>/scan</code> topic).</p> </li> <li> <p>Knowing all this information now, we can use the <code>rostopic pub</code> command to issue a navigation goal to our robot, via the <code>/move_base_simple/goal</code> topic. This command works exactly the same way as it did when we published messages to the <code>/cmd_vel</code> topic last week (when we made the robot move at a velocity of our choosing).</p> <p>Remember that the <code>rostopic pub</code> command takes the following format:</p> <pre><code>rostopic pub {topic_name} {message_type} {data}\n</code></pre> <p>...but to make life easier, we can use the autocomplete functionality in our terminal to help us format the message correctly:</p> <pre><code>rostopic pub {topic_name} {message_type}[SPACE][TAB]\n</code></pre> <p>Do this now, (replacing <code>{topic_name}</code> and <code>{message_type}</code> accordingly) to generate the full message structure that we will use to send the navigation goal to the robot, from the terminal. Don't press <code>Enter</code> yet though, as we will need to edit the message data in order to provide a valid navigation goal.</p> </li> <li> <p>There are four things in this message that need to be changed before we can publish it:</p> <ol> <li><code>frame_id: ''</code> should be changed to <code>frame_id: 'map'</code></li> <li> <p>The <code>pose.orientation.w</code> value needs to be changed to <code>1.0</code>:</p> <pre><code>pose:\n  orientation:\n    w: 1.0\n</code></pre> </li> <li> <p>The <code>pose.position.x</code> and <code>pose.position.y</code> parameters define the location, in the environment, that we want the robot to move to, as determined in the previous exercise:</p> <pre><code>pose:\n  position:\n    x: {desired location in x}\n    y: {desired location in y}\n</code></pre> <p>Scroll back through the message using the left arrow key on your keyboard (\u2190), and modify the four parameters of the message accordingly, setting your <code>x</code> and <code>y</code> coordinates to make the robot move to any of the four marker zones in the environment.</p> </li> </ol> </li> <li> <p>Once you're happy, hit <code>Enter</code> and watch the robot move on its own to the location that you specified!</p> <p> </p> <p>Notice how the green particle cloud arrows very quickly converge around the robot as it moves around? This is because the robot is becoming more certain of it's pose (its position and orientation) within the environment as it compares the boundaries its LiDAR sensor can actually see with the boundaries marked out in the map that you supplied to it.</p> </li> <li> <p>Have a go at requesting more goals by issuing further commands in the terminal (using <code>rostopic pub</code>) to make the robot move between each of the four zone markers.</p> </li> </ol> <p>Summary:</p> <p>We have just made a robot move by issuing navigation goal requests to an Action Server on our ROS Network. You will learn more about ROS Actions in Week 5, where you will start to understand how this communication method actually works. You will also learn how to create Action Client Nodes in Python, so that - in theory - everything that you have been doing on the command-line in this exercise could be done programmatically instead.</p> <p>As you have observed in this exercise, in order to use ROS navigation tools to make a robot move autonomously around an environment there are a few important things that we need to provide to the Navigation Stack:</p> <ol> <li>A map of the environment that we want to navigate around.     This means that our robot needs to have already explored the environment once beforehand to know what the environment actually looks like. We drove our robot around manually in this case but, often, some sort of basic exploratory behaviour would be required in the first instance so that the robot can safely move around and create a map (using SLAM) without crashing into things! You will learn more about robotic search/exploration strategies in your lectures.</li> <li>The robot's initial location within the environment.     ...so that it could compare the map file that we supplied to it with what it actually observes in the environment. If we didn't know where the robot was to begin with, then some further exploration would be required to start with, in order for the robot to build confidence in its actual pose in the environment, prior to navigating it.</li> <li>The coordinates of the places we want to navigate to.     This may seem obvious, but it's an extra thing that we need to establish before we are able to navigate autonomously.</li> </ol>"},{"location":"com2009/la1/week3/#further-reading","title":"Further Reading","text":"<p>The ROS Robot Programming eBook that we have mentioned previously goes into more detail on how SLAM and the autonomous navigation tools that you have just implemented actually work.  There is information in here on how these tools have been configured to work with the TurtleBot3 robots specifically.  We therefore highly recommend that you download this book and have a read of it.  You should read through Chapters 11.3 (\"SLAM Application\") and 11.4 (\"SLAM Theory\") in particular, and pay particular attention to the following:  </p> <ul> <li>What information is required for SLAM? One of these bits of information may be new to you: how does this relate to Odometry, which you do know about? (See Section 11.3.4)</li> <li>Which nodes are active in the SLAM process and what do they do?  What topics are published and what type of messages do they use?  How does the information flow between the node network?</li> <li>Which SLAM method did we use? What parameters had to be configured for our TurtleBot3 Waffle specifically, and what do all these parameters actually do?</li> <li>What are the 5 steps in the iterative process of pose estimation? </li> </ul> <p>We would also recommend you read Chapter 11.7 (\"Navigation Theory\") too, which should allow you to then answer the following:</p> <ul> <li>What is the algorithm that is used to perform pose estimation?</li> <li>What process is used for trajectory planning? </li> <li>How many nodes do we need to launch to activate the full navigation functionality on our ROS Network? (We asked you this earlier, and the best way to determine it might be to do it experimentally, i.e.: using the <code>rosnode</code> command-line tool perhaps?)!</li> </ul>"},{"location":"com2009/la1/week3/#wrapping-up","title":"Wrapping Up","text":"<p>This week you have learnt how to develop an odometry-based controller to make your robot follow a square motion path.  You will likely have observed some degree of error in this which, as you already know, could be due to the fact that Odometry data is determined by dead-reckoning and is therefore subject to drift and error.  Consider how other factors may impact the accuracy of control too:</p> <ul> <li>How might the rate at which the odometry data is sampled play a role?</li> <li>How quickly can your robot receive new velocity commands, and how quickly can it respond?</li> </ul> <p>Be aware that we did all this in simulation here too. In fact, in a real world environment, this type of navigation might be less effective, since things such as measurement noise and calibration errors can also have considerable impact.  You will have the opportunity to experience this first hand later in this course.</p> <p>Ultimately then, we have seen a requirement here for additional information to provide more confidence of a robot's location in its environment, in order to enhance its ability to navigate effectively and avoid crashing into things!</p> <p>You therefore also learnt about the LiDAR sensor today, and the information that can be obtained from it.  We explored where this data is published, how we access it, and what it tells us about a robot's immediate environment.  We then looked at some ways odometry and laser displacement data can be combined to perform advanced robotic functions such as the mapping of an environment and the subsequent navigation around it. This is all complicated stuff but, using ROS, we can leverage these tools with relative ease, which illustrates just how powerful this framework can be for developing robotic applications quickly and effectively without having to re-invent the wheel!</p>"},{"location":"com2009/la1/week3/#backup","title":"Saving your work","text":"<p>Remember, the work you have done in the WSL-ROS environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University U: Drive, allowing you to restore it at the start of the next session.</p> <ol> <li> <p>Hint: You can use the <code>roscd</code> command for this!\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/la1/week4/","title":"Week 4: ROS Services","text":"<p>Info</p> <p>You should be able to complete most of the exercises on this page within a two-hour lab session, but you might need to spend a bit more time on the final exercise.</p>"},{"location":"com2009/la1/week4/#introduction","title":"Introduction","text":""},{"location":"com2009/la1/week4/#aims","title":"Aims","text":"<p>In this session you will learn about another communication method that can be used to transmit data/information and invoke actions across a ROS Network: ROS Services. You will learn how ROS Services can be used in combination with the standard publisher/subscriber principles that you already know about to control a robot more effectively for certain operations.</p>"},{"location":"com2009/la1/week4/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Recognise how ROS Services differ from the standard topic-based publisher-subscriber approach, and identify appropriate use-cases for this type of messaging system.</li> <li>Implement Python node pairs to observe services in action, and understand how they work.</li> <li>Invoke different services using a range of service message types.</li> <li>Develop Python Service nodes of your own to perform specific robotic tasks.</li> <li>Harness Services, in combination with LiDAR data, to implement a basic obstacle avoidance behaviour.</li> <li>Demonstrate your understanding of ROS so far by developing a Python node which incorporates elements from this and previous parts of this course.</li> </ol>"},{"location":"com2009/la1/week4/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Creating a Service Server in Python and calling it from the command-line</li> <li>Exercise 2: Creating a Python Service Client Node</li> <li>Exercise 3: Making and calling your own Service</li> <li>Exercise 4: Approaching an object using a Service and closed-loop control</li> </ul>"},{"location":"com2009/la1/week4/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Service Server Code (for Exercise 1)</li> <li>The Service Client Code (for Exercise 2)</li> <li>Creating a <code>/scan</code> Callback Function</li> </ul>"},{"location":"com2009/la1/week4/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch WSL-ROS Launch your WSL-ROS environment by running the WSL-ROS shortcut in the Windows Start Menu (if you haven't already done so). Once installed, the Windows Terminal app should launch with an Ubuntu terminal instance ready to go (TERMINAL 1).</p> <p>Step 2: Restore your work When prompted (in TERMINAL 1), enter <code>Y</code> to restore your work from last time1.</p> <p>Step 3: Launch VS Code Follow these steps to launch VS Code correctly within the WSL-ROS environment.</p> <p>Step 4: Launch the Robot Simulation From TERMINAL 1, launch the TurtleBot3 Waffle \"Empty World\" simulation:</p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre> ...and then wait for the Gazebo window to open:</p> <p></p> Remember <p>You can also use the <code>tb3_empty_world</code> command-line alias to launch the simulation, rather than using that long <code>roslaunch</code> command!</p>"},{"location":"com2009/la1/week4/#an-introduction-to-services","title":"An Introduction to Services","text":"<p>So far, we've learnt about ROS topics and messages, and how individual nodes can access data on a robot by simply subscribing to topics that are being published by any other node on the system.  In addition to this, we also learnt how any node can publish messages to any topic: this essentially broadcasts the data contained in the message across the ROS Network, making it available to any other node on the network that may wish to access it.</p> <p>Another way to pass data between ROS Nodes is by using Services.  These are different to messages in that \"Service calls\" (that is, the process of requesting a service) occur only between one node and another:</p> <ol> <li>One node (a Service Client) sends a Request to another node.</li> <li>Another node (a Service Server) processes that request, performs an action and then sends back a Response.</li> </ol> <p></p> <p>Services are Synchronous (or sequential): When a ROS node sends a request to a service (as a Service Client) it can't do anything else until the service has been completed and the Service Server has sent a response back. This can be useful for a variety of reasons:</p> <ul> <li> <p>Discrete, short-duration actions:</p> <ul> <li>A robot might need to do something before it can move on to something else, i.e.: it needs to see something before it can move towards it.</li> <li>High definition cameras generate large amounts of data and consume battery power, so you may wish to turn a camera on for a specific amount of time (e.g. until an image has been captured) and then turn it off again.</li> </ul> </li> <li> <p>Computations: Remember that ROS is network-based so you might want to offload some computations to a remote computer or a different device on a robot, e.g.:</p> <ul> <li>A client might send some data and then wait for another process (the server) to process it and send back the result.</li> </ul> </li> </ul> <p>It's also worth noting that any number of ROS Client nodes can call a service, but you can only have a single Server providing that particular service at any one time.</p> <p></p> <p>Questions</p> <p>Can you think of any other scenarios where this type of communication protocol might be useful?</p> <p>You'll explore how all this works in the next two exercises, where you will create service Server and Client nodes in Python, launch them from the command-line and observe the outcomes.</p>"},{"location":"com2009/la1/week4/#ex1","title":"Exercise 1: Creating a Service Server in Python and calling it from the command-line","text":"<p>To start with, let's set up a service and learn how to make a call to it from the command-line to give you an idea of how this all works and why it might be useful.</p> <ol> <li> <p>First open up a new terminal instance (TERMINAL 2) and create a package called <code>week4_services</code> using the <code>catkin_create_pkg</code> tool as you have done in previous weeks:</p> <ol> <li> <p>Navigate to the <code>catkin_ws/src</code> directory:</p> <p>TERMINAL 2: <pre><code>cd ~/catkin_ws/src\n</code></pre></p> </li> <li> <p>Create the <code>week4_services</code> package and define <code>rospy</code>, <code>geometry_msgs</code> and <code>tuos_ros_msgs</code> as dependencies:</p> <p>TERMINAL 2: <pre><code>catkin_create_pkg week4_services rospy geometry_msgs tuos_ros_msgs\n</code></pre></p> </li> <li> <p>Run <code>catkin build</code> on the package:</p> <p>TERMINAL 2: <pre><code>catkin build week4_services\n</code></pre></p> </li> <li> <p>And then re-source your environment:</p> <p>TERMINAL 2: <pre><code>source ~/.bashrc\n</code></pre></p> Tip <p>We're having to do this <code>source ~/.bashrc</code> thing a lot aren't we?! We've created a handy alias for it... use <code>src</code> instead!</p> </li> </ol> </li> <li> <p>Then, navigate to your package <code>src</code> folder that should have been created by <code>catkin_create_pkg</code>:</p> <p>TERMINAL 2: <pre><code>roscd week4_services/src\n</code></pre></p> </li> <li> <p>Create a file called <code>move_server.py</code> (using <code>touch</code>) and set this to be executable (using <code>chmod</code>).        </p> </li> <li> <p>Then, open the file in VS Code, copy and paste this code and then save it. </p> <p>Note</p> <p>It's really important that you understand how the code above works, so that you know how to build your own service Servers in Python. Make sure you read the code annotations thoroughly.</p> </li> <li> <p>Return to the terminal window and launch the node using <code>rosrun</code>:</p> <p>TERMINAL 2: <pre><code>rosrun week4_services move_server.py\n</code></pre></p> <p>You should see the message:</p> <pre><code>[INFO] [#####]: the 'move_service' Server is ready to be called...\n</code></pre> </li> <li> <p>Then open up a new terminal window (TERMINAL 3)</p> </li> <li> <p>We can use the <code>rosservice</code> command to view all the services that are currently active on our system:</p> <p>TERMINAL 3: <pre><code>rosservice list\n</code></pre></p> <p>You should see the <code>/move_service</code> service that we defined in the Python code:</p> <pre><code>service_name = \"move_service\"\n</code></pre> </li> <li> <p>We can find out more about this using the <code>rosservice info</code> command:</p> <p>TERMINAL 3: <pre><code>rosservice info /move_service\n</code></pre></p> <p>Which should provide the following output:</p> <pre><code>Node: /move_service_server\nURI: #####\nType: tuos_ros_msgs/SetBool\nArgs: request_signal\n</code></pre> <p>You may notice that the node name is <code>/move_service_server</code>, as set in our Python code when we initialised the node:</p> <pre><code>rospy.init_node(f\"{service_name}_server\")\n</code></pre> <p>Type tells us the type of message this service uses, and we'll look at this in more detail later. Args tells us what input arguments we need to supply to the service in order to make a valid service call (or Request).</p> </li> <li> <p>We can now call this service from the command-line using the <code>rosservice</code> command again.  The autocomplete functionality in the terminal can help us format this message correctly.  Type the following text followed by a space and two tabs as illustrated:</p> <p>TERMINAL 3: <pre><code>rosservice call /move_service[SPACE][TAB]\n</code></pre></p> <p>which should autocomplete the rest of the command for us:</p> <pre><code>rosservice call /move_service \"request_signal: false\"\n</code></pre> </li> <li> <p>Press <code>[ENTER]</code> to issue this command and make a call to the service.  You should see the following response:</p> <pre><code>response_signal: False\nresponse_message: \"Nothing happened, set request_signal to 'true' next time.\"\n</code></pre> </li> <li> <p>Arrange your windows so that you can see both the Gazebo simulation with your robot in, and the terminal that you just issued the <code>rosservice call</code> command (TERMINAL 3).</p> </li> <li> <p>In TERMINAL 3 enter the <code>rosservice call</code> command again, but this time setting the input argument to <code>true</code>.  Observe the response to the simulated robot in Gazebo.  Switch back to TERMINAL 2 and observe the terminal output here too.</p> </li> </ol> <p>Summary:</p> <p>You have just created a node in Python to launch a service. This node acted as a Server: sitting idle and waiting, indefinitely, for its service to be called. We then issued the call to the service via the command-line, which then prompted our Service Server to carry out the tasks that we had defined within the Python code, namely:</p> <ol> <li>Start a timer.</li> <li>Issue a velocity command to the robot to make it move forwards.</li> <li>Wait for 5 seconds.</li> <li>Issue a velocity command to make the robot stop.</li> <li>Prepare a Service Response and issue this to the terminal in which we called the service (TERMINAL 3).</li> </ol>"},{"location":"com2009/la1/week4/#rossrv","title":"Using <code>rossrv</code>","text":"<p>In the previous exercise we used <code>rosservice list</code> to identify all the services that were currently active on the ROS system.  We then used <code>rosservice info</code> to find out a bit more about the service that we had launched with our Python node (which we called <code>/move_service</code>).</p> <pre><code>rosservice info /move_service:\n\nNode: /move_service_server\nURI: #####\nType: tuos_ros_msgs/SetBool\nArgs: request_signal\n</code></pre> <p>Type tells us the type of message this service uses. Just like a topic message there are two parts to this definition:</p> <pre><code>tuos_ros_msgs/SetBool\n</code></pre> <ol> <li>The service message is part of a package called <code>tuos_ros_msgs</code></li> <li>The message itself is called <code>SetBool</code></li> </ol> <p>We can find out more about this using the <code>rossrv</code> command, which has the same usage as the <code>rosmsg</code> command that you have already used previously (for interrogating topic messages). <code>rossrv</code> gives us information about all the service messages that are installed on our system and that are available for us to use in any ROS applications that we create:</p> <p>TERMINAL 3: <pre><code>rossrv info tuos_ros_msgs/SetBool\n</code></pre> ... which gives: </p> <pre><code>bool request_signal\n---\nbool response_signal\nstring response_message\n</code></pre>"},{"location":"com2009/la1/week4/#the-format-of-a-service-message","title":"The Format of a Service Message","text":"<p>As you can see from above, service messages have two parts to them, separated by three hyphens (<code>---</code>). Above the separator is the Service Request, and below it is the Service Response:</p> <pre><code>bool request_signal     &lt;-- Request\n---\nbool response_signal    &lt;-- Response (Parameter 1/2)\nstring response_message &lt;-- Response (Parameter 2/2)\n</code></pre> <p>In order to Call a service, we need to provide data to it in the format specified in the Request section of the message. A service Server (like the Python node we created above) will then send data back to the caller in the format specified in the Response section of the message.</p> <p>The <code>tuos_ros_msgs/SetBool</code> service message that we're working with here has a one request parameter:</p> <ol> <li>A boolean input called <code>request_signal</code>     ...which is the only thing we need to send to the Service Server in order to call the service.</li> </ol> <p>There are then two response parameters:</p> <ol> <li>A boolean flag called <code>response_signal</code></li> <li>A text string called <code>response_message</code>     ...both of these will be returned to the client, by the server, once the Service has completed.</li> </ol>"},{"location":"com2009/la1/week4/#ex2","title":"Exercise 2: Creating a Python Service Client Node","text":"<p>As well as calling a service from the command-line we can also build Python nodes to do the same thing (i.e. we can build Python Service Client Nodes). In this exercise you will learn how this is done.</p> <ol> <li> <p>TERMINAL 3 should be idle, so from here navigate to the <code>src</code> folder within the <code>week4_services</code> package that we created earlier:</p> <p>TERMINAL 3: <pre><code>roscd week4_services/src\n</code></pre></p> </li> <li> <p>Create a new file called <code>move_client.py</code> and make sure that this is executable.</p> </li> <li> <p>Launch the file in VS Code, copy and paste this code and then save the file. </p> <p>Note</p> <p>Once again, be sure to read the code annotations, and make sure that you understand how this Python Service Client Node works too!</p> </li> <li> <p>Return to TERMINAL 3 and launch the node using <code>rosrun</code>:</p> <p>TERMINAL 3: <pre><code>rosrun week4_services move_client.py\n</code></pre></p> <p>The response should be exactly the same as observed in Exercise 1.</p> </li> </ol>"},{"location":"com2009/la1/week4/#ex3","title":"Exercise 3: Making and calling your own Service","text":"<p>In this exercise you will create your own service Server to make the Waffle perform a specific movement for a given amount of time and then stop.</p> <p>A service message called <code>tuos_ros_msgs/TimedMovement</code> has already been set up to help you do this. Interrogate this using the <code>rossrv</code> command (as described above) to work out how to use this message in your Python Server node.</p> <p>The service should respond to four different movement commands to invoke four different actions:</p> <ol> <li><code>\"fwd\"</code>: Move forwards.</li> <li><code>\"back\"</code>: Move backwards.</li> <li><code>\"left\"</code>: Turn left.</li> <li><code>\"right\"</code>: Turn right.</li> </ol> <p>The Server should make the robot perform the desired action for a duration that is also specified within the service message (in seconds).</p> <p>Procedure:</p> <ol> <li>Close down the Service Server that is currently running in TERMINAL 2.</li> <li> <p>Create a new node in your <code>week4_services</code> package:</p> <ol> <li> <p>Navigate to the <code>week4_services/src</code> folder using <code>roscd</code>:</p> <p>TERMINAL 2: <pre><code>roscd week4_services/src\n</code></pre></p> </li> <li> <p>You can use the <code>move_server.py</code> node that you created earlier as a starting point if you want to. Copy the file and rename it <code>timed_move_server.py</code> using the <code>cp</code> command:</p> <p>TERMINAL 2: <pre><code>cp move_server.py timed_move_server.py\n</code></pre></p> </li> </ol> </li> <li> <p>Open the new <code>timed_move_server.py</code> file in VS Code and modify it as follows:</p> <ol> <li>Change the imports to utilise the correct service message type (<code>tuos_ros_msgs/TimedMovement</code>).</li> <li>Modify the <code>rospy.Service</code> call to use the <code>TimedMovement</code> service message type.</li> <li> <p>Develop the <code>callback_function()</code> to:</p> <ol> <li> <p>Process the two parameters that will be provided to the server via the <code>service_request</code> input argument.  </p> <p>Remember</p> <p>You can use <code>rossrv info ...</code> to find out what these two parameters are called, and their data types.</p> </li> <li> <p>Make the robot perform the correct action.</p> </li> <li>Return a correctly formatted service response message to the service caller.</li> <li>Launch your server node using <code>rosrun</code> from TERMINAL 2 and call the service from the command-line using the <code>rosservice call</code> command in TERMINAL 3, as you did earlier.</li> </ol> </li> </ol> </li> </ol>"},{"location":"com2009/la1/week4/#a-recap-on-everything-youve-learnt-so-far","title":"A recap on everything you've learnt so far...","text":"<p>You should now hopefully understand how to use the ROS Service architecture and understand why, and in what context, it might be useful to use this type of communication method in a robot application.</p> <p>Remember</p> <p>Services are synchronous and are useful for one-off, quick actions; or for offloading jobs or computations that might need to be done before something else can happen.  (Think of it as a transaction that you might make in a shop: You hand over some money, and in return you get a chocolate bar, for example!)</p> <p>Over the last four sessions we've learnt how to use a range of key ROS tools, and hopefully you're starting to understand how ROS works and how you might approach a robot programming task using this framework. In the final exercise of this session you'll consolidate some of the things that you've done so far:</p> <ul> <li>How to publish and subscribe to topics.</li> <li>How to make a robot move.</li> <li>How to interpret Laser Displacement Data from the LiDAR sensor.</li> <li>How to invoke an action using a ROS Service.</li> <li>How to develop ROS Nodes in Python, and how to use the Python Class Structure.</li> </ul>"},{"location":"com2009/la1/week4/#sim-env-mods","title":"Manipulating the Environment in Gazebo","text":"<p>In order to carry out the last exercise you'll also need to be able to manipulate the robot's simulated environment using some basic tools in Gazebo. First, make sure that there are no active processes running in TERMINALS 2 or 3, but leave the Gazebo simulation in TERMINAL 1 running.</p> <p>In the Gazebo simulation window, use the \"Box\" tool in the top toolbar to place a box in front of the robot:</p> <p></p> <p>Use the \"Scale Mode\" button to resize the box and use the \"Translation Mode\" button to reposition it.</p> <p></p> <p>Once you are happy with this, right-click on the object and select \"Delete\" to remove it from the world. </p> <p></p>"},{"location":"com2009/la1/week4/#ex4","title":"Exercise 4: Approaching an object using a Service and closed-loop control","text":"<p>For this exercise you need to build another Python Server node which must perform the following tasks:</p> <ol> <li>Make the robot move forwards, towards an object placed in front of it. As you know, you'll do this by publishing velocity commands to the <code>/cmd_vel</code> topic.</li> <li>The server node must then stop the robot before it hits the obstacle that you have placed in front of it by subscribing to the <code>/scan</code> topic and monitoring distance information from the LiDAR sensor telling us how far away the object is. </li> <li>The server must do this by considering two inputs received from a Service Request:<ol> <li>The speed (in m/s) at which to approach the object.</li> <li>The distance (in meters) at which the robot must stop in front of it.</li> </ol> </li> <li>A service message called <code>tuos_ros_msgs/Approach</code> is available for you to use for this exercise. Use this to build your service server. Remember, you can find out more about this message using <code>rossrv info</code>.</li> <li> <p>We haven't really done much work with the LiDAR data published to the <code>/scan</code> topic yet, so you might want to consider this suggested approach for building a <code>/scan</code> callback function. </p> <p>Tip</p> <p>You should use a class structure in your Python code here. Start off with the subscriber node from Week 1 and add code to this to build the functionality required for this exercise.</p> </li> </ol>"},{"location":"com2009/la1/week4/#wrapping-up","title":"Wrapping Up","text":"<p>In this session you have learnt about ROS Services and why they might be useful for a Robot:</p> <ul> <li>Services differ from standard topic-based communication methods in ROS in that they are a direct form of communication between one node and another.  </li> <li>The communication between the two nodes is sequential or synchronous: once a service Caller has called a service, it cannot continue until it has received a response.</li> <li>This is useful for controlling quick, short-duration tasks or for offloading computations (which could perhaps also be considered decision making).</li> </ul> <p>Having completed this week's exercises, you should now be able to:</p> <ul> <li>Create and execute Python Service Servers.</li> <li>Create and execute Python Service Callers, as well as call services from the command-line.</li> <li>Implement these principles with a range of different service message types to perform a number of different robot tasks.</li> <li>Use LiDAR data effectively for basic closed-loop robot control.</li> <li>Develop Python nodes which also incorporate principles from the previous three weeks of this course:<ul> <li>Publishing and subscribing to topics.</li> <li>Controlling the velocity and position of a robot.</li> <li>Using the Python Class architecture.</li> <li>Harnessing ROS and Linux command-line tools.</li> </ul> </li> </ul>"},{"location":"com2009/la1/week4/#backup","title":"Saving your work","text":"<p>Remember, the work you have done in the WSL-ROS environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University U: Drive, allowing you to restore it at the start of the next session.</p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command at any time.\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/la1/week5/","title":"Week 5: ROS Actions","text":"<p>Info</p> <p>You should be able to complete Exercises 1, 2 &amp; 3 on this page within a two-hour lab session, but you'll need to spend a bit more time on Exercise 4. There are also some advanced exercises that you might want to have a look at in your own time too...</p>"},{"location":"com2009/la1/week5/#introduction","title":"Introduction","text":""},{"location":"com2009/la1/week5/#aims","title":"Aims","text":"<p>This week you will learn about a third (and final) communication method available within ROS: Actions.  Actions are essentially an advanced version of ROS Services, and you will learn about exactly how these two differ and why you might choose to employ an action over a service for certain robotic tasks. </p>"},{"location":"com2009/la1/week5/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Recognise how ROS Actions differ from ROS Services and explain where this method might be useful in robot applications.</li> <li>Explain the structure of Action messages and identify the relevant information within them, enabling you to build Action Servers and Clients.</li> <li>Implement Python Action Client nodes that utilise concurrency and preemption.</li> <li>Develop Action Server &amp; Client nodes that could be used as the basis for a robotic search strategy.</li> </ol>"},{"location":"com2009/la1/week5/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Launching an Action Server and calling it from the command-line</li> <li>Exercise 2: Building a Python Action Client Node with concurrency</li> <li>Exercise 3: Building a Preemptive Python Action Client Node</li> <li>Exercise 4: Developing an \"Obstacle Avoidance\" behaviour using an Action Server</li> <li>Advanced (and optional!) exercises</li> </ul>"},{"location":"com2009/la1/week5/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Action Client Code (for Exercise 2)</li> <li>The Preemptive Action Client Code (for Exercise 3)</li> </ul>"},{"location":"com2009/la1/week5/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch WSL-ROS Launch your WSL-ROS environment by running the WSL-ROS shortcut in the Windows Start Menu (if you haven't already done so). Once installed, the Windows Terminal app should launch with an Ubuntu terminal instance ready to go (TERMINAL 1).</p> <p>Step 2: Restore your work When prompted (in TERMINAL 1), enter <code>Y</code> to restore your work from the previous sessions1.</p> <p>Step 3: Launch VS Code Follow these steps to launch VS Code correctly within the WSL-ROS environment.</p>"},{"location":"com2009/la1/week5/#calling-an-action-server","title":"Calling an Action Server","text":"<p>Before we talk about what actions actually are, we're going to dive straight in and see one in action (excuse the pun). As you may remember from the Week 3 session, you actually used a ROS Action to make your robot navigate autonomously in Exercise 4, by calling an action server from the command-line. We will do a similar thing now, in a different context, and this time we'll also look at what's going on in a bit more detail.</p>"},{"location":"com2009/la1/week5/#ex1","title":"Exercise 1: Launching an Action Server and calling it from the command-line","text":"<p>We'll play a little game here. We're going to launch our TurtleBot3 Waffle in a mystery environment now, and we're going to do this by launching Gazebo headless i.e. Gazebo will be running behind the scenes, but there'll be no Graphical User Interface (GUI) to show us what the environment actually looks like.  Then, we'll use an action server to make our robot scan the environment and take pictures for us, to reveal its surroundings!</p> <ol> <li> <p>To launch our TurtleBot3 Waffle in the mystery environment, use the following <code>roslaunch</code> command:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_ros_simulations mystery_world.launch\n</code></pre></p> <p>Messages in the terminal should indicate that something has happened, but that's about all you will see!</p> </li> <li> <p>Next, open up a new instance of the Windows Terminal Application by pressing the \"New Tab\" button whilst pressing the <code>Shift</code> key (we'll call this WT(B)).</p> </li> <li> <p>In WT(B) have a look at all the topics that are currently active on the ROS network (you should know exactly how to do this by now!)</p> </li> <li> <p>Return to the original Windows Terminal instance (the one with the Gazebo processes running, and which we'll now refer to as WT(A)), open up a new tab (WT(A) TERMINAL 2) and launch an action server that we have already prepared for you for this exercise:</p> <p>WT(A) TERMINAL 2: <pre><code>roslaunch tuos_ros_examples camera_sweep.launch\n</code></pre></p> </li> <li> <p>Now, return to WT(B) and take a look again at all the topics that are active on the ROS network.</p> <p>Questions</p> <ul> <li>What do you notice?</li> <li>Anything new there now compared to when you ran the same command before?</li> </ul> <p>You should in fact notice 5 new items in that list:</p> <pre><code>/camera_sweep_action_server/cancel\n/camera_sweep_action_server/feedback\n/camera_sweep_action_server/goal\n/camera_sweep_action_server/result\n/camera_sweep_action_server/status\n</code></pre> <p>A ROS action therefore has five messages associated with it. We'll talk about these in a bit more detail later on, but for now, all we need to know is that in order to call an action, we need to send the action server a Goal (which you may remember doing in Week 3).</p> Comparison to ROS Services <p>This is a bit like sending a Request to a ROS Service Server, like we did in the previous session.</p> </li> <li> <p>ROS Actions use topic messages (unlike ROS Services, which use dedicated service messages). We can therefore tap into the ROS network and observe the messages being published to these in exactly the same way as we have done in previous weeks using <code>rostopic echo</code>. In order to monitor some of these messages now, we'll launch a couple more instances of the Windows Terminal, so that we can view a few things simultaneously:</p> <ol> <li>Once again, launch an additional Windows Terminal instance by pressing the \"New Tab\" button whilst pressing the <code>Shift</code> key (this one will be called WT(C)):</li> <li>Do this again to launch another Windows Terminal instance, which we'll call WT(D)</li> <li>You should now have four Windows Terminal applications open! Arrange these so that they are all visible: </li> </ol> <p> </p> </li> <li> <p>In WT(C) run a <code>rostopic echo</code> command to echo the messages being published to the <code>/camera_sweep_action_server/feedback</code> topic:</p> <p>WT(C): <pre><code>rostopic echo /camera_sweep_action_server/feedback\n</code></pre></p> <p>To begin with, you'll see the message:</p> <pre><code>WARNING: no messages received and simulated time is active.\nIs /clock being published?\n</code></pre> <p>Don't worry about this.</p> </li> <li> <p>Do the same in WT(D), but this time to echo the messages being published to the <code>/result</code> part of the action server message.</p> </li> <li> <p>Now, going back to WT(B), run the <code>rostopic pub</code> command on the <code>/camera_sweep_action_server/goal</code> topic, using the autocomplete functionality in the terminal to help you format the message correctly:</p> <p>WT(B): <pre><code>rostopic pub /camera_sweep_action_server/goal[SPACE][TAB][TAB]\n</code></pre></p> <p>Which should provide you with the following:</p> <pre><code>rostopic pub /camera_sweep_action_server/goal tuos_ros_msgs/CameraSweepActionGoal \"header:\n  seq: 0\n  stamp:\n  secs: 0\n  nsecs: 0\n  frame_id: ''\ngoal_id:\n  stamp:\n  secs: 0\n  nsecs: 0\n  id: ''\ngoal:\n  sweep_angle: 0.0\n  image_count: 0\"\n</code></pre> </li> <li> <p>Edit the <code>goal</code> portion of the message by using the left arrow button on your keyboard to scroll back through the message. Modify the <code>sweep_angle</code> and <code>image_count</code> parameters:</p> <ul> <li>sweep_angle is the angle (in degrees) that the robot will rotate on the spot</li> <li>image_count is the number of images it will capture from its front-facing camera while it is rotating</li> </ul> </li> <li>Once you have decided on some values, hit <code>Enter</code> to actually publish the message and call the action server.     Keep an eye on all four terminal instances. What do you notice happening in each of them?</li> <li> <p>Now, in WT(B):</p> <ol> <li>Cancel the <code>rostopic pub</code> command by entering <code>Ctrl+C</code></li> <li> <p>Once the action had completed, a message should have been published in WT(D) (a \"result\"), informing you of the filesystem location where the action server has stored the images that have just been captured by the robot:</p> <pre><code>result:\n  image_path: \"~/myrosdata/action_examples/YYYYMMDD_hhmmss\"\n---\n</code></pre> </li> <li> <p>Navigate to this directory in WT(B) (using <code>cd</code>) and have a look at the content using <code>ll</code> (a handy alias for the <code>ls</code> command):</p> <p>You should see the same number of image files in there as you requested with the <code>image_count</code> parameter.</p> </li> <li> <p>Launch <code>eog</code> in this directory and click through all the images to reveal your robot's mystery environment:</p> <p>WT(B): <pre><code>eog .\n</code></pre></p> </li> </ol> </li> <li> <p>Finally, open another tab in the WT(A) terminal instance (WT(A) TERMINAL 3) and launch the Gazebo client to view the simulation that has, until now, been running headless:</p> <p>WT(A) TERMINAL 3: <pre><code>gzclient\n</code></pre></p> </li> <li> <p>The actual simulated environment should now be revealed!! To finish off, close down some active ROS processes and Windows Terminal instances that we've just been working with:</p> <ol> <li>Close down the <code>eog</code> window and the WT(B) Windows Terminal instance.</li> <li>Stop the <code>rostopic echo</code> commands that are running in WT(C) and WT(D) by entering <code>Ctrl+C</code> in each of them and then close each of these Windows Terminal instances too.</li> <li>Enter <code>Ctrl+C</code> in WT(A) TERMINAL 3 to stop the Gazebo GUI, but keep the terminal tab open. </li> <li>Leave the processes running in WT(A) TERMINAL 2 and 1 for now (the Action Server and the headless Gazebo processes).</li> </ol> </li> </ol> <p>Summary:</p> <p>Phew, that was a long one! Essentially, what we did here is launched an action server and then called it from the command-line using <code>rostopic pub</code>. Hopefully, while the action server was performing the task that we had requested, you also noticed that it was providing us with some real-time feedback on how it was getting on (in WT(C)). In the same way as a ROS Service, it should also have provided us with a result (in WT(D)), once the action had been completed.  Feedback is one of the key features that differentiates a ROS Action from a ROS Service, but there are other interesting features too, and we'll explore these in more detail now.</p>"},{"location":"com2009/la1/week5/#what-is-a-ros-action","title":"What is a ROS Action?","text":"<p>As you will have observed from the above exercise, a ROS Action actually seems to work a lot like a ROS Service.  We've seen that we have a feedback message associated with an Action though, which is indeed different, but this isn't the main differentiating feature. The key difference is that when a node calls a ROS Action (i.e. an action \"Caller\" or \"Client\"), it doesn't need to wait until the action is complete before it can move on to something else: it can continue to do other tasks at the same time. Unlike ROS Services then, ROS Actions are Asynchronous, which makes them useful when implementing robotic behaviours that take a longer time to execute, and which an Action Client might need to be updated on throughout the process.</p> <p>Recall the five messages associated with the action server from the exercise above, the messages had the following names:</p> <pre><code>/cancel\n/feedback\n/goal\n/result\n/status\n</code></pre> <p>The top item there hints at the most important feature of ROS Actions: they can be cancelled (or \"preempted\"), which we'll learn more about later.  </p> <p>The other thing to note is that - where we used the <code>rosservice</code> command to interrogate the ROS Services that were active on our ROS network previously - Actions use ROS Topics, so we use <code>rostopic</code> commands to interrogate action servers:</p> <ol> <li><code>rostopic list</code>: to identify the action servers that are available on the network.</li> <li><code>rostopic echo</code>: to view the messages being published by a given action server.</li> <li><code>rostopic pub</code>: to call an action from the command-line. </li> </ol>"},{"location":"com2009/la1/week5/#the-format-of-action-messages","title":"The Format of Action Messages","text":"<p>Like Services, Action Messages have multiple parts to them, and we need to know what format these action messages take in order to be able to call them. We don't have a tool like <code>rossrv</code> to do this for Actions though, instead we have to use <code>rosmsg</code>, or look for the message definition inside the Action Message Package.</p> <p>We ran <code>rostopic list</code> to identify our action server in the previous exercise, which told us that there was an action server running called <code>/camera_sweep_action_server</code>:</p> <pre><code>rostopic list\n[some topics...]\n/camera_sweep_action_server/cancel\n/camera_sweep_action_server/feedback\n/camera_sweep_action_server/goal\n/camera_sweep_action_server/result\n/camera_sweep_action_server/status\n[some more topics...]\n</code></pre>"},{"location":"com2009/la1/week5/#cancel-and-status","title":"\"Cancel\" and \"Status\"","text":"<p>Every ROS Action has both a cancel and status message associated with them. These are standardised, so the format of these two messages will always be the same, regardless of the type of Action Server we use. We won't worry about these too much for now, but we'll make use of them in some ROS Nodes that we'll build in a short while.</p> <p>The feedback, goal and result messages will be different for any given action server though, and so we need to know about the format of all of these before we attempt to make a call to the action server.</p> <p>We can run <code>rostopic info</code> on any of these to find out more about them...</p>"},{"location":"com2009/la1/week5/#goal","title":"\"Goal\"","text":"<p>Let's look at the goal to start with:</p> <p>TERMINAL 1: <pre><code>rostopic info /camera_sweep_action_server/goal\n</code></pre> From which we obtain the usual information:</p> <pre><code>Type: tuos_ros_msgs/CameraSweepActionGoal\n\nPublishers: None\n\nSubscribers:\n  * /camera_sweep_action_server (http://localhost:#####/)\n</code></pre> <p>The <code>Type</code> field tells us that the action message belongs to the <code>tuos_ros_msgs</code> package, and we can find out more about the <code>goal</code> message by using <code>rosmsg info</code>. You'll be familiar with how this works by now:</p> <pre><code>rosmsg info {messageType}\n</code></pre> <p>Where <code>{messageType}</code> is established from the output of the <code>rostopic info</code> command above: </p> <pre><code>Type: tuos_ros_msgs/CameraSweepActionGoal\n</code></pre> <p>When working with ROS Actions and the <code>rosmsg</code> command though, we can actually drop the word \"<code>Action</code>\" in the message Type, so our <code>rosmsg</code> command becomes:</p> <p>TERMINAL 1: <pre><code>rosmsg info tuos_ros_msgs/CameraSweepGoal\n</code></pre> Which will output:</p> <pre><code>float32 sweep_angle\nint32 image_count\n</code></pre> Further Info <p><code>rosmsg info tuos_ros_msgs/CameraSweepActionGoal</code> will work as well, but we get a lot of other information in the output that we're not all that interested in. Give it a go and see the difference, if you want to!</p> <p>In order to call this action server, we need to send a goal, and <code>rosmsg info</code> has just told us that there are two goal parameters that we must provide:</p> <ol> <li><code>sweep_angle</code>: a 32-bit floating-point value</li> <li><code>image_count</code>: a 32-bit integer</li> </ol> <p>So we know more about our Action Server's Goal now, but there are two other parameters we still know nothing about: Result and Feedback. It's important to know about all three things in order to be able to work with the Action Server effectively, and we can use an alternative approach to interrogate all three at the same time...</p>"},{"location":"com2009/la1/week5/#goal-feedback-and-result","title":"\"Goal,\" \"Feedback\" and \"Result\"","text":"<p>We know, from above, that the <code>/camera_sweep_action_server</code> messages are part of the <code>tuos_ros_msgs</code> package, so we can navigate to the package directory (using <code>roscd</code>) and look at the actual message definition. </p> <p>TERMINAL 1: <pre><code>roscd tuos_ros_msgs/\n</code></pre></p> <p>Actions are always contained within an <code>action</code> folder inside the package directory, so we can then navigate into this folder using <code>cd</code>:</p> <p>TERMINAL 1: <pre><code>cd action/\n</code></pre></p> <p>Use the <code>ll</code> command again here to view all the action messages within the package. Here you should see the <code>CameraSweep.action</code> message listed. Run <code>cat</code> on this file to view the full message definition:</p> <p>TERMINAL 1: <pre><code>cat CameraSweep.action\n</code></pre> <pre><code>#goal\nfloat32 sweep_angle    # the angular sweep over which to capture images (degrees)\nint32 image_count      # the number of images to capture during the sweep\n---\n#result\nstring image_path      # The filesystem location of the captured images\n---\n#feedback\nint32 current_image    # the number of images taken\nfloat32 current_angle  # the current angular position of the robot (degrees)\n</code></pre></p> <p>Questions</p> <ul> <li>What are the names of the result and feedback message parameters? (There are three parameters in total.)</li> <li>What datatypes do these parameters use?</li> </ul> <p>You'll learn how we use this information to develop Python Action Server &amp; Client nodes in the following exercises.</p>"},{"location":"com2009/la1/week5/#concurrent-activity","title":"Concurrent Activity","text":"<p>An Action Server provides feedback messages at regular intervals whilst performing an action and working towards its goal.  This is one way that an Action Client can monitor the progress of the action that it has requested.  Another way it can do this is by monitoring the status of an action.  Both of these features enable concurrency, allowing an action client to work on other things whilst waiting for the requested behaviour to be completed by the action server.</p> <p></p>"},{"location":"com2009/la1/week5/#ex2","title":"Exercise 2: Building a Python Action Client Node with Concurrency","text":"<ol> <li> <p>You should only have one Windows Terminal application instance open now, with three WSL-ROS terminal tabs in it. TERMINAL 3 should already be idle (i.e. not running any commands), and (if you haven't done so already) enter <code>Ctrl+C</code> in TERMINAL 1 and TERMINAL 2 to stop the headless Gazebo simulation processes and the Camera Sweep Action Server respectively. </p> </li> <li> <p>In TERMINAL 1 create a new package called <code>week5_actions</code> using the <code>catkin_create_pkg</code> tool as you have done previously. This time, define <code>rospy</code>, <code>actionlib</code> and <code>tuos_ros_msgs</code> as dependencies.</p> <p>Remember</p> <p>Make sure you're in your <code>~/catkin_ws/src/</code> folder when you run the <code>catkin_create_pkg</code> command!</p> </li> <li> <p>Once again, run <code>catkin build</code> on this and then re-source your environment:</p> <p>TERMINAL 1: First: <pre><code>catkin build week5_actions\n</code></pre> Then: <pre><code>source ~/.bashrc\n</code></pre></p> </li> <li> <p>Navigate to the <code>src</code> folder of this package, create a file called <code>action_client.py</code> (using <code>touch</code>) and set this to be executable (using <code>chmod</code>).        </p> </li> <li>Review the code provided here, and the annotations, then copy and paste the code into your newly created <code>action_client.py</code> file. </li> <li> <p>Then, in TERMINAL 2, execute the same launch file as before but this time with a couple of additional arguments:</p> <p>TERMINAL 2: <pre><code>roslaunch tuos_ros_simulations mystery_world.launch gui:=true camera_search:=true\n</code></pre></p> <p>... which will launch the Gazebo simulation in GUI mode this time, as well as the <code>/camera_sweep_action_server</code> too.</p> </li> <li> <p>In TERMINAL 1, use <code>rosrun</code> to call the action server with the <code>action_client.py</code> node that you have just created...</p> <p>... something not right? You may need to change the values that have been assigned to the goal parameters, in order for the client to successfully make a call to the server!</p> <p>The node we have just created, in its current form, uses a feedback callback function to perform some operations while the action server is working. In this case, it simply prints the feedback data that is coming from the Action Server.  That's it though, and the <code>client.wait_for_result()</code> line still essentially just makes the client node wait until the action server has finished doing its job before it can do anything else. This still therefore looks a lot like a service, so let's modify this now to really build concurrency into the client node.</p> </li> <li> <p>First, create a copy of your <code>action_client.py</code> node and call it <code>concurrent_action_client.py</code> (you will need to make sure you are still in the <code>src</code> directory of your <code>week5_actions</code> package before you run this command):</p> <p>TERMINAL 1: <pre><code>cp action_client.py concurrent_action_client.py\n</code></pre></p> </li> <li> <p>We want to use the status message from the action server now, and we can find out a bit more about this as follows:</p> <ol> <li>Use <code>rostopic info camera_sweep_action_server/status</code> to find the message type.</li> <li>Then, use <code>rosmsg info</code> (using the message type you have just identified) to tell you all the status codes that could be returned by the action server.</li> </ol> <p>You should have identified the following states, listed in the <code>status_list</code> portion of the message:</p> <pre><code>PENDING=0\nACTIVE=1\nPREEMPTED=2\nSUCCEEDED=3\nABORTED=4\nREJECTED=5\n...\n</code></pre> <p>We can set up our action client to monitor these status codes in a <code>while</code> loop, and then perform other operations inside this loop until the action has completed (or has been stopped for another reason). 1. To do this, replace the <code>client.wait_for_result()</code> line in the <code>concurrent_action_client.py</code> file with the following code:</p> <pre><code>rate = rospy.Rate(1)\ni = 1\nprint(\"While we're waiting, let's do our seven-times tables...\")\nwhile client.get_state() &lt; 2:\nprint(f\"STATE: Current state code is {client.get_state()}\")\nprint(f\"TIMES TABLES: {i} times 7 is {i*7}\")\ni += 1\nrate.sleep()\n</code></pre> </li> <li> <p>Run the <code>concurrent_action_client.py</code> node and see what happens this time.  Essentially, we know that we can carry on doing other things as long as the status code is less than 2 (either <code>PENDING</code> or <code>ACTIVE</code>), otherwise either our goal has been achieved, or something else has happened...</p> </li> </ol>"},{"location":"com2009/la1/week5/#preemptive_client","title":"Cancelling (or Preempting) an Action","text":"<p>Actions are extremely useful for controlling robotic tasks or processes that might take a while to complete, but what if something goes wrong, or if we just change our mind and want to stop an action before the goal has been reached? The ability to preempt an action is one of the things that makes them so useful.</p>"},{"location":"com2009/la1/week5/#ex3","title":"Exercise 3: Building a Preemptive Python Action Client Node","text":"<ol> <li>In TERMINAL 1 you should still be located within the <code>src</code> folder of your <code>week5_actions</code> package. If not, then go back there now! Create a new file called <code>preemptive_action_client.py</code> and make this executable.</li> <li> <p>Have a look at the code here, then copy and paste it into the <code>preemptive_action_client.py</code> node that you have just created.</p> <p>Here, we've built an action client that will cancel the call to the action server if we enter <code>Ctrl+C</code> into the terminal.  This is useful, because otherwise the action server would continue to run, even when we terminate the client.  A lot of the code is similar to the Action Client from the previous exercise, but we've built a class structure around this now for more flexibility.  Have a look at the code annotations and make sure that you understand how it all works.</p> </li> <li> <p>Run this using <code>rosrun</code>, let the server take a couple of images and then enter <code>Ctrl+C</code> to observe the goal cancelling in action.</p> <p>Note</p> <p>You'll need to set some values for the goal parameters again!</p> </li> <li> <p>We can also cancel a goal conditionally, which may also be useful if, say, too much time has elapsed since the call was made, or the caller has been made aware of something else that has happened in the meantime (perhaps we're running out of storage space on the robot and can't save any more images!) This is all achieved using the <code>cancel_goal()</code> method.</p> <ul> <li>Have a go now at introducing a conditional call to the <code>cancel_goal()</code> method once a total of 5 images have been captured.</li> <li>You could use the <code>captured_images</code> attribute from the <code>CameraSweepFeedback</code> message to trigger this.</li> </ul> </li> </ol>"},{"location":"com2009/la1/week5/#a-summary-of-ros-actions","title":"A Summary of ROS Actions","text":"<p>ROS Actions work a lot like ROS Services, but they have the following key differences:</p> <ol> <li>They are asynchronous: a client can do other things while it waits for an action to complete.</li> <li>They can be cancelled (or preempted): If something is taking too long, or if something else has happened, then an Action Client can cancel an Action whenever it needs to.</li> <li>They provide feedback: so that a client can monitor what is happening and act accordingly (i.e. preempt an action, if necessary).</li> </ol> <p></p> <p>This mechanism is therefore useful for robotic operations that may take a long time to execute, or where intervention might be necessary.</p>"},{"location":"com2009/la1/week5/#cam_swp_act_srv","title":"Creating Action Servers in Python","text":"<p>Important</p> <p>Cancel all active processes that you may have running before moving on.</p> <p>So far we have looked at how to call an action server, but what about if we actually want to set up our own? We've been working with a pre-made action server in the previous exercises, but so far we haven't really considered how it actually works. First, let's do some detective work... We launched the Action Server using <code>roslaunch</code> in Exercise 1:</p> <pre><code>roslaunch tuos_ros_examples camera_sweep.launch\n</code></pre> <p>Questions</p> <ul> <li>What does this tell us about the package that the action server node belongs to?</li> <li>Where, in the package directory, is this node likely to be located?</li> <li>How might we find out the name of the Python node from the <code>camera_sweep.launch</code> file?</li> </ul> <p>Once you've identified the name and the location of the source code, open it up in VS Code and have a look through it to see how it all works.</p> <p>Don't worry too much about all the content associated with obtaining and manipulating camera images in there, we'll learn more about this in the next session. Instead, focus on the general overall structure of the code and the way that the action server is implemented.</p> <ol> <li> <p>As a starting point, consider the way in which the action server is initialised and the way a callback function is defined to encapsulate all the code that will be executed when the action is called:</p> <pre><code>self.actionserver = actionlib.SimpleActionServer(\"/camera_sweep_action_server\", \nCameraSweepAction, self.action_server_launcher, auto_start=False)\nself.actionserver.start()\n</code></pre> </li> <li> <p>Look at how a <code>/cmd_vel</code> publisher and an <code>/odom</code> subscriber are defined in external classes:</p> <pre><code>self.robot_controller = Tb3Move()\nself.robot_odom = Tb3Odometry()\n</code></pre> <p>These are imported (at the start of the code) from an external <code>tb3.py</code> module that also lives in the same directory as the action server itself:</p> <pre><code>from tb3 import Tb3Move, Tb3Odometry\n</code></pre> <p>We do this to simplify the process of obtaining odometry data and controlling the robot, whilst keeping the actual action server code itself more concise. Have a look at the <code>tb3.py</code> module to discover exactly how these Python classes work.</p> </li> <li> <p>Look inside the action server callback function to see how the camera sweep operation is performed once the action has been called:</p> <pre><code>def action_server_launcher(self, goal):\n...\n</code></pre> <ol> <li> <p>Consider the error checking that is performed on the <code>goal</code> input variables, and how the call to the action server is aborted should any of these goal requests be invalid:</p> <pre><code>success = True\nif goal.sweep_angle &lt;= 0 or goal.sweep_angle &gt; 180:\nprint(\"Invalid sweep_angle! Select a value between 1 and 180 degrees.\")\nsuccess = False\n...\nif not success:\nself.result.image_path = \"None [ABORTED]\"\nself.actionserver.set_aborted(self.result)\nreturn\n</code></pre> </li> <li> <p>Consider how preemption is implemented in the server, and how the Action is stopped on receipt of a preempt request:</p> <pre><code>if self.actionserver.is_preempt_requested():\n...\n</code></pre> </li> <li> <p>Also have a look at the way a <code>feedback</code> message is constructed and published by the server:</p> <pre><code>self.feedback.current_image = i\nself.feedback.current_angle = abs(self.robot_odom.yaw)\nself.actionserver.publish_feedback(self.feedback)\n</code></pre> </li> <li> <p>Finally, consider how we tell the server that the action has been completed successfully, how the <code>result</code> message is published to the caller, and how we make the robot stop moving:</p> <pre><code>if success:\nrospy.loginfo(\"Camera sweep completed successfully.\")\nself.actionserver.set_succeeded(self.result)\nself.robot_controller.stop()\n</code></pre> </li> </ol> </li> </ol>"},{"location":"com2009/la1/week5/#ex4","title":"Exercise 4: Developing an \"Obstacle Avoidance\" behaviour using an Action Server","text":"<p>Knowing what you now do about ROS Actions, do you think the Service Server/Client systems that we developed last week were actually appropriate use cases for ROS Services?  Probably not!  In fact, Action Server/Client methods would have probably been more appropriate! </p> <p>You are now going to construct your own Action Server and Client nodes to implement a more effective obstacle avoidance behaviour that could form the basis of an effective search strategy. For this, you're going to need to build your own Search Server and Client.</p> <p>Step 1: Launch a simulation</p> <p>There's a simulation environment that you can use as you're developing your action server/client nodes for this exercise. Launch the simulation in TERMINAL 1, with the following <code>roslaunch</code> command: </p> <p>TERMINAL 1: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_stage_4.launch\n</code></pre></p> <p>Step 2: Build the Action Server</p> <ol> <li> <p>In TERMINAL 2 navigate to the <code>src</code> folder of your <code>week5_actions</code> package, create a Python script called <code>search_server.py</code>, and make it executable.</p> </li> <li> <p>The job of the Action Server node is as follows:</p> <ul> <li>The action server should make the robot move forwards until it detects an obstacle up ahead.</li> <li>Similarly to the Service Server that you created last week, your Action Server here should be configured to accept two goal parameters:<ol> <li>The speed (in m/s) at which the robot should move forwards when the action server is called. Consider doing some error checking on this to make sure a velocity request is less than the maximum speed that the robot can actually achieve (0.26 m/s)!</li> <li>The distance (in meters) at which the robot should stop ahead of any objects or boundary walls that are in front of it. To do this you'll need to subscribe to the <code>/scan</code> topic. Be aware that an object won't necessarily be directly in front of the robot, so you may need to monitor a range of <code>LaserScan</code> data points (within the <code>ranges</code> array) to make the collision avoidance effective (recall the LaserScan callback example and also have a look at the <code>Tb3LaserScan</code> class within the <code>tuos_ros_examples/tb3.py</code> module that might help you with this).</li> </ol> </li> <li> <p>Whilst your server performs its task it should provide the following feedback to the Action Caller:</p> <ol> <li> <p>The distance travelled (in meters) since the current action was initiated.</p> <p>To do this you'll need to subscribe to the <code>/odom</code> topic. Remember that there's a <code>Tb3Odometry</code> class within the <code>tuos_ros_examples/tb3.py</code> module that might help you with obtaining this data.</p> <p>Remember also that your robot's orientation shouldn't change over the course of a single action call, only its <code>linear.x</code> and <code>linear.y</code> positions should vary.  Bear in mind however that the robot won't necessarily be moving along the <code>X</code> or <code>Y</code> axis, so you will need to consider the total distance travelled in the <code>X-Y</code> plane.  You should have done this in the Week 3 <code>move_square</code> exercise, so refer to this if you need a reminder.</p> </li> </ol> </li> <li> <p>Finally, on completion of the action, your server should provide the following three result parameters:</p> <ol> <li>The total distance travelled (in meters) over the course of the action.</li> <li>The distance to the obstacle that made the robot stop (this should match, or very close to, the distance that was provided by the Action Client in the goal).</li> <li>The angle (in degrees) at which this obstacle is located in front of the robot (<code>Tb3LaserScan</code> class within the <code>tuos_ros_examples/tb3.py</code> module, which may already provide this).</li> </ol> </li> </ul> </li> <li> <p>An action message has been created for you to use for this exercise: <code>tuos_ros_msgs/Search.action</code>.  Navigate to the <code>action</code> folder of the <code>tuos_ros_msgs</code> package directory (or use <code>rosmsg info ...</code> in the terminal) to find out everything you need to know about this action message in order to develop your Action Server (and Client) nodes appropriately.</p> </li> <li> <p>We've put together some template code to help you with this. For further guidance though, you should also refer to the code for <code>/camera_sweep_action_server</code> node, which we talked about earlier: a lot of the techniques used by <code>/camera_sweep_action_server</code> node will be similar to what you'll need to do in this exercise. </p> </li> <li> <p>Whenever you're ready you can launch your action server from TERMINAL 2, using <code>rosrun</code>, as below:</p> <p>TERMINAL 2: <pre><code>rosrun week5_actions search_server.py\n</code></pre></p> </li> </ol> <p>Step 3: Build the Action Client</p> <ol> <li> <p>In TERMINAL 3 navigate to the <code>src</code> folder of your <code>week5_actions</code> package, create a Python script called <code>search_client.py</code>, and make it executable.</p> </li> <li> <p>The job of the Action Client node is as follows:</p> <ul> <li>The client needs to issue a correctly formatted goal to the server.</li> <li>The client should be programmed to monitor the feedback data from the Server.  If it detects (from the feedback) that the robot has travelled a distance greater than 2 meters without detecting an obstacle, then it should cancel the current action call using the <code>cancel_goal()</code> <code>actionlib</code> method.</li> </ul> </li> <li> <p>Use the techniques that we used in the Client node from Exercise 3 as a guide to help you with this. There's also a code template here to help you get started. </p> </li> <li> <p>Once you have everything in place launch the action client with <code>rosrun</code> as below:</p> <p>TERMINAL 3: <pre><code>rosrun week5_actions action_client.py\n</code></pre></p> <p>If all is good, then this client node should call the action server, which will - in turn - make the robot move forwards until it reaches a certain distance from an obstacle up ahead, at which point the robot will stop, and your client node will stop too. Once this happens, reorient your robot (using the <code>turtlebot3_teleop</code> node) and launch the client node again to make sure that it is robustly stopping in front of obstacles repeatedly, and when approaching them from a range of different angles. </p> <p>Important</p> <p>Make sure that your preemption functionality works correctly too, so that the robot never moves any further than 2 meters during a given action call!</p> </li> </ol>"},{"location":"com2009/la1/week5/#advanced","title":"Some advanced exercises (if you're feeling adventurous!)","text":"<p>Want to do more with the ROS skills that you have now developed?! Consider the following advanced exercises that you could try out now that you know how to use ROS Actions!</p> <p>Note</p> <p>We know that you have done a lot this week already, and these are really just suggestions for more advanced things that you may want to explore in your own time, or to help with the further work that you will do in Lab Assignment #2...</p>"},{"location":"com2009/la1/week5/#adv_ex1","title":"Advanced Exercise 1: Implementing a Search strategy","text":"<p>What you developed in the previous exercise could be used as the basis for an effective robot search strategy.  Up to now, your Action Client node should have the capability to call your <code>Search.action</code> server to make the robot move forwards by 2 meters, or until it reaches an obstacle (whichever occurs first), but you could enhance this further:</p> <ul> <li>Between action calls, your client node could make the robot turn on the spot to face a different direction and then issue a further action call to make the robot move forwards once again.</li> <li>The turning process could be done at random, or it could be informed by the result of the last action call, i.e.: if (on completion) the server has informed the client that it detected an object at an angle of, say, 10\u00b0 anti-clockwise from the front of the robot, then the client might then decide to turn the robot clockwise in an attempt to turn away from the object before issuing its next action call to make the robot move forwards again.</li> <li> <p>By programming your client node to repeat this process over and over again, the robot would (somewhat randomly) travel around its environment safely, stopping before it crashes into any obstacles and reorienting itself every time it stops moving forwards. This is effectively an implementation of a basic robotic search strategy! </p> <p>Enhancing this further...</p> <p>Imagine SLAM was running at the same time too... your robot could be building up a map of its environment in the background as it slowly explored every part of it!</p> </li> </ul>"},{"location":"com2009/la1/week5/#adv_ex2","title":"Advanced Exercise 2: Autonomous Navigation using waypoint markers","text":"<p>In the Week 3 session you used SLAM to construct a map of an environment (Exercise 3) and then issued navigation requests to the <code>move_base</code> action server, via the command-line, (Exercise 4) to make your robot move to a zone marker, based on coordinates that you had established beforehand. Now that you know how to build Action Client Nodes in Python you could return to your <code>week2_navigation</code> package and build a new node that makes the robot move sequentially between each zone marker programmatically.</p> <ul> <li>Your node could cycle through the coordinates of all four of the zone markers (or \"waypoints\") that you established whilst using SLAM to build a map of the environment (as per Exercise 3).</li> <li>Your node could monitor the status of the <code>move_base_simple</code> action call to know when the robot has reached a zone marker, so that it knows when to issue a further action call to move on to the next one.</li> <li>You could refer to the launch file that you created in the Week 3 session to launch all the Navigation processes that need to be running in order to enable and configure the ROS Navigation Stack appropriately for the TurtleBot3 robot.</li> </ul>"},{"location":"com2009/la1/week5/#wrapping-up","title":"Wrapping Up","text":"<p>In this session you've learnt:</p> <ul> <li>How ROS Actions work and why they might be useful.</li> <li>How to develop Action Client Nodes in Python which can perform other tasks concurrently to the action they have requested, and which can also cancel the requested action, if required.</li> <li>How to use standard ROS tools to interrogate the topic messages used by an action server, allowing you to build clients to call them, and to also allow you to build standalone action servers yourself using bespoke Action messages.</li> <li>How to harness this communication method to implement a behaviour that could be used as the basis for a genuine robotic search strategy. </li> </ul>"},{"location":"com2009/la1/week5/#topics-services-or-actions-which-to-choose","title":"Topics, Services or Actions: Which to Choose?","text":"<p>You should now have developed a good understanding of the three communication methods that are available within ROS to facilitate communication between ROS Nodes:</p> <ol> <li>Topic-based messaging.</li> <li>ROS Services.</li> <li>ROS Actions.</li> </ol> <p>Through this course you've now gained some practical experience using all three of these, but you may still be wondering how to select the appropriate one for a certain robot task... </p> <p>This ROS.org webpage summarises all of this very nicely (and briefly), so you should have a read through this to make sure you know what's what. In summary though:</p> <ul> <li>Topics: Are most appropriate for broadcasting continuous data-streams such as sensor data and robot state information, and for publishing data that is likely to be required by a range of Nodes across a ROS network.</li> <li>Services: Are most appropriate for very short procedures like quick calculations (inverse kinematics etc.) and performing short discrete actions that are unlikely to go wrong or will not need intervention (e.g. turning on a warning LED when a battery is low).</li> <li>Actions: Are most appropriate for longer running tasks (like moving a robot), for longer processing calculations (processing the data from a camera stream) or for operations where we might need to change our mind and do something different or cancel an invoked behaviour part way through.</li> </ul>"},{"location":"com2009/la1/week5/#backup","title":"Saving your work","text":"<p>Remember, the work you have done in the WSL-ROS environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University U: Drive, allowing you to restore it at the start of the next session.</p> <ol> <li> <p>Remember: you can also use the <code>wsl_ros restore</code> command, to restore your work at any time.\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/la1/week6/","title":"Week 6: Cameras, Machine Vision & OpenCV","text":"<p>Info</p> <p>You should be able to complete all the exercises on this page within a two-hour lab session.</p>"},{"location":"com2009/la1/week6/#introduction","title":"Introduction","text":""},{"location":"com2009/la1/week6/#aims","title":"Aims","text":"<p>This week we are finally going to make use of our TurtleBot3's camera, and look at how to work with images in ROS! Here we'll look at how to build ROS nodes that capture images and process them. We'll explore some ways in which this data can be used to inform decision-making in robotics applications.  </p>"},{"location":"com2009/la1/week6/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Use a range of ROS tools to interrogate camera image topics on a ROS Network and view the images being streamed to them.</li> <li>Use the computer vision library OpenCV with ROS, to obtain camera images and process them in real-time.  </li> <li>Apply filtering processes to isolate objects of interest within an image.</li> <li>Develop object detection nodes and harness the information generated by these processes to control a robot's position.</li> <li>Use camera data as a feedback signal to implement a line following behaviour using proportional control.</li> </ol>"},{"location":"com2009/la1/week6/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Using the <code>rqt_image_view</code> node whilst changing the robot's viewpoint</li> <li>Exercise 2: Object Detection</li> <li>Exercise 3: Locating image features using Image Moments</li> <li>Exercise 4: Line following</li> </ul>"},{"location":"com2009/la1/week6/#additional-resources","title":"Additional Resources","text":"<ul> <li>The Initial Object Detection Code (for Exercise 2)</li> <li>A Complete Worked Example of the <code>object_detection.py</code> Node</li> <li>A <code>line_follower</code> Template (for Exercise 4)</li> </ul>"},{"location":"com2009/la1/week6/#getting-started","title":"Getting Started","text":"<p>Step 1: Launch WSL-ROS Launch your WSL-ROS environment by running the WSL-ROS shortcut in the Windows Start Menu (if you haven't already done so). Once installed, the Windows Terminal app should launch with an Ubuntu terminal instance ready to go (TERMINAL 1).</p> <p>Step 2: Restore your work When prompted (in TERMINAL 1), enter <code>Y</code> to restore your work from last time1.</p> <p>Step 3: Launch VS Code Follow these steps to launch VS Code correctly within the WSL-ROS environment.</p> <p>Step 4: Launch the Robot Simulation In this session we'll start by working with the mystery world environment from last week. In TERMINAL 1, use the following <code>roslaunch</code> command to load it:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_ros_simulations coloured_pillars.launch\n</code></pre> ...and then wait for the Gazebo window to open:</p> <p></p>"},{"location":"com2009/la1/week6/#working-with-cameras-and-images-in-ros","title":"Working with Cameras and Images in ROS","text":""},{"location":"com2009/la1/week6/#camera-topics-and-data","title":"Camera Topics and Data","text":"<p>There are a number of tools that we can use to view the live images that are being captured by a robot's camera in ROS. As with all robot data, these streams are published to topics, so we firstly need to identify those topics.</p> <p>In a new terminal instance (TERMINAL 2), run <code>rostopic list</code> to see the full list of topics that are currently active on our system. Conveniently, all the topics related to our robot's camera are prefixed with <code>camera</code>! Filter the <code>rostopic list</code> output using <code>grep</code> (a Linux command), to filter the list and only show topics prefixed with <code>/camera</code>:</p> <p>TERMINAL 2: <pre><code>rostopic list | grep /camera\n</code></pre></p> <p>This should provide the following filtered list:</p> <pre><code>/camera/depth/camera_info\n/camera/depth/image_raw\n/camera/depth/points\n/camera/parameter_descriptions\n/camera/parameter_updates\n/camera/rgb/camera_info\n/camera/rgb/image_raw\n/camera/rgb/image_raw/compressed\n/camera/rgb/image_raw/compressed/parameter_descriptions\n/camera/rgb/image_raw/compressed/parameter_updates\n/camera/rgb/image_raw/compressedDepth\n/camera/rgb/image_raw/compressedDepth/parameter_descriptions\n/camera/rgb/image_raw/compressedDepth/parameter_updates\n/camera/rgb/image_raw/theora\n/camera/rgb/image_raw/theora/parameter_descriptions\n/camera/rgb/image_raw/theora/parameter_updates\n</code></pre> <p>The real TurtleBot3 Waffles that we have at the university have a slightly different camera module to that used by the simulated robots that we are working with here.  Despite this though, the camera data on our real robots is published to topics using the same ROS message formats as used in simulation, making it fairly straight-forward to transfer nodes that we develop in simulation here onto the real robots, when the time comes2.</p> <p>The first items in the list of camera topics above tell us that depth information is available here. Much like the real robots, the simulated versions that we are working with here also have a camera module capable of determining depth information as well as simply capturing images.  Remember from Week 3 though, that we also have a very capable LiDAR sensor to give us this type of information too, and so we won't really be using the depth capabilities of our camera in this session.</p> <p>The main thing we are actually interested in here is the RGB images that are captured by the camera, and the key topic that we'll therefore be using this week is:</p> <pre><code>/camera/rgb/image_raw\n</code></pre> <p>Run <code>rostopic info</code> on this to identify the message type.</p> <p>Now, run <code>rosmsg info</code> on this message type to find out exactly what information is published to the topic.  You should end up with a list that looks like this:</p> <pre><code>std_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\nuint32 height\nuint32 width\nstring encoding\nuint8 is_bigendian\nuint32 step\nuint8[] data\n</code></pre> <p></p> <p>Questions</p> <ol> <li>What type of message is used on this topic, and which package is this message derived from?</li> <li>Using <code>rostopic echo</code> and the information about the topic message (as shown above) determine the size of the images that our robot's camera will capture (i.e. its dimensions, in pixels).  It will be quite important to know this when we start manipulating these camera images later on. </li> <li>Finally, considering the list above again, which part of the message do you think contains the actual image data?</li> </ol>"},{"location":"com2009/la1/week6/#viz","title":"Visualising Camera Streams","text":"<p>We can view the images being streamed to the above camera topic (in real-time) in a variety of different ways, and we'll explore a couple of these now.</p> <p>One way is to use RViz, which can be launched using the following <code>roslaunch</code> command:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_gazebo turtlebot3_gazebo_rviz.launch\n</code></pre></p> <p>Once RViz launches, find the camera item in the left-hand panel and tick the checkbox next to it. This should open up a camera panel with a live stream of the images being obtained from the robot's camera!  The nice thing about this is that the real-time LiDAR data will also be overlaid on top of the images too!</p> <p></p> <p>Close down RViz by entering <code>Ctrl+C</code> in TERMINAL 2.  </p>"},{"location":"com2009/la1/week6/#ex1","title":"Exercise 1: Using the <code>rqt_image_view</code> node whilst changing the robot's viewpoint","text":"<p>Another tool we can use to view camera data-streams is the <code>rqt_image_view</code> node.</p> <ol> <li> <p>To launch this, use <code>rosrun</code> as follows:</p> <p>TERMINAL 2: <pre><code>rosrun rqt_image_view rqt_image_view\n</code></pre> </p> <p>This is a nice tool that allows us to easily view images that are being published to any camera topic on the ROS network. Another useful feature is the ability to save these images (as <code>.jpg</code> files) to the filesystem: See the \"Save as image\" button highlighted in the figure above.  This might be useful later on in this week's session.</p> </li> <li> <p>Click the drop-down box in the top left of the window to select an image topic to display.  Select <code>/camera/rgb/image_raw</code> (if it's not already selected).</p> </li> <li> <p>Keep this window open now, and launch a new terminal instance (TERMINAL 3).</p> </li> <li> <p>Launch the <code>turtlebot3_teleop</code> node, either using the full command or a handy alias: <code>tb3_teleop</code>! Rotate your robot on the spot, keeping an eye on the <code>rqt_image_view</code> window as you do this.  Stop the robot once one of the coloured pillars in the arena is roughly in the centre of the robot's field of vision, then close the <code>turtlebot3_teleop</code> node and the <code>rqt_image_view</code> node by entering <code>Ctrl+C</code> in TERMINAL 3 and TERMINAL 2 respectively.</p> </li> </ol>"},{"location":"com2009/la1/week6/#opencv","title":"OpenCV and ROS","text":"<p>OpenCV is a mature and powerful computer vision library designed for performing real-time image analysis, and it is therefore extremely useful for robotic applications.  The library is cross-platform and there is a Python API (<code>cv2</code>), which we'll be using to do some computer vision tasks of our own during this lab session. While we can work with OpenCV using Python straight away (via the API), the library can't directly interpret the native image format used by the ROS, so there is an interface that we need to use.  The interface is called CvBridge, which is a ROS package that handles the conversion between ROS and OpenCV image formats.  We'll therefore need to use these two libraries (OpenCV and CvBridge) hand-in-hand when developing ROS nodes to perform computer vision related tasks.</p>"},{"location":"com2009/la1/week6/#ex2","title":"Exercise 2: Object Detection","text":"<p>One common job that we often want a robot to perform is object detection, and we will illustrate how this can be achieved by colour filtering to detect the coloured pillar that your robot should now be looking at.  In this exercise you will learn how to use OpenCV to capture images, filter them and perform other analysis to confirm the presence and location of features that we might be interested in.</p> <p>Step 1</p> <ol> <li>First create a new package in your <code>catkin_ws/src</code> directory called <code>week6_vision</code> with <code>rospy</code>, <code>cv_bridge</code>, <code>sensor_msgs</code> and <code>geometry_msgs</code> as dependencies.</li> <li>Then, run <code>catkin build</code> on the package and then re-source your environment (as you've done so many times by now!)</li> <li>In the <code>src</code> folder of the package you have just created, create a new Python file called <code>object_detection.py</code>. What else do we need to do to this file before we can run it? Do it now!</li> <li>Copy the code here, save the file, then read the annotations so that you understand how this node works and what should happen when you run it. </li> <li> <p>Run the node using <code>rosrun</code>.</p> <p>Warning</p> <p>This node will capture an image and display it in a pop-up window. Once you've viewed the image in this pop-up window MAKE SURE YOU CLOSE THE POP-UP WINDOW DOWN so that the node can complete its execution!</p> </li> <li> <p>As you should know from reading the explainer, the node has just obtained an image and saved it to a location on the filesystem.  Navigate to this filesystem location and view the image using <code>eog</code>.</p> </li> </ol> <p>What you may have noticed from the terminal output when you ran the <code>object_detection.py</code> node is that the robot's camera captures images at a native size of 1080x1920 pixels (you should already know this from interrogating the <code>/camera/rgb/image_raw/width</code> and <code>/height</code> messages using <code>rostopic echo</code> earlier, right?!).  That's over 2 million pixels in total in a single image (2,073,600 pixels per image, to be exact), each pixel having a blue, green and red value associated with it - so that's a lot of data in a single image file! </p> <p>Question</p> <p>The size of the image file (in bytes) was actually printed to the terminal when you ran the <code>object_detection.py</code> node. Did you notice how big it was exactly?</p> <p>Processing an image of this size is therefore hard work for a robot: any analysis we do will be slow and any raw images that we capture will occupy a considerable amount of storage space. The next step then is to reduce this down by cropping the image to a more manageable size.</p> <p>Step 2</p> <p>We're going to modify the <code>object_detection.py</code> node now to:</p> <ul> <li>Capture a new image in its native size</li> <li>Crop it down to focus in on a particular area of interest</li> <li> <p>Save both of the images (the cropped one should be much smaller than the original).</p> </li> <li> <p>In your <code>object_detection.py</code> node locate the line:</p> <pre><code>show_and_save_image(cv_img, img_name = \"step1_original\")\n</code></pre> </li> <li> <p>Underneath this, add the following additional lines of code:</p> <pre><code>crop_width = width - 400\ncrop_height = 400\ncrop_y0 = int((width / 2) - (crop_width / 2))\ncrop_z0 = int((height / 2) - (crop_height / 2))\ncropped_img = cv_img[crop_z0:crop_z0+crop_height, crop_y0:crop_y0+crop_width]\nshow_and_save_image(cropped_img, img_name = \"step2_cropping\")\n</code></pre> </li> <li> <p>Run the node again.  </p> <p>Remember</p> <p>Make sure you close all of these pop-up windows down after viewing them to ensure that all your images are saved to the filesystem and the node completes all of its tasks successfully.</p> <p>The code that you have just added here has created a new image object called <code>cropped_img</code>, from a subset of the original by specifying a desired <code>crop_height</code> and <code>crop_width</code> relative to the original image dimensions.  Additionally, we have also specified where in the original image (in terms of pixel coordinates) we want this subset to start, using <code>crop_y0</code> and <code>crop_z0</code>. This process is illustrated in the figure below:</p> <p> </p> <p>The original image (<code>cv_img</code>) is cropped using a process called \"slicing\":</p> <p><pre><code>cropped_img = cv_img[\ncrop_z0:crop_z0+crop_height,\ncrop_y0:crop_y0+crop_width\n]\n</code></pre> This may seem quite confusing, but hopefully the figure below illustrates what's going on here:</p> <p> </p> </li> </ul> <p>Step 3</p> <p>As discussed above, an image is essentially a series of pixels each with a blue, green and red value associated with it to represent the actual image colours.  From the original image that we have just obtained and cropped, we then want to get rid of any colours other than those associated with the pillar that we want the robot to detect.  We therefore need to apply a filter to the pixels, which we will ultimately use to discard any pixel data that isn't related to the coloured pillar, whilst retaining data that is.  </p> <p>This process is called masking and, to achieve this, we need to set some colour thresholds. This can be difficult to do in a standard Blue-Green-Red (BGR) or Red-Green-Blue (RGB) colour space, and you can see a good example of this in this article from RealPython.com.  We will apply some steps discussed in this article to convert our cropped image into a Hue-Saturation-Value (HSV) colour space instead, which makes the process of colour masking a bit easier.</p> <ol> <li> <p>First, analyse the Hue and Saturation values of the cropped image. To do this, first navigate to the \"myrosdata/week6_images\" directory, where the raw image has been saved:</p> <p>TERMINAL 2: <pre><code>cd ~/myrosdata/week6_images\n</code></pre></p> <p>Then, run the following ROS Node (from the <code>tuos_ros_examples</code> package), supplying the name of the cropped image as an additional argument:</p> <pre><code>rosrun tuos_ros_examples image_colours.py step2_cropping.jpg\n</code></pre> </li> <li> <p>The node should produce a scatter plot, illustrating the Hue and Saturation values of each of the pixels in the image. Each data point in the plot represents a single image pixel and each is coloured to match its RGB value:</p> <p> </p> </li> <li> <p>You should see from the image that all the pixels related to the coloured pillar that we want to detect are clustered together.  We can use this information to specify a range of Hue and Saturation values that can be used to mask our image: filtering out any colours that sit outside this range and thus allowing us to isolate the pillar itself. The pixels also have a Value (or \"Brightness\"), which isn't shown in this plot. As a rule of thumb, a range of brightness values between 100 and 255 generally works quite well.</p> <p> </p> <p>In this case then, we select upper and lower HSV thresholds as follows:</p> <pre><code>lower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\n</code></pre> <p>Use the plot that has been generated here to determine your own upper and lower thresholds. </p> <p>OpenCV contains a built-in function to detect which pixels of an image fall within a specified HSV range: <code>cv2.inRange()</code>.  This outputs a matrix, the same size and shape as the number of pixels in the image, but containing only <code>True</code> (<code>1</code>) or <code>False</code> (<code>0</code>) values, illustrating which pixels do have a value within the specified range and which don't.  This is known as a Boolean Mask (essentially, a series of ones or zeroes).  We can then apply this mask to the image, using a Bitwise AND operation, to get rid of any image pixels whose mask value is <code>False</code> and keep any flagged as <code>True</code> (or in range).</p> </li> <li> <p>To do this, first locate the following line in your <code>object_detection.py</code> node:</p> <pre><code>show_and_save_image(cropped_img, img_name = \"step2_cropping\")\n</code></pre> </li> <li> <p>Underneath this, add the following:</p> <pre><code>hsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\nlower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\nimg_mask = cv2.inRange(hsv_img, lower_threshold, upper_threshold)\nshow_and_save_image(img_mask, img_name = \"step3_image_mask\")\n</code></pre> </li> <li> <p>Now, run the node again. Three images should be generated and saved now.  As shown in the figure below, the third image should simply be a black and white representation of the cropped image, where the white regions should indicate the areas of the image where pixel values fall within the HSV range specified earlier.  Notice (from the text printed to the terminal) that the cropped image and the image mask have the same dimensions, but the image mask file has a significantly smaller file size.  While the mask contains the same number of pixels, these pixels only have a value of <code>1</code> or <code>0</code>, whereas - in the cropped image of the same pixel size - each pixel has a Red, Green and Blue value: each ranging between <code>0</code> and <code>255</code>, which represents significantly more data.</p> <p> </p> </li> </ol> <p>Step 4 </p> <p>Finally, we can apply this mask to the cropped image, generating a final version of it where only pixels marked as <code>True</code> in the mask retain their RGB values, and the rest are simply removed.  As discussed earlier, we use a Bitwise AND operation to do this and, once again, OpenCV has a built-in function to do this: <code>cv2.bitwise_and()</code>.</p> <ol> <li> <p>Locate the following line in your <code>object_detection.py</code> node:</p> <pre><code>show_and_save_image(img_mask, img_name = \"step3_image_mask\")\n</code></pre> </li> <li> <p>And, underneath this, add the following:</p> <pre><code>filtered_img = cv2.bitwise_and(cropped_img, cropped_img, mask = img_mask)\nshow_and_save_image(filtered_img, img_name = \"step4_filtered_image\")\n</code></pre> </li> <li> <p>Run this node again, and a fourth image should also be generated now, this time showing the cropped image taken from the robot's camera, but only containing data related to coloured pillar, with all other background image data removed (and rendered black):</p> <p> </p> </li> </ol>"},{"location":"com2009/la1/week6/#image-moments","title":"Image Moments","text":"<p>You have now successfully isolated an object of interest within your robot's field of vision, but perhaps we want to make our robot move towards it, or - conversely - make our robot navigate around it and avoid crashing into it!  We therefore also need to know the position of the object in relation to the robot's viewpoint, and we can do this using image moments.</p> <p>The work we have just done above led to us obtaining what is referred to as a colour blob.  OpenCV also has built-in tools to allow us to calculate the centroid of a colour blob like this, allowing us to determine where exactly within an image the object of interest is located (in terms of pixels).  This is done using the principle of image moments: essentially statistical parameters related to an image, telling us how a collection of pixels (i.e. the blob of colour that we have just isolated) are distributed within it.  You can read more about Image Moments here and - from this - we can learn that the central coordinates of a colour blob can be obtained by considering some key moments of the image mask that we obtained from thresholding earlier:</p> <ul> <li><code>M00</code>: the sum of all non-zero pixels in the image mask (i.e. the size of the colour blob, in pixels)</li> <li><code>M10</code>: the sum of all the non-zero pixels in the horizontal (y) axis, weighted by row number</li> <li><code>M01</code>: the sum of all the non-zero pixels in the vertical (z) axis, weighted by column number</li> </ul> <p>Remember</p> <p>We refer to the horizontal as the y-axis and the vertical as the z-axis here, to match the terminology that we have used previously to define our robot's principal axes.</p> <p>We don't really need to worry about the derivation of these moments too much though.  OpenCV has a built-in <code>moments()</code> function that we can use to obtain this information from an image mask (such as the one that we generated earlier):</p> <pre><code>m = cv2.moments(img_mask)\n</code></pre> <p>So, using this we can obtain the <code>y</code> and <code>z</code> coordinates of the blob centroid quite simply:</p> <pre><code>cy = m['m10']/(m['m00']+1e-5)\ncz = m['m01']/(m['m00']+1e-5)\n</code></pre> <p>Notice, that we are adding a very small number to the <code>M00</code> moment here to make sure that the divisor in the above equations is never zero and thus ensuring that we never get caught out by any \"divide-by-zero\" errors. When might this be the case?</p> <p></p> <p>Once again, there is a built-in OpenCV tool that we can use to add a circle onto an image to illustrate the centroid location within the robot's viewpoint: <code>cv2.circle()</code>.  This is how we produced the red circle that you can see in the figure above.  You can see how this is implemented in a complete worked example of the <code>object_detection.py</code> node from the previous exercise. </p> <p>In our case, we can't actually change the position of our robot in the z axis, so the <code>cz</code> centroid component here might not be that important to us for navigation purposes.  We may however want to use the centroid coordinate <code>cy</code> to understand where a feature is located horizontally in our robot's field of vision, and use this information to turn towards it (or away from it, depending on what we are trying to achieve).  We'll look at this in a bit more detail now.</p>"},{"location":"com2009/la1/week6/#ex3","title":"Exercise 3: Locating image features using Image Moments","text":"<p>Inside the <code>tuos_ros_examples</code> package there is a node that has been developed to illustrate how all the OpenCV tools that you have explored so far could be used to search an environment and stop a robot when it is looking directly at an object of interest. All the tools that are used in this node should be familiar to you by now, and in this exercise you're going to make a copy of this node and modify it to enhance its functionality.</p> <ol> <li> <p>The node is called <code>colour_search.py</code>, and it is located in the <code>src</code> folder of the <code>tuos_ros_examples</code> package. Copy this into the <code>src</code> folder of your own <code>week6_vision</code> package by first ensuring that you are located in the desired destination folder:</p> <p>TERMINAL 2: <pre><code>roscd week6_vision/src\n</code></pre></p> </li> <li> <p>Then, copy the <code>colour_search.py</code> node using <code>cp</code> as follows:</p> <p>TERMINAL 2: <pre><code>cp ~/catkin_ws/src/COM2009/tuos_ros_examples/src/colour_search.py ./\n</code></pre></p> </li> <li> <p>In the same way as last week, you'll also need to copy the <code>tb3.py</code> module across from the <code>tuos_ros_examples</code> package too, as this is used by the <code>colour_search.py</code> node to make the robot move:</p> <p>TERMINAL 2: <pre><code>cp ~/catkin_ws/src/COM2009/tuos_ros_examples/src/tb3.py ./\n</code></pre></p> </li> <li> <p>Open up the <code>colour_search.py</code> file in VS Code to view the content.  Have a look through it and see if you can make sense of how it works.  The overall structure should be fairly familiar to you by now: we have a Python class structure, a Subscriber with a callback function, a main loop where all the robot control takes place and a lot of the OpenCV tools that you have explored so far in this session.  Essentially this node functions as follows:</p> <ol> <li>The robot turns on the spot whilst obtaining images from its camera (by subscribing to the <code>/camera/rgb/image_raw</code> topic).</li> <li>Camera images are obtained, cropped, then a threshold is applied to the cropped images to detect the blue pillar in the simulated environment.</li> <li>If the robot can't see a blue pillar then it turns on the spot quickly.</li> <li>Once detected, the centroid of the blue blob representing the pillar is calculated to obtain its current location in the robot's viewpoint.</li> <li>As soon as the blue pillar comes into view the robot starts to turn more slowly instead.</li> <li>The robot stops turning as soon as it determines that the pillar is situated directly in front of it (determined using the <code>cy</code> component of the blue blob centroid).</li> <li>The robot then waits for a while and then starts to turn again.</li> <li>The whole process repeats until it finds the blue pillar once again.</li> </ol> </li> <li>Run the node as it is to see this in action.  Observe the messages that are printed to the terminal throughout execution.</li> <li>Your task is to then modify the node so that it stops in front of every coloured pillar in the arena (there are four in total). For this, you may need to use some of the methods that we have explored so far this week.<ol> <li>You might first want to use some of the methods that we used to obtain and analyse some images from the robot's camera:<ol> <li>Use the <code>turtlebot3_teleop</code> node to manually move the robot, making it look at every coloured pillar in the arena individually.</li> <li>Run the <code>object_detection.py</code> node that you developed in the previous exercise to capture an image, crop it, save it to the filesystem and then feed this cropped image into the <code>image_colours.py</code> node from the <code>tuos_ros_examples</code> package (as you did earlier)</li> <li>From the plot that is generated by the <code>image_colours.py</code> node, determine some appropriate HSV thresholds to apply for each coloured pillar in the arena.</li> </ol> </li> <li>Once you have the right thresholds, then you can add these to your <code>colour_search.py</code> node so that it has the ability to detect every pillar in the same way that it currently detects the blue one.</li> </ol> </li> </ol>"},{"location":"com2009/la1/week6/#ex4","title":"Exercise 4: Line following","text":"<p>OK, time for something a bit more interesting now: line following!</p> <ol> <li>Make sure that your <code>colour_search.py</code> node is no longer running, and also close down the Gazebo simulation that is currently active by entering <code>Ctrl+C</code> in TERMINAL 1.</li> <li> <p>In TERMINAL 2 you should still be located in your <code>week6_vision/src</code> directory, but if not then go there now:</p> <p>TERMINAL 2: <pre><code>roscd week6_vision/src\n</code></pre></p> </li> <li> <p>Then, perform the necessary steps to create a new empty Python file called <code>line_follower.py</code> and prepare it for execution.</p> </li> <li>Once that's done open up the empty file in VS Code.</li> <li>Then, have a look at the template Python code for this exercise, and be sure to read the annotations too. </li> <li>Copy and paste this into your empty <code>line_follower.py</code> file and save it.</li> <li> <p>Now, in TERMINAL 1 run a new simulation:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_ros_simulations line_following.launch\n</code></pre></p> <p>Your robot should be launched into an environment with a coloured track painted on the floor:</p> <p> </p> </li> <li> <p>In TERMINAL 2, run the <code>line_follower.py</code> node as it is (using <code>rosrun</code>) and see what happens... Not very impressive eh?!  That's where you come in!  There are a few issues with this that you will need to address:</p> <ul> <li>Image Cropping: You might want to have a look at the cropping that is currently being performed to hone in on a particular region of the robot's viewpoint... does this look optimal?  Could it be improved?</li> <li>HSV Thresholds: Do these look appropriate for detecting the colour of the line? Determine some appropriate HSV thresholds to apply using methods that you have used earlier in this session.</li> <li>Velocity Limiting: The Node uses a proportional controller. Essentially, we specify a proportional gain (<code>kp</code>), and multiply this gain by a position error to obtain an angular velocity.  What happens if this calculation outputs an angular velocity that is excessive, or is larger than the maximum angular velocity that our robot can achieve?  Consider, therefore, doing some error checking on the <code>ang_vel</code> calculation output to ensure that any angular velocity commands are kept within some sensible limits before being published to the <code>/cmd_vel</code> topic.</li> <li>Tuning the Gain: PID gains need to be tuned in order to be appropriate for a particular system.  Make adjustments to the proportional gain parameter (<code>kp</code>) until a sensible response is achieved, and the robot follows the line effectively.</li> <li> <p>Conditions where the Line Can't be Seen: The angular velocity is determined by considering the <code>y</code> coordinate of a colour blob centroid.  What happens in situations where the blob of colour can't be seen in the robot's field of vision though?  What influence would this have on the velocity commands that are published to the <code>/cmd_vel</code> topic? Consider a situation where the robot can't see the line to begin with... Try launching the robot in the arena in a different location instead, and think about how you might approach this situation:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_ros_simulations line_following.launch x_pos:=3 y_pos:=-3 yaw:=0\n</code></pre></p> </li> <li> <p>Stopping: Finally, what happens when the robot reaches the finish line?  How could you add additional functionality to ensure that the robot stops when it reaches this point?  What features of the arena could you use to trigger this?</p> </li> </ul> </li> </ol>"},{"location":"com2009/la1/week6/#wrapping-up","title":"Wrapping Up","text":"<p>In this session you have learnt how to use data from a robot's camera to extract further information about its environment.  The camera allows our robot to \"see\" and the information that we obtain from this device can allow us to develop more advanced robotic behaviours such as searching for objects, follow things or - conversely - moving away or avoiding them.  You have learnt how to do some basic tasks with OpenCV, but this is a huge and very capable library of computer vision tools, and we encourage you to explore this further yourselves to enhance some of the basic principles that we have shown you today.</p>"},{"location":"com2009/la1/week6/#backup","title":"Saving your work","text":"<p>Remember, the work you have done in the WSL-ROS environment during this session will not be preserved for future sessions or across different University machines automatically! To save the work you have done here today you should now run the following script in any idle WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will export your home directory to your University U: Drive, allowing you to restore it again later.</p> <ol> <li> <p>You can also use the <code>wsl_ros restore</code> command.\u00a0\u21a9</p> </li> <li> <p>Camera topic names are slightly different on the real robots though, so look out for that!\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/la1/week1/publisher/","title":"Week 1 Publisher Node","text":""},{"location":"com2009/la1/week1/publisher/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>publisher.py</code> file and review the annotations to understand how it all works.</p> <p>Tip</p> <p>Don't forget the Shebang! See below for further details...</p> publisher.py<pre><code>#!/usr/bin/env python3\n# A simple ROS publisher node in Python\nimport rospy # (1)!\nfrom std_msgs.msg import String # (2)!\nclass Publisher(): # (3)!\ndef __init__(self): # (4)!\nself.node_name = \"simple_publisher\" # (5)!\ntopic_name = \"chatter\" # (6)!\nself.pub = rospy.Publisher(topic_name, String, queue_size=10) # (7)!\nrospy.init_node(self.node_name, anonymous=True) # (8)!\nself.rate = rospy.Rate(10) # (9)!\nself.ctrl_c = False # (10)!\nrospy.on_shutdown(self.shutdownhook) \nrospy.loginfo(f\"The '{self.node_name}' node is active...\") # (11)!\ndef shutdownhook(self): # (12)!\nprint(f\"Stopping the '{self.node_name}' node at: {rospy.get_time()}\")\nself.ctrl_c = True\ndef main_loop(self):\nwhile not self.ctrl_c: # (13)!\npublisher_message = f\"rospy time is: {rospy.get_time()}\"\nself.pub.publish(publisher_message)\nself.rate.sleep()\nif __name__ == '__main__': # (14)!\npublisher_instance = Publisher() # (15)!\ntry:\npublisher_instance.main_loop() # (16)!\nexcept rospy.ROSInterruptException:\npass\n</code></pre> <ol> <li> <p><code>rospy</code> is the Python client library for ROS, and we need to import this in order to create ROS Nodes in Python.</p> </li> <li> <p>We also need to import the <code>String</code> message type from the <code>std_msgs.msg</code> library for publishing our messages.</p> </li> <li> <p>We create a Python class called <code>Publisher()</code> to encapsulate all the functionality of our node.</p> </li> <li> <p>The <code>__init__()</code> method is called as soon as an instance of the <code>Publisher()</code> class is created.</p> </li> <li> <p>We define a name for this node and assign it to <code>self.node_name</code>. We can call the node anything that we want, but it's good to give it a meaningful name as this is the name that will be used to register the node on the ROS Network.</p> </li> <li> <p>We also define the name of a topic that we want to publish messages to (<code>\"chatter\"</code> in this case). If this is the name of a topic that already exists on the ROS Network, then we need to ensure that we use the correct message type, in order to be able to publish to it. In this case however, a topic called \"chatter\" shouldn't currently exist, so it will be created for us, and we can choose whatever type of message we want to use (a <code>String</code> message from the <code>std_msgs</code> library in our case).</p> </li> <li> <p>We then create an instance of a <code>rospy.Publisher()</code> object within our class: this creates the topic on the ROS Network (if it doesn't already exist). We therefore need to tell it the name of the topic that we want to create, and we also need to specify that we will be publishing <code>String</code> type messages.</p> </li> <li> <p>Then, we initialise our Python node, using the name that we defined earlier (<code>\"simple_publisher\"</code>), setting the <code>anonymous</code> flag to <code>True</code> to ensure that the node name is unique, by appending random numbers to it (we'll observe this when we run the node shortly).</p> </li> <li> <p>Then, we instantiate a <code>rospy.Rate()</code> object and set the frequency to 10 Hz, so that our publisher will publish messages at this frequency.</p> </li> <li> <p>This is used to shut down a ROS node effectively:</p> <ol> <li>First, we create a <code>ctrl_c</code> variable within the parent class and initialise it to <code>False</code>.</li> <li>Then, we use the <code>rospy.on_shutdown()</code> method to register a shutdown hook (in this case a function called <code>shutdownhook</code>). This will be called when rospy detects that the node has been asked to stop (i.e. by a user entering <code>Ctrl+C</code> in the terminal, for example). The shutdown hook function must take no arguments.</li> </ol> </li> <li> <p>Finally, we issue a message to indicate that our node is active (this will appear in the terminal that we run the node in):</p> </li> <li> <p>This method is called by the <code>rospy.on_shutdown()</code> method when the node is stopped. </p> <p>Here, we can include any important shutdown processes (making a robot stop moving, for instance). In this case, we just print a message to the terminal and then set the <code>ctrl_c</code> variable to <code>True</code>, which will stop the <code>main_loop()</code> method...</p> </li> <li> <p>The <code>while</code> loop here makes sure that the <code>main_loop()</code> runs continuously, until the node is shut down (via the <code>self.ctrl_c</code> flag):</p> <p>Inside the <code>while</code> loop we create a publisher message (a simple string in this case), publish it using the <code>pub</code> object we created in the initialisation stage, and then use the <code>rate</code> object (also created earlier) to then make the node \"sleep\" for as long as required to satisfy the frequency that we defined earlier.</p> </li> <li> <p>This <code>__name__</code> check, ensures that our node is the main executable (i.e. it has been executed directly (via <code>rosrun</code>), and hasn't been called by another script):</p> </li> <li> <p>We create an instance of the <code>Publisher</code> class that we created above (which executes the <code>__init__</code> method automatically). </p> </li> <li> <p>We call the <code>main_loop()</code> of our <code>Publisher()</code> class to execute the core functionality of the node. We wrap this inside a <code>try-except-pass</code> statement to look for a <code>rospy.ROSInterruptException</code> error, which can be output by rospy when the user presses <code>Ctrl+C</code> or the node is shutdown in some other way.</p> </li> </ol>"},{"location":"com2009/la1/week1/publisher/#shebang","title":"The Shebang","text":"<p>The very first line of code looks like a comment, but it is actually a very crucial part of the script:</p> <pre><code>#!/usr/bin/env python3\n</code></pre> <p>This is called the Shebang, and it tells the operating system which interpreter to use to execute the code. In our case here, it tells the operating system where to find the right Python interpreter that should be used to actually run the code.</p> <p> \u2190 Back to Week 1 - Exercise 6 </p>"},{"location":"com2009/la1/week1/subscriber/","title":"Week 1 Subscriber Node","text":""},{"location":"com2009/la1/week1/subscriber/#the-code","title":"The Code","text":"<p>Copy all the code below into your <code>subscriber.py</code> file and (again) make sure you read the annotations to understand how it all works!</p> subscriber.py<pre><code>#!/usr/bin/env python3\n# A simple ROS subscriber node in Python\nimport rospy # (1)!\nfrom std_msgs.msg import String\nclass Subscriber(): # (2)!\ndef callback(self, topic_message): # (3)!\nprint(f\"The '{self.node_name}' node obtained the following message: '{topic_message.data}'\")\ndef __init__(self): # (4)!\nself.node_name = \"simple_subscriber\"\ntopic_name = {BLANK}\nrospy.init_node(self.node_name, anonymous=True)\nself.sub = rospy.Subscriber(topic_name, String, self.callback)\nrospy.loginfo(f\"The '{self.node_name}' node is active...\")\ndef main_loop(self):\nrospy.spin() # (5)!\nif __name__ == '__main__': # (6)!\nsubscriber_instance = Subscriber()\nsubscriber_instance.main_loop()\n</code></pre> <ol> <li> <p>As with our publisher node, we need to import the <code>rospy</code> client library and the <code>String</code> message type from the <code>std_msgs.msg</code> library in order to write a Python ROS Node and use the relevant ROS messages:</p> </li> <li> <p>This time, we create a Python Class called <code>Subscriber()</code> instead.</p> </li> <li> <p>When building a subscriber, we need a callback function. Within this function, we define what we want to do with the messages that we obtain from the topic we're listening (subscribing) to:</p> <p>In this case, we simply want to print the <code>String</code> message to the terminal.</p> </li> <li> <p>Here we define the initialisation operations for the class:</p> <ol> <li>Here, we firstly initialise a rospy node with a custom name (in the same way as we initialised the publisher node earlier). </li> <li>Then, we create a <code>rospy.Subscriber</code> object, set this to listen to the topic that we want to receive messages from, specify the message type used by this topic, and then define the callback function to use to process the data whenever a message comes in.</li> <li>Then, we send a message to the terminal to indicate that our node is running.</li> </ol> </li> <li> <p>The <code>rospy.spin()</code> method simply makes sure that our <code>main_loop()</code> keeps running until the node is shut down externally (i.e. by a user entering <code>Ctrl+C</code>).</p> </li> <li> <p>Finally, the code is executed by again performing a <code>__name__</code> check, creating an instance of the <code>Subscriber()</code> class and calling the <code>main_loop()</code> method from that class.</p> </li> </ol> <p>Fill in the Blank!</p> <p>Replace the <code>{BLANK}</code> in the code above with the name of the topic that our <code>publisher.py</code> node was set up to publish to!</p>"},{"location":"com2009/la1/week1/subscriber/#dfts","title":"Don't Forget the Shebang!","text":"<p>First, don't forget the shebang, it's very important!</p> <pre><code>#!/usr/bin/env python3\n</code></pre>"},{"location":"com2009/la1/week1/subscriber/#a-simpler-approach","title":"A Simpler Approach","text":"<p>The above code uses a Python Class structure.  This approach will be very useful when we start to do more complex things later in the course, but for this exercise you could also achieve the same using the following simplified approach:</p> <pre><code>#!/usr/bin/env python3\nimport rospy\nfrom std_msgs.msg import String\nnode_name = \"simple_subscriber\"\ntopic_name = {BLANK}\ndef callback_function(topic_message):\nprint(f\"The '{node_name}' node obtained the following message: '{topic_message.data}'\")\nrospy.init_node(node_name, anonymous=True)\nsub = rospy.Subscriber(topic_name, String, callback_function)\nrospy.loginfo(f\"The '{node_name}' node is active...\")\nrospy.spin()\n</code></pre> <p> \u2190 Back to Week 1 - Exercise 7 </p>"},{"location":"com2009/la1/week3/move_square/","title":"Week 3 'move_square' Python Template","text":"<p>A combined publisher-subscriber node to achieve odometry-based control...</p> <p>Below you will find a template Python script to show you how you can both publish to <code>/cmd_vel</code> and subscribe to <code>/odom</code> in the same node.  This will help you build a closed-loop controller to make your robot follow a square motion path of size: 1m x 1m. </p> <p>You can publish velocity commands to <code>/cmd_vel</code> to make the robot move, monitor the robot's position and orientation in real-time, determine when the desired movement has been completed, and then update the velocity commands accordingly.  </p>"},{"location":"com2009/la1/week3/move_square/#suggested-approach","title":"Suggested Approach","text":"<p>Moving in a square can be achieved by switching between two different movement states sequentially: Moving forwards and turning on the spot. At the start of each movement step we can read the robot's current odometry, and then use this as a reference to compare to, and to tell us when the robot's position/orientation has changed by the required amount, e.g.:</p> <ol> <li>With the robot stationary, read the odometry to determine its current X and Y position in the environment.</li> <li>Move forwards until the robot's X and Y position indicate that it has moved linearly by 0.5m.</li> <li>Stop moving forwards.</li> <li>Read the robot's odometry to determine its current orientation (\"yaw\"/<code>\u03b8z</code>).</li> <li>Turn on the spot until the robot's orientation changes by 90\u00b0.</li> <li>Stop turning.</li> <li>Repeat.  </li> </ol> move_square.py<pre><code>import rospy\n# import the Twist message for publishing velocity commands:\nfrom geometry_msgs.msg import Twist\n# import the Odometry message for subscribing to the odom topic:\nfrom nav_msgs.msg import Odometry\n# import the function to convert orientation from quaternions to angles:\nfrom tf.transformations import euler_from_quaternion\n# import some useful mathematical operations (and pi), which you may find useful:\nfrom math import sqrt, pow, pi\nclass Square():\ndef callback_function(self, topic_data: Odometry):\n# obtain relevant topic data: pose (position and orientation):\npose = topic_data.pose.pose\nposition = pose.position\norientation = pose.orientation\n# obtain the robot's position co-ords:\npos_x = position.x\npos_y = position.y\n# convert orientation co-ords to roll, pitch &amp; yaw \n# (theta_x, theta_y, theta_z):\n(roll, pitch, yaw) = euler_from_quaternion(\n[orientation.x, orientation.y, orientation.z, orientation.w], \"sxyz\"\n)\n# We're only interested in x, y and theta_z\n# so assign these to class variables (so that we can\n# access them elsewhere within our Square() class):\nself.x = pos_x\nself.y = pos_y\nself.theta_z = yaw\n# If this is the first time that the callback_function has run\n# (e.g. the first time a message has been received), then\n# obtain a \"reference position\" (used to determine how far\n# the robot has moved during its current operation)\nif self.startup:\n# don't initialise again:\nself.startup = False\n# set the reference position:\nself.x0 = self.x\nself.y0 = self.y\nself.theta_z0 = self.theta_z\ndef __init__(self):\nnode_name = \"move_square\"\n# a flag if this node has just been launched\nself.startup = True\n# This might be useful in the main_loop() (to switch between \n# turning and moving forwards)\nself.turn = False\n# setup a '/cmd_vel' publisher and an '/odom' subscriber:\nself.pub = rospy.Publisher(\"cmd_vel\", Twist, queue_size=10)\nself.sub = rospy.Subscriber(\"odom\", Odometry, self.callback_function)\nrospy.init_node(node_name, anonymous=True)\nself.rate = rospy.Rate(10)  # hz\n# define the robot pose variables and initialise them to zero:\n# variables for the robot's \"current position\":\nself.x = 0.0\nself.y = 0.0\nself.theta_z = 0.0\n# variables for a \"reference position\":\nself.x0 = 0.0\nself.y0 = 0.0\nself.theta_z0 = 0.0\n# define a Twist message instance, to set robot velocities\nself.vel = Twist()\nself.ctrl_c = False\nrospy.on_shutdown(self.shutdownhook)\nrospy.loginfo(f\"the {node_name} node has been initialised...\")\ndef shutdownhook(self):\n# publish an empty twist message to stop the robot\n# (by default all velocities will be zero):\nself.pub.publish(Twist())\nself.ctrl_c = True\ndef main_loop(self):\nwhile not self.ctrl_c:\n# here is where your code would go to control the motion of your\n# robot. Add code here to make your robot move in a square of\n# dimensions 1 x 1m...\n# publish whatever velocity command has been set in your code above:\nself.pub.publish(self.vel)\n# maintain the loop rate @ 10 hz\nself.rate.sleep()\nif __name__ == \"__main__\":\nnode = Square()\ntry:\nnode.main_loop()\nexcept rospy.ROSInterruptException:\npass\n</code></pre>"},{"location":"com2009/la1/week3/move_square/#alternative-approach-waypoint-tracking","title":"Alternative Approach: Waypoint Tracking","text":"<p>A square motion path can be fully defined by the co-ordinates of its four corners, and we can make the robot move to each of these corners one-by-one, using its odometry system to monitor its real-time position, and adapting linear and angular velocities accordingly.</p> <p>This is slightly more complicated, and you might want to wait until you have a bit more experience with ROS before tackling it this way (we'll also cover this in the COM2009 lecture course).</p> <p> \u2190 Back to Week 3 - Exercise 1 </p>"},{"location":"com2009/la1/week4/move_client/","title":"Week 4 Move Service-Client Python Example","text":"<p>Copy all the code below into your <code>move_client.py</code> file and review the annotations to understand how it all works.</p> <p>DFTS!!</p> <p>(Don't forget the shebang!) <pre><code>#!/usr/bin/env python3\n</code></pre></p> move_client.py<pre><code>#!/usr/bin/env python3\nimport rospy # (1)!\nfrom tuos_ros_msgs.srv import SetBool, {BLANK} # (2)!\nimport sys # (3)!\nservice_name = \"move_service\" # (4)!\nrospy.init_node(f\"{service_name}_client\") # (5)!\nrospy.wait_for_service(service_name) # (6)!\nservice = rospy.ServiceProxy(service_name, SetBool) # (7)!\nservice_request = {BLANK}() # (8)!\nservice_request.request_signal = True # (9)!\nservice_response = service(service_request) # (10)!\nprint(service_response) # (11)!\n</code></pre> <ol> <li> <p>Again, the first step when building a Python node is to import the <code>rospy</code> library so that Python and ROS can interact. </p> </li> <li> <p>This service client will use the <code>SetBool</code> service message from the <code>tuos_ros_msgs</code> package, so we import the full definition of the <code>SetBool</code> Service Message, as well as the portion of the message that we will need to use to actually issue a service call.</p> </li> <li> <p>We import the Python <code>sys</code> module to do some error handling for us,  we don't need to worry about too much.</p> </li> <li> <p>Define the name of the service that we want to call, and assign this to a variable called <code>service_name</code> (for convenience, since we'll refer to this a couple of times). </p> </li> <li> <p>Initialise the client node (give it a name).</p> </li> <li> <p>Wait until the service that we want to call is actually running, execution of this node will not progress beyond this point until the service is detected on the ROS network (launched by the Server).</p> </li> <li> <p>Once it is running, we create a connection to it and specify the service message type that it uses (as defined above).</p> </li> <li> <p>Create an instance of the <code>{BLANK}</code> part of the service message, and populate this with the data that the server is expecting.</p> </li> <li> <p>Remember: Using <code>rossrv info</code> on this service message in a terminal tells us the attribute names for both the Request and Response:</p> <p><pre><code>rossrv info tuos_ros_msgs/SetBool\n</code></pre> ...gives us the following: <pre><code>bool request_signal\n---\nbool response_signal\nstring response_message\n</code></pre></p> </li> <li> <p>Use the <code>rospy.ServiceProxy</code> instance that we created earlier (called <code>service</code>) to actually send the <code>service_request</code> message to the service and obtain a response back from the Server (once it's complete).</p> </li> <li> <p>To finish off, we print the response to the terminal to give the user some feedback. Job done!</p> </li> </ol> <p>Fill in the Blank!</p> <p>Consider the <code>import</code> statement for the service Server that we created earlier... Which part of the <code>SetBool</code> Service message was imported here? Now consider that you need to build a client to call this service... which part of the <code>SetBool</code> Service message is needed in order to call a service? </p> <p>Note: the same <code>{BLANK}</code> appears in two places in the code above - the answer is the same in both places!</p> <p> \u2190 Back to Week 4 - Exercise 2 </p>"},{"location":"com2009/la1/week4/move_server/","title":"Week 4 Move Service-Server Python Example","text":"<p>Copy all the code below into your <code>move_server.py</code> file and review the annotations to understand how it all works.</p> <p>Remember</p> <p>Don't forget the shebang!</p> <pre><code>#!/usr/bin/env python3\n</code></pre> move_server.py<pre><code>#!/usr/bin/env python3 \nimport rospy\nfrom {BLANK}.msg import Twist # (1)!\nfrom tuos_ros_msgs.srv import SetBool, SetBoolResponse # (2)!\nservice_name = \"move_service\"\npub = rospy.Publisher('/cmd_vel', Twist, queue_size=1) # (3)!\ndef callback_function(service_request): # (4)!\nvel = Twist()\nservice_response = SetBoolResponse() # (5)!\nif service_request.request_signal == True: # (6)!\nprint(f\"The '{service_name}' Server received a 'true' request and the robot will now move for 5 seconds...\") # (7)!\nStartTime = rospy.get_rostime() # (8)!\nvel.linear.x = 0.1\npub.publish(vel) # (9)!\nrospy.loginfo('Published the velocity command to /cmd_vel')\nwhile (rospy.get_rostime().secs - StartTime.secs) &lt; 5: # (10)!\ncontinue\nrospy.loginfo('5 seconds have elapsed, stopping the robot...')\nvel.linear.x = 0.0\npub.publish(vel) # (11)!\nservice_response.response_signal = True # (12)!\nservice_response.response_message = \"Request complete.\"\nelse: # (13)!\nservice_response.response_signal = False\nservice_response.response_message = \"Nothing happened, set request_signal to 'true' next time.\"\nreturn service_response\nrospy.init_node(f\"{service_name}_server\") # (14)!\nmy_service = rospy.Service(service_name, SetBool, callback_function) # (15)!\nrospy.loginfo(f\"the '{service_name}' Server is ready to be called...\") # (16)!\nrospy.spin() # (17)!\n</code></pre> <ol> <li> <p>As you should know by now, in order to develop any ROS node in Python we first need to import the <code>rospy</code> library so that we can interact with ROS. We're also going to be issuing velocity commands to the robot, so we need to import the <code>Twist</code> message from the correct message package as well.</p> </li> <li> <p>We also need to import the Service Message that we want to use for the service that we will set up. This service will use the <code>SetBool</code> service message from a custom <code>tuos_ros_msgs</code> package that we've created for you.</p> <p>Here, we import two different things from the <code>tuos_ros_msgs</code> package:</p> <ol> <li>A definition of the full service message: <code>SetBool</code>, which we need to use when we create the service later.</li> <li>The Response portion of the service message: <code>SetBoolResponse</code>, which we will use to issue a response to the service caller.</li> </ol> </li> <li> <p>Next, we set up a publisher to the <code>/cmd_vel</code> topic, so that we can publish velocity commands to the robot (using <code>Twist</code> messages). Hopefully this part is starting to become familiar to you by now!</p> </li> <li> <p>Here we define a <code>callback_function</code>. Any code within this function will be executed whenever the service is called.</p> <p>The function can take one input argument only, in this case we are calling it <code>service_request</code>. This is where the <code>rospy.Service</code> instance that we set up earlier will put the data that it obtains from a <code>/move_service</code> call, whenever a Request is made.</p> </li> <li> <p>We create an instance of the Response portion of the <code>SetBool</code> service message, which we will populate with data later on (based on the outcome of the actions that the service server performs).</p> </li> <li> <p>We then analyse the service Request data (this is the data that is passed to the Server node, whenever a call to the service is made by a caller, or client). We know how to access the data within the service request from using the <code>rossrv info</code> command, which provides us with the following information:</p> <pre><code>rossrv info tuos_ros_msgs/SetBool:\n\nbool request_signal\n---\nbool response_signal\nstring response_message\n</code></pre> <p>The Request message will therefore contain a boolean value called <code>request_signal</code>, so we can call this value from the input to our callback function (which we called <code>service_request</code>). Using an <code>if</code> statement, we check if this value is <code>True</code> or <code>False</code>, and then define some actions for each situation accordingly...</p> </li> <li> <p>Print a status message to tell the Service caller that a <code>True</code> value has been received.</p> </li> <li> <p>Get the current ROS time.</p> </li> <li> <p>Set a linear velocity for the robot, publish this to the <code>/cmd_vel</code> topic using the publisher that we set up earlier (<code>pub</code>).</p> </li> <li> <p>Here, we use a while loop to act as a 5-second timer (by keeping an eye on the current ROS time using <code>get_rostime()</code>). Once 5 seconds have elapsed, this while loop will end.</p> </li> <li> <p>Once the time has elapsed, we publish another velocity command to make the robot stop.</p> </li> <li> <p>Finally, we can format a service Response using the <code>SetBoolResponse</code> instance that we set up earlier (<code>service_response</code>). Again, we know the names of the attributes in the service response from the <code>rossrv info</code> command:</p> <pre><code>rossrv info tuos_ros_msgs/SetBool:\n\nbool request_signal\n---\nbool response_signal\nstring response_message\n</code></pre> </li> <li> <p>If, the value of the <code>service_request.request_signal</code> was actually found to be <code>False</code> by our <code>if</code> statement earlier, then we do nothing other than send a service response, to indicate that nothing has happened!</p> </li> <li> <p>This part of the code will execute before anything in the callback function above. First, we initialise our new ROS Node with a name (<code>\"move_service_server\"</code>).</p> </li> <li> <p>Then, we create a <code>rospy.Service</code> instance where we define:</p> <ol> <li>The name of the service that this node will launch (<code>service_name = \"move_service\"</code> at the beginning of the code).</li> <li>The full service message format that the service will use, in this case: <code>SetBool</code>, which we imported earlier.</li> <li>A callback function, in this case called <code>callback_function</code>, which will define what we want this service Server to do once the service is called.</li> </ol> </li> <li> <p>Send some information to the terminal to indicate that the node has been launched successfully, and that the Service is ready to be called.</p> </li> <li> <p>The <code>rospy.spin()</code> function keeps our node running indefinitely (so that the callback function can continue to execute, whenever the service is called). </p> </li> </ol> <p>Fill in the Blank!</p> <p>Which message package does the <code>Twist</code> message belong to?</p> <p> \u2190 Back to Week 4 - Exercise 1 </p>"},{"location":"com2009/la1/week4/scan_callback/","title":"A LaserScan Callback Function","text":"<p>Based on what you have learnt throughout this course so far, you should now be able to set up a subscriber within your code to subscribe to the <code>/scan</code> topic:</p> <pre><code>self.subscriber = rospy.Subscriber('/scan', LaserScan, self.callback)\n</code></pre> <p>The above assumes that you are using a Python Class structure in your code (which you really should be!), and we also assume that you have already imported the <code>LaserScan</code> message type correctly at the start of your code as well:</p> <pre><code>from sensor_msgs.msg import LaserScan\n</code></pre> <p>You could then develop a callback function (called <code>callback()</code> in this case) using the approach illustrated below. </p> <p>Note</p> <p>This example requires the <code>numpy</code> Python library, which you'll need to import at the start of your code in the following way: <pre><code>import numpy as np\n</code></pre></p> <p>The <code>callback()</code> could then be developed to obtain (for instance) a 40\u00b0 arc of <code>LaserScan</code> data ahead of the robot:</p> <pre><code>def scan_callback(self, scan_data):\nleft_arc = scan_data.ranges[0:21]\nright_arc = scan_data.ranges[-20:]\nfront_arc = np.array(left_arc[::-1] + right_arc[::-1])\nself.min_distance = front_arc.min()\n# Optional Extra:\narc_angles = np.arange(-20, 21)\nself.object_angle = arc_angles[np.argmin(front_arc)]\n</code></pre> <p>The distance to the closest object in front of our robot is then available throughout our class, as an attribute called: <code>self.min_distance</code>.</p> <p>What we're doing here is illustrated in the figure below:</p> <p></p> <p>Let's talk through it too:</p> <ol> <li> <p>From the front of the robot, we obtain a 20\u00b0 arc of scan data either side of the x-axis:</p> <pre><code>left_arc = scan_data.ranges[0:21]\nright_arc = scan_data.ranges[-20:]\n</code></pre> </li> <li> <p>Then, we combine the <code>left_arc</code> and <code>right_arc</code> data arrays, flip them so that the data is arranged from left (-20\u00b0) to right (+20\u00b0), and then convert to a <code>numpy</code> array:</p> <pre><code>front_arc = np.array(left_arc[::-1] + right_arc[::-1])\n</code></pre> </li> <li> <p>Then, we obtain the minimum distance value within the <code>front_arc</code> array, to tell us the distance to the closest thing up ahead:    </p> <pre><code>self.min_distance = front_arc.min()\n</code></pre> </li> <li> <p>Optionally, we can also determine the angular position of the closest object up ahead too...</p> <ol> <li>Create another <code>numpy</code> array to represent the angles (in degrees) associated with each of the data-points in the <code>front_arc</code> array above:     <pre><code>arc_angles = np.arange(-20, 21)\n</code></pre></li> <li>Determine the angle at which the minimum distance value is located in front of the robot, using the <code>numpy.argmin()</code> method:     <pre><code>self.object_angle = arc_angles[np.argmin(front_arc)]\n</code></pre></li> </ol> </li> </ol> <p> \u2190 Back to Week 4 - Exercise 4 </p>"},{"location":"com2009/la1/week5/action_client/","title":"Week 5 Camera Sweep Action Client","text":"<p>Copy all the code below into your <code>move_client.py</code> file.  Then, review the code annotations to understand how it all works.</p> <p>(Oh, and DFTS!)</p> action_client.py<pre><code>#!/usr/bin/env python3\nimport rospy # (1)!\nimport actionlib # (2)!\nfrom tuos_ros_msgs.msg import CameraSweepAction, CameraSweepGoal, CameraSweepFeedback # (3)!\nnode_name = \"camera_sweep_action_client\"\naction_server_name = \"/camera_sweep_action_server\"\ncaptured_images = 0\ndef feedback_callback(feedback_data: CameraSweepFeedback): # (4)!\nglobal captured_images\ncaptured_images = feedback_data.{BLANK}\nprint(f\"FEEDBACK: Current yaw: {feedback_data.current_angle:.1f} degrees. \"\nf\"Image(s) captured so far: {captured_images}...\")\nrospy.init_node(node_name) # (5)!\nclient = actionlib.SimpleActionClient(action_server_name, \nCameraSweepAction) # (6)!\nclient.wait_for_server() # (7)!\ngoal = CameraSweepGoal()\ngoal.sweep_angle = 0\ngoal.image_count = 0\nclient.send_goal(goal, feedback_cb=feedback_callback) # (8)!\nclient.wait_for_result() # (9)!\nprint(f\"RESULT: Action State = {client.get_state()}\") # (10)!\nprint(f\"RESULT: {captured_images} images saved to {client.get_result()}\")\n</code></pre> <ol> <li> <p>As you know by now, in order to develop ROS nodes using Python we need to use the <code>rospy</code> library. </p> </li> <li> <p>If we want to work with ROS Actions, we also need to import <code>actionlib</code>.</p> </li> <li> <p>We know that the <code>/camera_sweep_action_server</code> uses <code>CameraSweepAction</code> messages from the <code>tuos_ros_msgs</code> package, so we import the full message definition: <code>CameraSweepAction</code> as well as the <code>Goal</code> message (which we use to actually make a call to the server). </p> <p>As we now know, ROS Actions provide feedback, so we're also importing the <code>Feedback</code> part of the <code>CameraSweepAction</code> message into our client node as well, so that it can be kept up to date with what the Action Server is doing, in real-time.</p> </li> <li> <p>To process this feedback data, we need to define a feedback callback function, to handle the data whenever a new feedback message comes in.</p> <p>Here we're using Python's \"Type Annotations\" feature:</p> <p><pre><code>feedback_data: CameraSweepFeedback\n</code></pre> ... which informs the interpreter that the <code>feedback_data</code> that is received by the <code>feedback_callback</code> function will be of the <code>CameraSweepFeedback</code> type.</p> <p>All this really does is allow autocomplete functionality to work with in our editor (VS Code), whenever we want to pull an attribute from the <code>feedback_data</code> object.</p> </li> <li> <p>Standard practice when we construct ROS nodes: we must initialise them with a name, where the <code>node_name</code> variable was assigned earlier on in the code: </p> <pre><code>node_name = \"camera_sweep_action_client\"\n</code></pre> </li> <li> <p>Then, we connect to the action server using the <code>actionlib.SimpleActionClient()</code> method, provide this with the name of the server that we wish to connect to and the type of messages that are used on the server (in this case <code>CameraSweepAction</code> messages).</p> <p>(The <code>action_server_name</code> variable was also assigned earlier on in the code too: <code>action_server_name = \"/camera_sweep_action_server\"</code>)</p> </li> <li> <p>This makes the node wait until the action server is live on the ROS Network (if it isn't already). Execution of the code can't continue past this point until the Action Server is visible on the ROS Network.</p> </li> <li> <p>Once the server is available, we construct a goal message and send this to the action server, whilst also pointing it to the callback function that we defined earlier, which will be used to process the feedback messages.</p> <p>Tip</p> <p>Both goal parameters have been set to <code>0</code> (above), so you'll need to change these in order for the client to successfully make a call to the server!</p> </li> <li> <p>Then, we simply wait for the action to complete.</p> </li> <li> <p>Once it has completed, the server provides us with a result, so we simply print that (as well as the current state of the action) to the terminal. </p> </li> </ol> <p>Fill in the Blank!</p> <p>Which attribute of the <code>feedback_data</code> object tells us how many images have been captured over the course of the Camera Sweep Action? There are a number of ways we can work this out:</p> <ol> <li>You could use the same approach as we used earlier. </li> <li>You could run <code>rosmsg info tuos_ros_msgs/CameraSweepFeedback</code> in a terminal.</li> <li>You could use the autocomplete/variable suggestions provided in VS Code!</li> </ol> <p> \u2190 Back to Week 5 - Exercise 2 </p>"},{"location":"com2009/la1/week5/preemptive_action_client/","title":"Week 5 Preemptive Action Client","text":"<p>Copy all the code below into your <code>preemptive_action_client.py</code> file and then review the annotations!</p> preemptive_action_client.py<pre><code>#!/usr/bin/env python3\nimport rospy\nimport actionlib\nfrom tuos_ros_msgs.msg import CameraSweepAction, CameraSweepGoal, CameraSweepFeedback\nclass preemptiveActionClient(): # (1)!\ngoal = CameraSweepGoal() # (2)!\ndef feedback_callback(self, feedback_data: CameraSweepFeedback): # (3)!\nself.captured_images = feedback_data.current_image\nprint(f\"FEEDBACK: Current yaw: {feedback_data.current_angle:.1f} degrees. \"\nf\"Image(s) captured so far: {self.captured_images}...\")\ndef __init__(self): # (4)!\nself.captured_images = 0\nself.action_complete = False\nnode_name = \"preemptive_camera_sweep_action_client\"\naction_server_name = \"/camera_sweep_action_server\"\nrospy.init_node(node_name)\nself.rate = rospy.Rate(1)\nself.client = actionlib.SimpleActionClient(action_server_name, \nCameraSweepAction)\nself.client.wait_for_server()\nrospy.on_shutdown(self.shutdown_ops)\ndef shutdown_ops(self): # (5)!\nif not self.action_complete:\nrospy.logwarn(\"Received a shutdown request. Cancelling Goal...\")\nself.client.cancel_goal()\nrospy.logwarn(\"Goal Cancelled...\")\n# get the result: (6) \nrospy.sleep(1) # wait for the result to come in\nprint(\"RESULT:\")\nprint(f\"  * Action State = {self.client.get_state()}\")\nprint(f\"  * {self.captured_images} image(s) saved to {self.client.get_result()}\")\ndef send_goal(self, images, angle): # (7)!\nself.goal.sweep_angle = angle\nself.goal.image_count = images\n# send the goal to the action server:\nself.client.send_goal(self.goal, feedback_cb=self.feedback_callback)        \ndef main_loop(self):\nself.send_goal(images = 0, angle = 0) # (8)!\ni = 1 # (9)!\nprint(\"While we're waiting, let's do our seven-times tables...\")\nwhile self.client.get_state() &lt; 2:\nprint(f\"STATE: Current state code is {self.client.get_state()}\")\nprint(f\"TIMES TABLES: {i} times 7 is {i*7}\")\ni += 1\nself.rate.sleep()\nself.action_complete = True # (10)!\nif __name__ == '__main__':\n{BLANK} # (11)!\n</code></pre> <ol> <li> <p>Everything is now contained within a Python Class.</p> </li> <li> <p>Instantiate a goal message object, which we'll use later to call the Camera Sweep action.</p> </li> <li> <p>The feedback callback function is exactly the same as the one in the <code>action_client.py</code> from Exercise 2. Because we're working inside a Python Class now, we can make the <code>captured_images</code> variable available beyond the scope of this function by using the <code>self</code> prefix (previously this was achieved using the <code>global</code> statement).</p> </li> <li> <p>Classes require an <code>__init__()</code> method, which will be executed as soon as the class is instantiated. Here, we do all our initialisations:</p> <ul> <li>Initialise some variables (<code>captured_images</code>, <code>action_complete</code>) and make them available throughout the class by prefixing with <code>self</code>.</li> <li>Initialise the node (with a name).</li> <li>Set a rate (1 Hz).</li> <li>Create a connection to the action server and wait for it to become available.</li> <li>Specify a function to be executed when the node is stopped (<code>shutdown_ops()</code>).</li> </ul> <p>... none of this should be new to you now!</p> </li> <li> <p>The actual shutdown operations are defined here. This is how we make sure that the current goal is cancelled (using <code>cancel_goal()</code>), so that the action doesn't keep on running if this node is stopped prematurely (before the action has completed).  This function will also execute when the action server completes successfully, so we use an <code>action_complete</code> flag to check whether this is the case (i.e. to avoid trying to cancel the goal if it's already finished!):</p> </li> <li> <p>This bit will execute regardless of whether the action completed successfully or was preempted. Here, we're getting three things and printing them to the terminal:</p> <ol> <li>The result from the action server.</li> <li>The action state (using the <code>get_state()</code> method).</li> <li>The final number of captured images (obtained from the last feedback message that was issued before the action stopped).</li> </ol> </li> <li> <p>The way the goal is defined and issued to the server is exactly the same as before, except this time it's done within this class method, so that it can be called from <code>main_loop()</code>.</p> </li> <li> <p>Call the goal.</p> </li> <li> <p>All this is the same as before (<code>action_client.py</code>), i.e. monitor the state of the action with a <code>while</code> loop and do some concurrent operations (seven-times tables again!).</p> </li> <li> <p>The only difference is that we set this flag to <code>True</code> if the action manages to complete successfully.</p> </li> <li> <p>Fill in the Blank!<p>We have contained all our code inside a nice Python Class now, but how do we actually instantiate it and invoke the Action Call? (We've been doing this from the very beginning, and the process is very much the same here!)</p> </p> </li> </ol> <p> \u2190 Back to Week 5 - Exercise 3 </p>"},{"location":"com2009/la1/week5/search_client/","title":"Week 5 Search Client Template","text":"search_client.py<pre><code>#! /usr/bin/env python3\n# search_client.py\nimport rospy\nimport actionlib\nfrom tuos_ros_msgs.msg import SearchAction, SearchGoal, SearchFeedback\nclass SearchActionClient():\ngoal = SearchGoal()\ndef feedback_callback(self, feedback_data: SearchFeedback):\n## TODO: get the current distance travelled, from the feedback message\n## and assign this to a class variable...\nself.distance = ...\ndef __init__(self):\nself.distance = 0.0\nself.action_complete = False\nrospy.init_node(\"search_action_client\")\nself.rate = rospy.Rate(1)\n## TODO: setup a \"simple action client\" with a callback function\n## and wait for the server to be available...\nself.client = ...\nrospy.on_shutdown(self.shutdown_ops)\ndef shutdown_ops(self):\nif not self.action_complete:\nrospy.logwarn(\"Received a shutdown request. Cancelling Goal...\")\n## TODO: cancel the goal request, if this node is shutdown before the action has completed...\nrospy.logwarn(\"Goal Cancelled...\")\n## TODO: Print the result here...\ndef main_loop(self):\n## TODO: assign values to all goal parameters\n## and send the goal to the action server...\nself.goal...\nwhile self.client.get_state() &lt; 2:\n## TODO: Construct an if statement and cancel the goal if the \n## distance travelled exceeds 2 meters...\nif self.distance ...\n# break out of the while loop to stop the node:\nbreak\nself.rate.sleep()\nself.action_complete = True\nif __name__ == '__main__':\n## TODO: Instantiate the node and call the main_loop() method from it...\n</code></pre> <p> \u2190 Back to Week 5 - Exercise 4 </p>"},{"location":"com2009/la1/week5/search_server/","title":"Week 5 Search Server Template","text":"search_server.py Template<pre><code>#! /usr/bin/env python3\n# search_server.py\n# Import the core Python modules for ROS and to implement ROS Actions:\nimport rospy\nimport actionlib\n# Import all the necessary ROS message types:\nfrom tuos_ros_msgs.msg import SearchAction, SearchFeedback, SearchResult, SearchGoal\n# Import the tb3 modules from tb3.py\nfrom tb3 import Tb3Move, Tb3Odometry, Tb3LaserScan\n# Import some other useful Python Modules\nfrom math import sqrt, pow\nclass SearchActionServer():\nfeedback = SearchFeedback() \nresult = SearchResult()\ndef __init__(self):\n## TODO: create a \"simple action server\" with a callback function, and start it...\nself.actionserver = actionlib.SimpleActionServer(...)\n# pull in some useful publisher/subscriber functions from the tb3.py module:\nself.vel_controller = Tb3Move()\nself.tb3_odom = Tb3Odometry()\nself.tb3_lidar = Tb3LaserScan()\nrospy.loginfo(\"The 'Search Action Server' is active...\")\n# The action's \"callback function\":\ndef action_server_launcher(self, goal: SearchGoal):\nrate = rospy.Rate(10)\n## TODO: Implement some checks on the \"goal\" input parameter(s)\nsuccess = True\nif goal...\nsuccess = False\nif not success:\n## TODO: abort the action server if an invalid goal has been requested...\nreturn\n## TODO: Print a message to indicate that the requested goal was valid\nprint(f\"...\")\n# Get the robot's current odometry from the Tb3Odometry() class:\nself.posx0 = self.tb3_odom.posx\nself.posy0 = self.tb3_odom.posy\n# Get information about objects up ahead from the Tb3LaserScan() class:\nself.closest_object = self.tb3_lidar.min_distance\nself.closest_object_location = self.tb3_lidar.closest_object_position\n## TODO: set the robot's forward velocity (as specified in the \"goal\")...\nself.vel_controller...\n## TODO: establish a conditional statement so that the  \n## while loop continues as long as the distance to the closest object\n## ahead of the robot is always greater than the \"approach distance\"\n## (as specified in the \"goal\")...\nwhile {something} &gt; {something_else}:\n# update LaserScan data:\nself.closest_object = self.tb3_lidar.min_distance\nself.closest_object_location = self.tb3_lidar.closest_object_position\n## TODO: publish a velocity command to make the robot start moving \nself.vel_controller...\n# check if there has been a request to cancel the action mid-way through:\nif self.actionserver.is_preempt_requested():\n## TODO: take appropriate action if the action is cancelled (pre-empted)...\nsuccess = False\n# exit the loop:\nbreak\n# determine how far the robot has travelled so far:\nself.distance = sqrt(pow(self.posx0 - self.tb3_odom.posx, 2) + pow(self.posy0 - self.tb3_odom.posy, 2))\n## TODO: update all feedback message values and publish a feedback message:\nself.feedback...\n## TODO: update all result parameters:\nself.result...\nrate.sleep()\nif success:\nrospy.loginfo(\"approach completed successfully.\")\n## TODO: Set the action server to \"succeeded\" and stop the robot...\nif __name__ == '__main__':\nrospy.init_node(\"search_action_server\")\nSearchActionServer()\nrospy.spin()\n</code></pre> <p>Important</p> <p>The template above uses the <code>tb3.py</code> module from the <code>tuos_ros_examples</code> package, which contains various helper functions to make the robot move and to read data (a.k.a. subscribe) from some key topics that will be useful for the task at hand. To use this, you'll need to copy the module across to your own <code>week5_actions/src</code> folder so that your <code>search_server.py</code> node can import it. In TERMINAL 2, copy the <code>.py</code> file as follows:</p> <pre><code>cp ~/catkin_ws/src/COM2009/tuos_ros_examples/src/tb3.py ~/catkin_ws/src/week5_actions/src/\n</code></pre> <p> \u2190 Back to Week 5 - Exercise 4 </p>"},{"location":"com2009/la1/week6/line_follower/","title":"Week 6 Line Following","text":"<p>Copy all the code below into your <code>line_follower.py</code> file.  Then, review the annotations to understand how it all works.</p> line_follower.py<pre><code>#!/usr/bin/env python3\nimport rospy\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom sensor_msgs.msg import Image\nfrom tb3 import Tb3Move\nclass LineFollower(object): # (1)!\ndef __init__(self):\nnode_name = \"line_follower\"\nrospy.init_node(node_name, anonymous=True)\nself.rate = rospy.Rate(5)\nself.cvbridge_interface = CvBridge()\nself.img_sub = rospy.Subscriber(\"/camera/rgb/image_raw\", Image, self.camera_cb)\nself.robot_controller = Tb3Move()\nself.ctrl_c = False\nrospy.on_shutdown(self.shutdown_ops)\ndef shutdown_ops(self):\nself.robot_controller.stop()\ncv2.destroyAllWindows()\nself.ctrl_c = True\ndef camera_cb(self, img_data):\ntry:\ncv_img = self.cvbridge_interface.imgmsg_to_cv2(img_data, desired_encoding=\"bgr8\")\nexcept CvBridgeError as e:\nprint(e)\n_, width, _ = cv_img.shape # (2)!\ncrop_width = 500\ncrop_height = 40\ncrop_x = int((width / 2) - (crop_width / 2))\ncropped_img = cv_img[10:crop_height, 10:crop_x+crop_width]\nhsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\nlower = (100, 100, 100)\nupper = (255, 255, 255)\nmask = cv2.inRange(hsv_img, lower, upper)\nres = cv2.bitwise_and(cropped_img, cropped_img, mask = mask)\nm = cv2.moments(mask)\ncy = m['m10'] / (m['m00'] + 1e-5) # (3)!\ncz = m['m01'] / (m['m00'] + 1e-5)\ncv2.circle(res, (int(cy), int(cz)), 10, (255, 0, 0), 2)\ncv2.imshow(\"filtered image\", res)\ncv2.waitKey(1)\ny_error = cy - (width / 2) # (4)!\nkp = 1.0 / 50.0\nfwd_vel = 0.1 # (5)!\nang_vel = kp * y_error # (6)!\nprint(f\"Y-error = {y_error:.3f} pixels, ang_vel = {ang_vel:.3f} rad/s\")\nself.robot_controller.set_move_cmd(fwd_vel, ang_vel)\nself.robot_controller.{BLANK} # (7)!\ndef main(self):\nwhile not self.ctrl_c:\nself.rate.sleep()\nif __name__ == '__main__':\nlf_instance = LineFollower()\ntry:\nlf_instance.main()\nexcept rospy.ROSInterruptException:\npass\n</code></pre> <ol> <li> <p>A lot of the things in here you will already be familiar with from the previous exercises, so we won't go into too much detail on all of this again.  The main thing you will notice is that we have once again built a class structure around this, which should now be familiar to you from previous weeks of this course.  </p> </li> <li> <p>In this case we want our robot to follow the line that is printed on the floor. We do this by applying the same image processing steps as in the previous exercises, to isolate the colours associated with the line and calculate its location in the robot's viewpoint.</p> </li> <li> <p>We'll use the centroid component <code>cy</code> to determine how far the robot needs to turn in order to keep the line in the centre of its vision:</p> </li> <li> <p>We are implementing proportional control here.</p> COM2009 Students! <p>PID control was covered by Prof Moore in Lecture 6!</p> <p>Ideally, we want the centre of the line on the floor to be in the centre of the robot's viewpoint at all times: this is our target position.  The actual position is where the line on the floor actually is, i.e.: the <code>cy</code> centroid component.  The position error is then the difference between the actual and target position:</p> </li> <li> <p>The only way we can reduce this error is by changing the robot's angular velocity.  The robot always needs to travel with forward velocity, so we define a fixed value at all times to achieve this. </p> </li> <li> <p>In order to correct for our position error, we multiply it by a proportional gain (<code>kp</code>), which will provide us with an angular velocity that should start to make the error reduce.</p> <p>If the proportional gain is set appropriately, this should ensure that our position error (<code>y_error</code>) is always kept to a minimum, so that the robot follows the line!</p> </li> <li> <p>We then simply set these two velocities in our <code>robot_controller</code> object and then publish them to the <code>/cmd_vel</code> topic using methods from the <code>Tb3Move()</code> class. </p> <p>Fill in the Blank!</p> <p>There is a method within the <code>Tb3Move()</code> class which allows us to publish a velocity command to the <code>/cmd_vel</code> topic. What is it? (Have a look at the <code>tb3.py</code> source code if you need a reminder).</p> </li> </ol> <p> \u2190 Back to Week 6 - Exercise 4 </p>"},{"location":"com2009/la1/week6/object_detection/","title":"Week 6 Object Detection Node","text":"<p>Copy all the code below into your <code>object_detection.py</code> file, and make sure you read the annotations!</p> <p>.. oh, and I'm sure I don't need to say it by now, but... DFTS!</p> object_detection.py<pre><code>#!/usr/bin/env python3\nimport rospy\nfrom pathlib import Path # (1)!\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError # (2)!\nfrom sensor_msgs.msg import Image # (3)!\n# Initialisations: (4)\nnode_name = \"object_detection_node\"\nrospy.init_node(node_name)\nprint(f\"Launched the '{node_name}' node. Currently waiting for an image...\")\nrate = rospy.Rate(5)\nbase_image_path = Path.home().joinpath(\"myrosdata/week6_images/\")\nbase_image_path.mkdir(parents=True, exist_ok=True) # (5)!\ncvbridge_interface = CvBridge() # (6)!\nwaiting_for_image = True # (7)!\ndef show_and_save_image(img, img_name): # (8)!\nfull_image_path = base_image_path.joinpath(f\"{img_name}.jpg\") # (9)!\nprint(\"Opening the image in a new window...\")\ncv2.imshow(img_name, img) # (10)!\nprint(f\"Saving the image to '{full_image_path}'...\")\ncv2.imwrite(str(full_image_path), img) # (11)!\nprint(f\"Saved an image to '{full_image_path}'\\n\"\nf\"image dims = {img.shape[0]}x{img.shape[1]}px\\n\"\nf\"file size = {full_image_path.stat().st_size} bytes\") # (12)!\nprint(\"Please close down the image pop-up window to continue...\")\ncv2.waitKey(0) # (13)!\ndef camera_cb(img_data): # (14)!\nglobal waiting_for_image # (15)!\ntry:\ncv_img = cvbridge_interface.imgmsg_to_cv2(img_data, desired_encoding=\"bgr8\") # (16)!\nexcept CvBridgeError as e:\nprint(e)\nif waiting_for_image == True: # (17)!\nheight, width, channels = cv_img.shape\nprint(f\"Obtained an image of height {height}px and width {width}px.\")\nshow_and_save_image(cv_img, img_name = \"step1_original\")\nwaiting_for_image = False\nrospy.Subscriber(\"/camera/rgb/image_raw\", Image, camera_cb) # (18)!\nwhile waiting_for_image: # (19)!\nrate.sleep()\ncv2.destroyAllWindows() # (20)!\n</code></pre> <ol> <li> <p>Of course, we always need to import <code>rospy</code> so that Python can work with ROS. What we're also importing here is the Python <code>Path</code> class from the <code>pathlib</code> module, which will be used to do a few file operations.</p> </li> <li> <p>Then, we're importing the OpenCV library for Python (remember the Python API that we talked about earlier), which is called <code>cv2</code>, and also that ROS-to-OpenCV bridge interface that we talked about earlier too: <code>cv_bridge</code>.</p> <p>From <code>cv_bridge</code> we're importing the <code>CvBridge</code> and <code>CvBridgeError</code> classes from the <code>cv_bridge</code> library specifically.</p> </li> <li> <p>We need to subscribe to an image topic in order to obtain the messages being published to it. You should've already identified the type of message that is published to the <code>/camera/rgb/image_raw</code> topic, so we import that message type here (from the <code>sensor_msgs</code> package) so that we can build a subscriber to the topic later.</p> </li> <li> <p>Next, we're doing a number of initialisations that should be very familiar to you by now:</p> <ol> <li>Giving our node a name.</li> <li>Initialising the node (i.e. registering it on the ROS network using <code>rospy.init_node()</code>).</li> <li>Specifying a rate at which we want the node to run.</li> </ol> </li> <li> <p>Then, we define a filesystem location that we'll use to save images to. We know that there's a directory in the home directory of the WSL-ROS filesystem called \"myrosdata\", so we can use Pathlib's <code>Path.home().joinpath(...)</code> to define it (without necessary needing to know the name of the home directory itself). Then, we use the Pathlib <code>Path.mkdir()</code> method to create this directory, if it doesn't exist already.</p> </li> <li> <p>Here, we create an instance of the <code>CvBridge</code> class that we imported earlier, and which we'll use later on to convert ROS image data into a format that OpenCV can understand.</p> </li> <li> <p>We're creating a flag to indicate whether the node has obtained an image yet or not. For this exercise, we only want to obtain a single image, so we will set the <code>waiting_for_image</code> flag to <code>False</code> in our camera callback function once an image has been obtained, to avoid capturing any more.</p> </li> <li> <p>This function defines some image operations that we will need to repeat multiple times (this will become apparent later). The further annotations explain more about what's going on inside this function...</p> </li> <li> <p>Construct a full file path for an image (using the <code>Path.joinpath()</code> method) from:</p> <ol> <li>The <code>base_image_path</code> that we defined earlier and </li> <li> <p>An image name that is passed into this function via the <code>img_name</code> argument.</p> <p>We'll use this to save the file to our filesystem later on.</p> </li> </ol> </li> <li> <p>Display the actual image in a pop-up window:</p> <ol> <li>The image data is passed into the function via the <code>img</code> argument,</li> <li>We need to give the pop-up window a name, so in this case we are using the <code>img_name</code> argument that has also been passed into the function.</li> </ol> </li> <li> <p>This saves the image to a <code>.jpg</code> file.  We're supplying the <code>full_image_path</code> that was created above, and also the actual image data (<code>img</code>) so that the function knows what image we want to save.</p> </li> <li> <p>We're printing a message to the terminal to inform us of (a) where the image has been saved to, (b) how big the image was (in terms of its pixel dimensions) and (c) how big the image file is (in bytes).</p> </li> <li> <p>We're supplying a value of <code>0</code> here, which tells this function to wait indefinitely before allowing our <code>show_and_save_image()</code> function to end. If we had supplied a value here (say: <code>1</code>) then the function would simply wait 1 millisecond and then close the pop-up window down. In our case however, we want some time to actually look at the image and then close the window down ourselves, manually. Once the window has been closed, the execution of our code is able to continue...</p> </li> <li> <p>Here, we're defining a callback function for a <code>rospy.Subscriber()</code>...</p> </li> <li> <p>We want to make changes to the <code>waiting_for_image</code> flag inside this function, but make sure that these changes are also observed outside the function too (i.e. by the <code>while</code> loop that we talked about above).  So, we change the scope of the variable to global here using the <code>global</code> statement.</p> </li> <li> <p>We're using the CvBridge interface to take our ROS image data and convert it to a format that OpenCV will be able to understand.  In this case we are specifying conversion (or \"encoding\") to an 8-bit BGR (Blue-Green-Red) image format: <code>\"bgr8\"</code>.</p> <p>We contain this within a <code>try-except</code> block though, which is the recommended procedure when doing this.  Here we try to convert an image using the desired encoding, and if a <code>CvBridgeError</code> is raised then we print this error to the terminal.  Should this happen, this particular execution of the camera callback function will stop.</p> </li> <li> <p>Then we check the <code>waiting_for_image</code> flag to see if this is the first image that has been received by the node.  If so, then:</p> <ol> <li>Obtain the height and width of the image (in pixels), as well as the number of colour channels.</li> <li>Print the image dimensions to the terminal.</li> <li>Pass the image data to the <code>show_and_save_image()</code> function (as discussed earlier). We also pass a descriptive name for the image to this function too (<code>img_name</code>).</li> <li>Finally, we set the <code>waiting_for_image</code> flag to <code>False</code> so that we only ever perform these processing steps once (we only want to capture one image remember!).  This will then trigger the main <code>while</code> loop to stop, thus causing the overall execution of the node to stop too.</li> </ol> </li> <li> <p>Create subscriber to the <code>/camera/rgb/image_raw</code> topic, telling the <code>rospy.Subscriber()</code> function the message type that is used by this topic (<code>sensor_msgs/Image</code> - as imported above), and we point it to a callback function (<code>camera_cb</code>, in this case), to define the processes that should be performed every time a message is obtained on this topic (in this case, the messages will be our camera images)</p> </li> <li> <p>Go into a <code>while</code> loop, and use the <code>rate.sleep()</code> method to maintain this loop at a speed of 5 Hz (as defined earlier) whilst checking the <code>waiting_for_image</code> flag to see if an image has been obtained by our subscriber yet.  We only really want to obtain a single image here, so once the <code>waiting_for_image</code> flag changes to <code>False</code>, the <code>while</code> loop will stop.</p> </li> <li> <p>Finally, <code>cv2.destroyAllWindows()</code> ensures that any OpenCV image pop-up windows that may still be active or in memory are destroyed before the node shuts down. </p> </li> </ol> <p> \u2190 Back to Week 6 - Exercise 2 </p>"},{"location":"com2009/la1/week6/object_detection_complete/","title":"Week 6 Object Detection Node: Complete","text":"<p>Here's a full example of the <code>object_detection.py</code> node that you should have developed during The Object Detection Exercise.  Also included here is an illustration of how to use the <code>cv2.circle()</code> method to create a marker on an image illustrating the centroid of the detected feature, as discussed here.</p> object_detection_complete.py<pre><code>#!/usr/bin/env python3\nimport rospy\nfrom pathlib import Path\nimport cv2\nfrom cv_bridge import CvBridge, CvBridgeError\nfrom sensor_msgs.msg import Image\nnode_name = \"object_detection_node\"\nrospy.init_node(node_name)\nprint(f\"Launched the '{node_name}' node. Currently waiting for an image...\")\nrate = rospy.Rate(5)\nbase_image_path = Path.home().joinpath(\"myrosdata/week6_images/\")\nbase_image_path.mkdir(parents=True, exist_ok=True)\ncvbridge_interface = CvBridge()\nwaiting_for_image = True\ndef show_and_save_image(img, img_name):\nfull_image_path = base_image_path.joinpath(f\"{img_name}.jpg\")\nprint(\"Opening the image in a new window...\")\ncv2.imshow(img_name, img)\nprint(f\"Saving the image to '{full_image_path}'...\")\ncv2.imwrite(str(full_image_path), img)\nprint(f\"Saved an image to '{full_image_path}'\\n\"\nf\"image dims = {img.shape[0]}x{img.shape[1]}px\\n\"\nf\"file size = {full_image_path.stat().st_size} bytes\")\nprint(\"Please close down the image pop-up window to continue...\")\ncv2.waitKey(0)\ndef camera_cb(img_data):\nglobal waiting_for_image  \ntry:\ncv_img = cvbridge_interface.imgmsg_to_cv2(img_data, desired_encoding=\"bgr8\")\nexcept CvBridgeError as e:\nprint(e)\nif waiting_for_image == True:\nheight, width, channels = cv_img.shape\nprint(f\"Obtained an image of height {height}px and width {width}px.\")\nshow_and_save_image(cv_img, img_name = \"step1_original\")\ncrop_width = width - 400\ncrop_height = 400\ncrop_y0 = int((width / 2) - (crop_width / 2))\ncrop_z0 = int((height / 2) - (crop_height / 2))\ncropped_img = cv_img[crop_z0:crop_z0+crop_height, crop_y0:crop_y0+crop_width]\nshow_and_save_image(cropped_img, img_name = \"step2_cropping\")\nhsv_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2HSV)\nlower_threshold = (115, 225, 100)\nupper_threshold = (130, 255, 255)\nimg_mask = cv2.inRange(hsv_img, lower_threshold, upper_threshold)\nshow_and_save_image(img_mask, img_name = \"step3_image_mask\")\nfiltered_img = cv2.bitwise_and(cropped_img, cropped_img, mask = img_mask)\n# FINDING THE IMAGE CENTROID: (1)\nm = cv2.moments(img_mask) # (2)!\ncy = m['m10'] / (m['m00'] + 1e-5)\ncz = m['m01'] / (m['m00'] + 1e-5) # (3)!\ncv2.circle(\nfiltered_img, \n(int(cy), int(cz)), \n10, (0, 0, 255), 2) # (4)!\nshow_and_save_image(filtered_img, img_name = \"step4_filtered_image\")\nwaiting_for_image = False\nrospy.Subscriber(\"/camera/rgb/image_raw\", Image, camera_cb)\nwhile waiting_for_image:\nrate.sleep()\ncv2.destroyAllWindows()\n</code></pre> <ol> <li> <p>Everything here should be familiar to you from earlier in this exercise, except for this section...</p> </li> <li> <p>Here, we obtain the moments of our colour blob by providing the boolean representation of it (i.e. the <code>img_mask</code>) to the <code>cv2.moments()</code> function.</p> </li> <li> <p>Then, we are determining where the central point of this colour blob is located by calculating the <code>cy</code> and <code>cz</code> coordinates of it.  This provides us with pixel coordinates relative to the top left-hand corner of the image.</p> </li> <li> <p>Finally, this function allows us to draw a circle on our image at the centroid location so that we can visualise it.  Into this function we pass:</p> <ol> <li>The image that we want the circle to be drawn on.  In this case: <code>filtered_img</code>.</li> <li>The location that we want the circle to be placed, specifying the horizontal and vertical pixel coordinates respectively: <code>(int(cy), int(cz))</code>.</li> <li>How big we want the circle to be: here we specify a radius of 10 pixels.</li> <li>The colour of the circle, specifying this using a Blue-Green-Red colour space: <code>(0, 0, 255)</code> (i.e.: pure red in this case)</li> <li>Finally, the thickness of the line that will be used to draw the circle, in pixels.</li> </ol> </li> </ol> <p> \u2190 Back to Week 6 - Exercise 2 </p>"},{"location":"com2009/la2/","title":"Lab Assignment #2: Team Robotics Challenge","text":""},{"location":"com2009/la2/#team-robotics-challenge","title":"Team Robotics Challenge","text":"<p>In Lab Assignment #2 you will put into practice everything that you have learnt about ROS so far, and explore the capabilities of the framework further.</p> <p>For the rest of the Semester (Weeks 7-12) you will work in teams to develop a ROS package to make a TurtleBot3 Waffle complete a series of robotic tasks in both simulation and the real world. There are five tasks to complete in total, all of which you will be assessed on as part of Lab Assignment #2.</p> <p>This assignment is worth 30% of the overall mark for the COM2009 (&amp; 3009) course.</p>"},{"location":"com2009/la2/getting-started/","title":"Getting Started","text":"<p>There are a few things you should do now, before you start to sink your teeth into the Assignment Tasks themselves. Consider the tasks on this page as your \"To-Do List\" for Week 7 (the first week of Lab Assignment #2):</p> <p>To-Do:</p> <ul> <li> Step 1: Launch WSL-ROS and Restore Your Work</li> <li> Step 2: Download the Assignment #2 Simulation Resources</li> <li> Step 3: Set Up Your Team's ROS Package<ul> <li> 3.1. Configuring Git in WSL-ROS</li> <li> 3.2. Creating Your Team's Lab Assignment #2 ROS package</li> <li> 3.3. Pushing Your Package to GitHub</li> <li> 3.4. Transferring Your Package from WSL-ROS to a Robot Laptop</li> </ul> </li> <li> Step 4: Getting Started with the Real Robots</li> </ul>"},{"location":"com2009/la2/getting-started/#step1","title":"Step 1: Launch WSL-ROS and Restore Your Work","text":"<ul> <li>Load up your WSL-ROS environment by running the WSL-ROS shortcut in the Windows Start Menu. </li> <li> <p>When prompted, enter <code>Y</code> to restore your work.</p> <p>Remember</p> <p>You can also use the <code>wsl_ros restore</code> command to do this.</p> </li> </ul>"},{"location":"com2009/la2/getting-started/#step2","title":"Step 2: Download the Assignment #2 Simulation Resources","text":"<p>We've put together a ROS package called <code>com2009_simulations</code> to help you with this assignment. This contains a number of simulation resources to help you work through the assignment #2 tasks. Where a task is to be assessed on a real robot, simulations are provided that are representative of the real-world environment that your robot might be placed in.</p> <p>The <code>com2009_simulations</code> package is part of the COM2009 Course Repo, which you should have already downloaded to your own Catkin Workspace in Assignment #1, Week 2. Follow the steps below now, to make sure you've got all the latest updates:</p> <ol> <li> <p>In a WSL-ROS terminal instance, navigate to the COM2009 repo:</p> <pre><code>cd ~/catkin_ws/src/COM2009/\n</code></pre> </li> <li> <p>Then, pull down any updates from GitHub:  </p> <pre><code>git pull\n</code></pre> </li> <li> <p>From the same terminal location, run <code>catkin build</code> to compile the new package (for good measure):</p> <pre><code>catkin build com2009*\n</code></pre> </li> <li> <p>Finally, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> <li> <p>To test that everything's working, launch a simulation environment called <code>beacon_colours</code> from the <code>com2009_simulations</code> package:</p> <p><pre><code>roslaunch com2009_simulations beacon_colours.launch\n</code></pre> The <code>beacon_colours</code> arena, illustrating all possible colours that your robot might need to detect in the Assignment #2 Tasks. </p> </li> </ol>"},{"location":"com2009/la2/getting-started/#step3","title":"Step 3: Set Up Your Team's ROS Package","text":"<p>As discussed earlier, everything that your team submit for this lab assignment must be contained within a single ROS package. Inside this you will develop all the necessary nodes to make a TurtleBot3 Waffle (real or simulated) complete each of the assignment tasks. Each task will be assessed by the Teaching Team via launch files that you must also provide within your package. You'll therefore need to create one launch file per task (we'll talk more about this later).</p> <p>It's best to create your team's ROS package in WSL-ROS to start with. We recommend that you then initialise this as a Git repo, and push it to GitHub to allow for sharing and collaboration amongst your team1. You should be able to transfer your package between WSL-ROS and a robot laptop using Git and GitHub (or other remote package repository) too, see below for further details on this.</p> <p>Only one member of your team needs to create your ROS package and upload it to GitHub (or other). Regardless of who does this part though, you will all need to interact with Git for this assignment, and you should therefore each set up your own individual Git configurations in the WSL-ROS environment. Do this now by following the instructions in the next section...</p>"},{"location":"com2009/la2/getting-started/#git","title":"3.1. Configuring Git in WSL-ROS","text":"<p>All team members should do this bit!</p> <p>Git will automatically set your name and email address using the WSL-ROS username and the hostname of the remote machine that you are working on. It is important that this is changed to match your own personal identity!</p> <ol> <li> <p>From a WSL-ROS terminal instance located in your home directory run the following command to edit the Git configuration file:</p> <pre><code>git config --global --edit\n</code></pre> <p>This will open up the <code>.gitconfig</code> file in Nano (a terminal-based text editor). By default, this file should have the following content:</p> <pre><code># This is Git's per-user configuration file.\n[user]\n# Please adapt and uncomment the following lines:\n#   name = TUoS Student\n#   email = student@#####.shefuniad.shef.ac.uk\n</code></pre> </li> <li> <p>Uncomment the <code>name = ...</code> and <code>email = ...</code> lines by removing the <code>#</code> at the start of each of these lines in the file. Then, change <code>TUoS Student</code> and <code>student@#####.shefuniad.shef.ac.uk</code> to match your own personal identity.</p> </li> <li> <p>Once done, press Ctrl+X to exit Nano. Before it closes, you'll be asked if you want to save the changes that you have made, so enter Y to confirm this and then press Enter to confirm that you want to keep the same <code>/home/student/.gitconfig</code> file name (to overwrite the original content).</p> </li> <li> <p>Finally, don't forget to run <code>wsl_ros backup</code> to save these changes to your external WSL-ROS backup file, so that they will always be restored whenever you run <code>wsl_ros restore</code> in a fresh WSL-ROS instance. </p> </li> </ol>"},{"location":"com2009/la2/getting-started/#create-pkg","title":"3.2. Creating Your Team's Lab Assignment #2 ROS package","text":"<p>Note</p> <p>Only one team member actually needs to do this bit, but you will then need to share it with the rest of your team members via an online code repository (e.g. GitHub), by making the rest of your team members collaborators.</p> <ol> <li> <p>In a WSL-ROS terminal instance, navigate to the <code>catkin_ws/src</code> directory of the WSL-ROS filesystem:</p> <pre><code>cd ~/catkin_ws/src/\n</code></pre> </li> <li> <p>Use the <code>catkin_create_pkg</code> tool to create a new ROS package in exactly the same way as you did multiple times during Lab Assignment #1:</p> <pre><code>catkin_create_pkg com2009_team{} rospy\n</code></pre> <p>...replacing the <code>{}</code> with your team number!</p> <p>Warning</p> <p>It's really important that you follow the naming conventions that we specify when defining your package (and creating your launch files). If you don't then you could receive no marks!</p> </li> <li> <p>Run <code>catkin build</code> and then re-source your environment:</p> <p><pre><code>catkin build com2009_team{}\n</code></pre> <pre><code>source ~/.bashrc\n</code></pre></p> </li> <li> <p>Then navigate into the package directory that should have just been created:</p> <pre><code>cd com2009_team{}/\n</code></pre> </li> <li> <p>This should already contain a <code>src</code> folder for you to populate with all your Python ROS nodes. Create a <code>launch</code> folder in here too, which you will use to store all your launch files:</p> <pre><code>mkdir launch\n</code></pre> </li> <li> <p>Create a placeholder file in the <code>src</code> and <code>launch</code> directories, just to make sure that these folders both get pushed to GitHub when we get to that part in the following section:</p> <pre><code>touch src/placeholder &amp;&amp; touch launch/placeholder\n</code></pre> <p>(you can delete these later on, once you start creating your own Nodes and launch files.)</p> </li> <li> <p>Then follow the steps in the next section to import this project to GitHub (other online code repositories should work similarly)...</p> </li> </ol>"},{"location":"com2009/la2/getting-started/#github","title":"3.3. Pushing Your Package to GitHub","text":"<p>Note</p> <p>Only one member of your team needs to do this bit too!</p> <p>These instructions are taken from this GitHub Docs page. Instructions may vary if you are using other online code repositories (such as GitLab for instance), so check with your target provider.</p> <ol> <li> <p>Create a new repository on GitHub.com, but DON'T initialise the new repository with a README, license, or gitignore (you can do this later, it'll cause issues if you do it at this stage).</p> <p> From docs.github.com </p> <p>Call this repository <code>com2009_team{}</code> to match the ROS package that you've just created.</p> </li> <li> <p>Head back to the WSL-ROS terminal instance that you used to create your ROS package in the previous section. This should still be located in the root of your new package directory, but just to be sure, you can run: </p> <pre><code>roscd com2009_team{}\n</code></pre> </li> <li> <p>Initialise your package as a Git repo:</p> <pre><code>git init -b main\n</code></pre> </li> <li> <p>Stage all the initial files in your package (including the placeholders) for an initial commit:</p> <pre><code>git add .\n</code></pre> <p>Warning</p> <p>Don't forget the <code>.</code> at the end there!</p> </li> <li> <p>Then commit them:</p> <pre><code>git commit -m \"First commit\"\n</code></pre> </li> <li> <p>Head back to GitHub. At the top of your repository on GitHub.com's Quick Setup page, click the  button to copy the remote repository URL (HTTPS).</p> <p> From docs.github.com </p> </li> <li> <p>Then, go back to your WSL-ROS terminal and add the URL to the remote repository:</p> <pre><code>git remote add origin {REMOTE_URL}\n</code></pre> <p>Change <code>{REMOTE_URL}</code> to the URL that you copied in the previous step, and get rid of the curly brackets (<code>{}</code>)!</p> <p>Then verify the new remote URL:  <pre><code>git remote -v\n</code></pre></p> </li> <li> <p>Finally, push the changes from your package in WSL-ROS (your \"local\" repo), to your \"remote\" repository on GitHub:</p> <pre><code>git push origin main\n</code></pre> <p>You'll then be asked to enter your GitHub username, followed by a password. This password is not your GitHub account password!  </p> <p>Warning</p> <p>Your GitHub account password won't work here! You'll need to generate a personal access token (classic) and use this instead!</p> </li> </ol>"},{"location":"com2009/la2/getting-started/#laptop","title":"3.4. Transferring Your Package from WSL-ROS to a Robot Laptop","text":"<p>You'll need to transfer your package to a robot laptop whenever you want to work on a real robot in the real robot arena during the labs! There is a Catkin Workspace on each of the robot laptops and (much the same as in the WSL-ROS environment) your package must reside within this workspace on the laptop too!</p> <p>Note</p> <p>Make sure that the laptop is connected to \"eduroam\" when you try to do this!</p> <ol> <li> <p>From a terminal instance on the laptop navigate to the Catkin Workspace <code>src</code> directory:</p> <pre><code>cd ~/catkin_ws/src\n</code></pre> </li> <li> <p>Clone your package into this directory using <code>git</code>:</p> <pre><code>git clone {REMOTE_URL}\n</code></pre> <p>Where <code>{REMOTE_URL}</code> is the https URL to your repo on GitHub.</p> </li> <li> <p>Run Catkin Build to make sure that any resources within your package that need to be compiled (custom ROS messages, etc.) are compiled onto the laptop so that they can be used locally:</p> <pre><code>catkin build com2009_team{}\n</code></pre> <p>...again, replacing the <code>{}</code> with your team number.</p> </li> <li> <p>Then, re-source your environment:</p> <pre><code>source ~/.bashrc\n</code></pre> Pro Tip <p>There's a <code>src</code> alias for that command on the laptops too!</p> </li> </ol> <p>Don't forget to commit and push any updates that you make to your ROS package while working on the laptop back to your remote repository!</p>"},{"location":"com2009/la2/getting-started/#step4","title":"Step 4: Getting Started with the Real Robots","text":"<p>As you'll know by now, some Lab Assignment #2 Tasks will be assessed using real robots, and you'll therefore have access to the robots for every lab session from Week 7 onwards, so that you can work on these tasks as you wish. All the details on how the robots work, how to get them up and running and start programming them can be found in the \"Waffles\" section of this course site. You should proceed now as follows (in your teams):</p> <ol> <li>Everyone must complete a health and safety quiz (on Blackboard) before you (or your team) work with the real robots for the first time. Head to Blackboard and do this now, if you haven't already.</li> <li>Each team has been assigned a specific robot (there's a list on Blackboard). When you're ready, speak to a member of the teaching team who will provide you with the robot that has been assigned to you.</li> <li> <p>Work through each and every page of the \"Waffles\" section of this site, in order!</p> <p>Make sure you have a go at the Initial Exercises, which will help to get you started. Then, move onto the Fact-Finding Missions: there are 5 of these in total, and it's really important that you complete them all.  </p> </li> </ol> <p> <p>Setup complete, now on to the assignment tasks...</p> <p></p> <ol> <li> <p>As a University of Sheffield student, you can apply for the GitHub Student Developer Pack, which gives you access to a range of developer tools including GitHub Pro. GitHub Pro allows you to have unlimited collaborators on your repositories, which might help you to collaborate on your ROS package with your team.\u00a0\u21a9</p> </li> </ol>"},{"location":"com2009/la2/overview/","title":"Overview","text":""},{"location":"com2009/la2/overview/#the-tasks","title":"The Tasks","text":"<p>There are five tasks in total that you must complete for Lab Assignment #2. Each task is marked and goes towards your final grade for Lab Assignment #2. Tasks will be assessed either in simulation or on a real robot. The assessment format for each task and the overall marking breakdown is shown in the table below. There are 100 marks available, in total, for Lab Assignment #2.</p> <p> Task Details Marks Assessment Format 1 A deployable ROS Package 20/100 Real Robots 2 Obstacle Avoidance 15/100 Simulation 3 Maze Navigation 15/100 Simulation 4 Detection, Search &amp; Beaconing 15/100 Simulation 5 Real-World Exploration 35/100 Real Robots <p></p>"},{"location":"com2009/la2/overview/#submission-details","title":"Submission Details","text":"<p>There are two submission deadlines, summarised in the table below (see Blackboard for exact dates and times).</p> <p> Submission Task(s) Deadline A Task 1 Week 9 B Tasks 2, 3, 4 &amp; 5 Week 12 <p></p> <p>For each submission, you'll need to provide a ROS package (as a <code>.tar</code> file) to a submission portal on Blackboard. </p> <p>Before you get started on any of the programming tasks you should (as a team) create a single ROS package (further details on the next page). You can then add all the necessary functionality for each task as you go along. For each submission, you'll then need to create a copy of your package in its current state by creating a <code>.tar</code> archive of it, and submit this to Blackboard by the specified deadline (the export process is explained here). </p> <p>Note</p> <p>You should work on each task as a team, and you only need to make one submission per team for each task.</p>"},{"location":"com2009/la2/overview/#assessment","title":"Assessment","text":"<p>For simulation-based tasks Your team's submission will be assessed by extracting and running your package in the same WSL-ROS environment that you have been working with throughout Lab Assignment #1.</p> <p>For real-robot-based tasks Your team's submission will be assessed by the Teaching Team by extracting and running your package on one of our robotics laptops, that you will use extensively throughout Weeks 7-12. Your submission will be used to control a real robot in the Diamond Computer Room 3 Robot Arena.</p> <p>Regardless of the assessment type, you will receive a recording of the assessment afterwards so that you can see how well your robot performed in each of the tasks, and so that you can see exactly how your marks were awarded!</p>"},{"location":"com2009/la2/overview/#launching-your-code","title":"Launching Your Code","text":"<p>In order to launch the necessary functionality within your package for a given task you will need to include correctly named launch files, <code>task1.launch</code>, <code>task2.launch</code>, etc. This will allow you to ensure that all the required functionality is executed when your submission is assessed, and also ensures that we know exactly how to launch this functionality in order to assess it. Full details of the requirements for each launch file are provided on the associated task page.</p> <p>Warning</p> <p>It's up to you to ensure that your code launches as intended for a given task. If it doesn't, then you'll be awarded zero marks, so make sure you test it all out prior to submission! </p>"},{"location":"com2009/la2/overview/#key-requirements","title":"Key Requirements","text":"<p>In order to be awarded any marks for any task outlined in the table above, you must ensure that the following key requirements are met in regard to the ROS package that you submit (as well as any additional requirements specific to a given task):</p> <ol> <li> <p>Your package must be submitted to Blackboard as a <code>.tar</code> file with the following naming convention:</p> <pre><code>com2009_team{}.tar\n</code></pre> <p>Where the <code>{}</code> is replaced with your own team number. See here for how to create a <code>.tar</code> archive of your package.</p> </li> <li> <p>Your ROS package directory, when extracted, must follow a similar naming convention:</p> <pre><code>com2009_team{}/\n</code></pre> <p>Again, replacing the <code>{}</code> with your own team number!</p> </li> <li> <p>Your ROS package name must also be the same, so that the following would work (for example):</p> <pre><code>roslaunch com2009_team100 task1.launch\n</code></pre> <p>(assuming you are Team 100!)</p> </li> <li> <p>Finally (and most importantly), your ROS package must work \"out-of-the-box\", i.e. the Teaching Team won't be able to make any modifications or fix any errors for you! </p> </li> </ol> <p>Warning</p> <p>Failure to follow these requirements could result in you being awarded zero marks!</p>"},{"location":"com2009/la2/submission/","title":"Exporting your ROS Package for Submission","text":"<p>When it comes to submission time, it's important that you follow the steps below carefully to create an archive of your ROS package correctly. We recommend that you do this from WSL-ROS on a University Managed Desktop Computer (rather than one of the robot laptops), so that you can save it to your University <code>U:</code> Drive.</p> <ol> <li> <p>First, navigate to the <code>catkin_ws/src</code> directory in a WSL-ROS terminal instance:</p> <pre><code>cd ~/catkin_ws/src/\n</code></pre> </li> <li> <p>Then, use the <code>tar</code> command to create an archive of your package:</p> <pre><code>tar -cvf /mnt/u/wsl-ros/com2009_team{}.tar com2009_team{}\n</code></pre> <p>... replacing <code>{}</code> with your own team number, of course!</p> <p>This will create the <code>.tar</code> archive in your own personal University <code>U:</code> Drive, which you can access using the Windows File Explorer...</p> </li> <li> <p>In Windows, open up Windows Explorer, click \"This PC\" in the left-hand toolbar and locate your own personal <code>U:</code> Drive in the \"Network locations\" area.</p> </li> <li> <p>In here there should be a <code>wsl-ros</code> folder, which should contain the <code>com2009_team{}.tar</code> file that you just created.</p> </li> <li> <p>Submit this <code>.tar</code> file to Blackboard via the appropriate submission portal.</p> </li> </ol>"},{"location":"com2009/la2/task1/","title":"Task 1: Creating a Deployable ROS Package","text":"<p>Submit a working ROS package that can be successfully deployed to control a real TurtleBot3 Waffle, making it follow a prescribed motion profile whilst printing key information to the terminal.</p> <p>Assessment Format: Real Robots Marks: 20/100</p>"},{"location":"com2009/la2/task1/#summary","title":"Summary","text":"<p>The main aims of this task are as follows:</p> <ol> <li>Learn how to work with the real robots. </li> <li>Get to grips with the process for preparing your ROS package for submission (as required for this and all further tasks).</li> </ol> <p>A lot of the functionality that you will need to implement here will be familiar to you from Lab Assignment #1, but you'll need to enhance things a bit. The main objective is to create a ROS node (or multiple nodes) that make your robot follow a figure-of-eight pattern on the robot arena floor. The figure-of-eight trajectory should be generated by following two loops, both 1 meter in diameter, as shown below. </p> <p> </p> The figure-of-eight path for Task 1. <p>Whilst doing this, you will also need to print some robot odometry data to the terminal at regular intervals (see below for the specifics). In order to get the terminal message formatting right, you might want to have a look at the documentation on Python String Formatting, or refer to any of the code examples that we provided you with during Lab Assignment #1 that involved printing messages to the terminal.</p> <p>With regard to the odometry and keeping track of your robot's pose, remember what we covered in Lab Assignment #1, Week 2.</p>"},{"location":"com2009/la2/task1/#details","title":"Details","text":"<ol> <li>The robot must start by moving anti-clockwise, following a circular motion path of 1 m diameter (\"Loop 1\", as shown in the figure above).</li> <li>Once complete, the robot must then turn clockwise to follow a second circular path, again of 1 m diameter (\"Loop 2\").</li> <li>After Loop 2 the robot must stop, at which point it should be located back at its starting point.</li> <li>The velocity of the robot should be set to ensure that the whole sequence takes approximately 60 seconds to complete (5 seconds).</li> <li> <p>The robot's current odometry data should be printed to the terminal throughout, where messages should be of the following format: </p> <pre><code>x={x} [m], y={y} [m], yaw={yaw} [degrees].\n</code></pre> <p>Where <code>{x}</code>, <code>{y}</code> and <code>{yaw}</code> should be replaced with the correct real-time odometry data as follows:</p> <ol> <li><code>{x}</code>: the robot's linear position in the X axis, quoted in meters to two decimal places.</li> <li><code>{y}</code>: the robot's linear position in the Y axis, quoted in meters to two decimal places.</li> <li><code>{yaw}</code>: the robot's orientation about the Z axis, quoted in degrees to one decimal place.</li> </ol> <p>The data should be quoted relative to its starting position at the beginning of the task, e.g. at the start of the task (before the robot has moved) the terminal messages should read:</p> <pre><code>x=0.00 [m], y=0.00 [m], yaw=0.0 [degrees].\n</code></pre> <p>These message should be printed to the terminal at a rate of 1Hz. It doesn't matter if the messages continue to be printed to the terminal after the robot has stopped (i.e. after the figure-of-eight has been completed).</p> </li> <li> <p>The ROS package that you submit must contain a launch file called <code>task1.launch</code>, which will be used by the teaching team to execute the functionality from within your package. This functionality must be launch-able via the command: </p> <pre><code>roslaunch com2009_team{} task1.launch\n</code></pre> <p>... where <code>{}</code> will be replaced with your team number.</p> <p>(ROS will already be running on the robot before we attempt to execute your launch file).</p> </li> </ol>"},{"location":"com2009/la2/task1/#a-note-on-odometry","title":"A note on Odometry","text":"<p>When the robot is placed in the arena at the start of the task its odometry may not necessarily read zero, so you will need to compensate for this. You'll therefore need to grab the robot pose from the <code>/odom</code> topic before your robot starts moving, and then use that as the zero-reference to convert all the subsequent odometry readings that you obtain throughout the task.</p>"},{"location":"com2009/la2/task1/#simulation-resources","title":"Simulation Resources","text":"<p>It's easier to develop your node(s) in simulation before testing things out on a real robot. You can use the standard <code>empty_world</code> environment to do this, which (as you'll recall from Lab Assignment #1) can be launched in WSL-ROS using the following command:</p> <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre> <p>(or by using the <code>tb3_empty_world</code> command-line alias).</p> <p>For the real task, there will be cylindrical objects placed at the theoretical centre of each of the figure-of-eight loops, so your robot will need to move around these as it completes the task. We have therefore also created a simulation environment that is representative of the real world environment during the assessment. This is available in the <code>com2009_simulations</code> package, and can be launched with the following command:</p> <pre><code>roslaunch com2009_simulations task1.launch\n</code></pre> <p> </p> The development arena for Task 1. <p>Note</p> <p>There won't be any loop markers on the real robot arena floor during the assessment.</p>"},{"location":"com2009/la2/task1/#marking","title":"Marking","text":"<p>This task will be assessed by the teaching team before the further submission deadlines in Week 12. If things don't work then we'll let you know what happened, so that you can try to resolve things for the further assignment submissions.</p> <p>There are 20 marks available for this task in total, awarded based on the criteria outlined below.</p>"},{"location":"com2009/la2/task1/#criterion-a-a-deployable-ros-package","title":"Criterion A: A deployable ROS package","text":"<p>Marks: 10/20 </p> <p>You will be awarded full marks here if you submit a ROS package containing a launch file as specified above, you submit your package according to the Key Submission Requirements and your package can successfully be deployed on one of the robotics laptops. If any of these requirements are not met then you will be awarded zero marks for the entire task, gulp!</p>"},{"location":"com2009/la2/task1/#criterion-b-figure-of-eight","title":"Criterion B: Figure-of-eight","text":"<p>Marks: 5/20</p> <p> Criteria Details Marks B.1: Direction of travel The robot must move anticlockwise for the first loop (\"Loop 1\") and then clockwise for the second (\"Loop 2\"). 1 B.2: Loop 1 The loop must be ~1 m in diameter, centered about the red beacon. 1 B.3: Loop 2 The loop must be ~1 m in diameter, centered about the blue beacon. 1 B.4: Stopping Once the robot completes its figure of eight, it must stop with both wheels within 10 cm of the start line. 1 B.5: Timing The robot must complete the full figure of eight and stop in 55-65 seconds. 1 <p></p>"},{"location":"com2009/la2/task1/#criterion-c-odometry-messages","title":"Criterion C: Odometry messages","text":"<p>Marks: 5/20</p> <p> Criteria Details Marks C.1: Rate Messages should be printed to the terminal at a rate of 1 Hz. 1 C.2: Format The messages printed to the terminal should be formatted exactly as detailed above. 1 C.3: Data Each message value (<code>x</code>, <code>y</code> and <code>yaw</code>) should be plausible, that is: they each correspond to the actual pose of the robot, based on a zero point at the start/finish point (as illustrated above). In addition, each value must be quoted in the correct units (meters or degrees, as appropriate). 3 <p></p> <p>If your package is not deployable on the first submission then you'll have an opportunity to resubmit this before the end of Week 12, giving you a second chance to get things right and obtain some marks here. Any resubmissions will not be eligible for any marks for Criterion A on the second attempt though, so your team will only be able to achieve a maximum mark of 10/20 in such cases.</p> <p>Warning</p> <p>We will be offering a resubmission opportunity for this task only. Resubmission of any further tasks won't be possible!</p>"},{"location":"com2009/la2/task2/","title":"Task 2: Obstacle Avoidance","text":"<p>Develop the ROS node(s) that allow a TurtleBot3 Waffle robot to autonomously explore a simulated environment containing various obstacles. The robot must explore as much of the environment as possible in 90 seconds without crashing into anything!</p> <p>Assessment Format: Simulation Marks: 15/100</p>"},{"location":"com2009/la2/task2/#summary","title":"Summary","text":"<p>In Lab Assignment #1, Week 3 you were introduced to the LiDAR sensor on the robot, and learned what the data from this sensor tells us about the distance to any objects that are present in the robot's environment. In Week 5 Exercise 4 you should have used this data, in combination with the ROS Action framework, as the basis for a basic obstacle avoidance control system. Then, in Advanced Exercise 1 (Week 5) we discussed how this could be developed further into an effective search strategy by developing an action client node that would make successive calls to the action server to keep the robot moving randomly, and indefinitely, around an arena whilst avoiding obstacles.</p> <p>This is one approach that you could use for this first task, but there are other (and potentially simpler) ways that this could be achieved too. </p> <p>Consider COM2009 Lecture 3 (\"Sensing, Actuation &amp; Control\"), for instance, where you were introduced to Cybernetic Control Principles and some of Braitenberg's \"Vehicles\" were discussed and implemented on a Lego robot.  In particular, \"Vehicle 3b\" might well be relevant to consider as a simple method to achieve an obstacle avoidance behaviour.</p> <p>Another aspect of this task is exploration: your robot will be awarded marks based on how much of the environment it is able to navigate around. Consider the search strategies that were discussed in Lecture 8 (\"Local Guidance Strategies\"), such as \"Brownian Motion\" and \"Levy walks,\" and how something along these lines could be implemented on the TurtleBot3 Waffle.</p>"},{"location":"com2009/la2/task2/#details","title":"Details","text":"<p>The simulated environment that your robot will need to explore for this will be a square arena of 5.0 m 5.0 m, so slightly bigger than the real robot arena in Diamond Computer Room 3. For the task, the arena will contain a number of \"obstacles,\" i.e.: short wooden walls and coloured cylinders. Your robot will need to be able to detect these obstacles and navigate around them in order to fully explore the space.</p> <ol> <li>The robot will start in the centre of the arena (denoted \"Zone 5\").</li> <li>It must explore the environment for 90 seconds without touching any of the arena walls or the obstacles within it.</li> <li>If the robot makes contact with anything before the time has elapsed then the attempt will be stopped.</li> <li>Nine equal-sized zones are marked out on the arena floor and the robot must enter as many of these as possible during the attempt.</li> <li> <p>The robot must be moving for the entire duration of the task. Simply just turning on the spot for the whole time doesn't count!</p> <p></p> </li> <li> <p>The ROS package that you submit must contain a launch file called <code>task2.launch</code>, such that the functionality that you develop for Task 2 can be launched from your package via the command:</p> <pre><code>roslaunch com2009_team{} task2.launch\n</code></pre> <p>Test this out in WSL-ROS before submission to make sure that it works!</p> </li> <li> <p>This task will be assessed in simulation and the simulated environment will already be running before we attempt to execute your launch file. </p> </li> </ol> <p>Note</p> <p>The location, orientation and quantity of obstacles in the arena will not be revealed beforehand, so the ROS package that you develop will need to be able to accommodate an unknown environment. </p>"},{"location":"com2009/la2/task2/#simulation-resources","title":"Simulation Resources","text":"<p>Within the <code>com2009_simulations</code> package there is an example arena which can be used to develop and test out your team's obstacle avoidance node(s) for this task. The simulation can be launched using the following <code>roslaunch</code> command:</p> <pre><code>roslaunch com2009_simulations obstacle_avoidance.launch\n</code></pre> <p></p> <p> </p> The `obstacle_avoidance` development arena. <p>Tip</p> <p>The location, orientation and quantity of obstacles will be different for the final assessment of this task!</p> <p>Have a go at varying the environment yourselves, using the methods that we used in Lab Assignment #1, Week 4 to move, resize or delete existing objects, or add new simple box geometries.</p>"},{"location":"com2009/la2/task2/#marking","title":"Marking","text":"<p>There are 15 marks available for Task 2 in total, awarded based on the following criteria:</p> <p> Criteria Marks Details A: Run time 7/15 You will be awarded marks for the amount of time that your robot spends exploring the environment before 90 seconds has elapsed, or the robot makes contact with anything in its environment (as per the table below). B: Exploration 8/15 You will be awarded 1 mark for each new arena zone that the robot manages to enter (excluding the one it starts in). The robot only needs to enter each zone once, but its full body must be inside the zone marking to be awarded the mark. <p></p> <p>Marks for \"Run Time\" will be awarded as follows:</p> <p></p> <p> Time (Seconds) Marks 0-9 0 10-19 1 20-29 2 30-39 3 40-49 4 50-59 5 60-89 6 The full 90! 7 <p></p>"},{"location":"com2009/la2/task3/","title":"Task 3: Maze Navigation","text":"<p>Develop the ROS node(s) that allow a simulated TurtleBot3 Waffle robot to navigate a maze in 150 seconds or less without crashing into anything.</p> <p>Assessment Format: Simulation Marks: 15/100</p>"},{"location":"com2009/la2/task3/#summary","title":"Summary","text":"<p>For this task your robot will need to navigate a maze of corridors without touching the walls.</p> <p>As with Task 2, the LiDAR sensor will be essential here, allowing the robot to detect the maze walls and maintain a safe distance from them. Your robot will need to progress through the maze all the way to the end in order to be awarded maximum marks for this. The robot's Odometry System might prove useful too; allowing you to keep track of where your robot is and where it has already been, so that it continues to progress through the maze and doesn't waste time going back on itself. The maze will also contain dead-ends, so you'll need to make sure your robot is able to navigate its way out of these and (ideally) not go down them repeatedly!</p> <p>One common method for solving mazes is to use a wall following algorithm. The maze that your simulated robot will be placed in won't contain any islands, i.e. it will be \"simply connected\", so this is one method you might choose to adopt here.</p>"},{"location":"com2009/la2/task3/#details","title":"Details","text":"<p>As with Task 2, the simulated arena that your robot will be assessed in for this will be a square of 5.0 m x 5.0 m. There will only be wooden walls in the arena for this task, no other objects. The maze will be constructed to ensure there is always enough space for a TurtleBot3 Waffle to comfortably pass through any apertures or corridors.</p> <ol> <li>To begin, the robot will be located in a blue \"Start Zone\" at the start of the maze.</li> <li>The robot must then autonomously navigate the maze in order to find its way to a green \"Finish Zone\" (or get as close as possible to it).</li> <li>The robot must do this without touching any of the arena walls: penalties will be applied for those that do (See the table below).</li> <li>The robot's progress will be measured at 10% increments throughout the maze, using distance markers printed on the arena floor (where the finish line is at 100%).</li> <li> <p>The robot will have a maximum of 150 seconds to navigate the maze.</p> <p></p> </li> <li> <p>Your team's ROS package must contain a launch file called <code>task3.launch</code>, such that the functionality that you develop for this maze navigation task can be launched from your package via the command:</p> <pre><code>roslaunch com2009_team{} task3.launch\n</code></pre> </li> <li> <p>Once again, this task will be assessed in simulation and the simulated environment (the arena containing the maze) will already be running before we attempt to execute your launch file.</p> </li> </ol>"},{"location":"com2009/la2/task3/#simulation-resources","title":"Simulation Resources","text":"<p>Within the <code>com2009_simulations</code> package there is an example maze navigation arena which can be used to develop and test out your team's maze navigation node(s) for this task. The simulation can be launched using the following <code>roslaunch</code> command:</p> <pre><code>roslaunch com2009_simulations maze_nav.launch\n</code></pre> <p> <p> </p> The `maze_nav` arena for Task 3. <p></p> <p>Note</p> <ol> <li>The maze that will be used for the assessment will be a different configuration to this</li> <li>Corners in the final maze won't necessarily all be at right angles like they are in the simulation: some may be acute or obtuse!</li> </ol>"},{"location":"com2009/la2/task3/#marks","title":"Marking","text":"<p>There are 15 marks available for this task in total, awarded based on the following criteria:</p> <p> Criteria Marks Details A: Progress through the maze 10/15 Marks will be awarded based on the highest percentage value distance marker that your robot crosses within the 150-second time limit (it doesn't matter if the robot happens to turn around and move back behind a marker again at any point during the assessment). Marks will be awarded at 10% increments only (i.e. no fractional marks), but the whole of the robot must have crossed the progress marker in order to be awarded the associated marks. B: An \"incident-free-run\" 5/15 If the robot completes the task (or the 150 seconds elapses) without it making contact with anything in the arena then your team will be awarded the maximum marks here. Marks will be deducted for each contact that the robot makes with the environment, to a minimum of 0 (i.e. there will be no negative marking here: the minimum mark that you can receive for this is zero). Your robot must at least pass the 10% progress marker to be eligible for these marks. <p></p>"},{"location":"com2009/la2/task4/","title":"Task 4: Detection, Search & Beaconing","text":"<p>Develop the ROS node(s) that enable a TurtleBot3 Waffle to search an environment, detect a coloured object and beacon towards it, stopping in proximity without crashing into it!</p> <p>Assessment Format: Simulation Marks: 15/100</p>"},{"location":"com2009/la2/task4/#summary","title":"Summary","text":"<p>For this task your robot will be placed in a \"Search Arena\" containing a number of different coloured objects (\"beacons\"). Building on the exploration behaviour that you will have developed for Task 2, the robot will need to search the arena for a beacon of a particular \"Target Colour\" (each beacon in the arena will have a unique colour). The arena will contain three \"Start Zones\" (also uniquely coloured) and your robot will be located in one of these three zones to begin with (selected at random). The colour of the Start Zone indicates the Target Colour for the search task (the colour of the beacon that the robot needs to find). Once the target object has been detected, the robot will need to move towards it (i.e. \"beaconing\") and stop within a \"Stop Zone\" printed on the floor surrounding it. The robot must stop in the Stop Zone without touching the beacon!</p> <p>The first thing that your robot will need to do in this task is detect the colour of the zone that it starts in. We learnt about colour detection in Lab Assignment #1, Week 6, where we used OpenCV to analyse the images published to the <code>/camera/rgb/image_raw/</code> topic. Use what we did here, as well as the further work that we did in Week 6 Exercise 3, as a starting point for achieving the desired behaviour for this initial part of the task. Note that in the Week 6 exercises you developed algorithms to detect an object of a certain colour, but here you need to work the other way around and actually establish the colour of the object instead, so you will have to reverse the logic a bit. </p> <p>You'll then need to explore the arena for the target object. Remember that both the start zone and the target object will share the same colour, so take care not to detect the start zone as the beacon! Odometry might be useful here to inform your robot of where it started from so that it knows to rule out anything in that vicinity as a potential target. </p> <p>Having located the target object within the environment your robot will then need to move towards it and stop within the allocated stop zone. This technique is known as Beaconing, and we talked about some strategies for this in COM2009 Lecture 8. Perhaps you could consider an implementation of Braitenberg's Vehicle 3a as a way to control your robot's trajectory and approach to the target object?</p> <p>The concept of Visual Homing (also discussed in COM2009 Lecture 8) might also be worth considering as a method to control the position and trajectory of a robot based on images from its camera. Remember that the robot's camera can also infer depth, and the LiDAR sensor provides this sort of information too. There is therefore a wealth of information available here to help with this.</p>"},{"location":"com2009/la2/task4/#details","title":"Details","text":"<p>The arena used for this task will be 5.0 m x 5.0 m again and the beacons that you'll be searching for will be coloured boxes or cylinders, all between 200 mm and 400 mm in height. The Stop Zone surrounding each beacon will be 500 mm greater than the beacon's dimensions in the <code>X</code> and <code>Y</code> axis.</p> <p>There are only six possible target colours that will be used in this task, so your ROS node(s) will only need to accommodate these. The colours are listed below, and there is also a simulated environment in the <code>com2009_simulations</code> package called <code>beacon_colours</code> to illustrate these too.</p> <p> <p> </p> The range of possible beacon colours that could be used in this task. <p> </p> <p>As for the task itself:</p> <ol> <li>Your robot will first need to determine the \"Target Colour\" by analysing the Start Zone that it has been placed in within the simulated arena.</li> <li>The arena will contain three Start Zones, each of a different colour, and your robot could be launched into any one of these (selected at random).</li> <li> <p>Once the colour of the start zone has been determined by the robot a message must be printed to the terminal to indicate which beacon colour will be targetted. This terminal message must be formatted as follows:</p> <p></p> <pre><code>SEARCH INITIATED: The target beacon colour is {}.\n</code></pre> <p>Where <code>{}</code> is replaced by the name of the target colour as defined in the table inset in the figure above.</p> </li> <li> <p>The robot then needs to navigate the arena, avoiding contact with any of the objects that are located within it whilst searching for the beacon of the correct colour.</p> </li> <li> <p>Once the target beacon has been detected, a message must be printed to the terminal to indicate that this has happened. The terminal message needs to be clearly visible and readable, and the robot must be facing the target beacon when it is printed. This terminal message should be formatted as follows:</p> <p></p> <pre><code>TARGET DETECTED: Beaconing initiated.\n</code></pre> </li> <li> <p>The robot then needs to start moving towards the beacon, stopping when it is close enough to be within the stop zone surrounding it, but not close enough to actually make contact. As discussed above, the stop zone surrounding each object will be 500 mm greater than the beacon dimensions in the <code>X-Y</code> plane.</p> </li> <li> <p>A further message must be printed to the terminal to indicate that the robot has successfully and intentionally stopped within the designated area. This terminal message should be formatted as follows:</p> <p></p> <pre><code>BEACONING COMPLETE: The robot has now stopped.\n</code></pre> </li> <li> <p>The robot will have a maximum of 90 seconds to complete this task.</p> </li> <li> <p>Your team's ROS package must contain a launch file called <code>task4.launch</code>, such that the functionality that you develop for Task 4 can be launched from your package via the command:</p> <pre><code>roslaunch com2009_team{} task4.launch\n</code></pre> <p>As before, your robot will already have been launched into the simulated environment before the teaching team attempt to execute your launch file.</p> </li> </ol>"},{"location":"com2009/la2/task4/#simulation-resources","title":"Simulation Resources","text":"<p>Within the <code>com2009_simulations</code> package there is an environment called <code>beaconing</code>, which can be used to develop and test out your ROS node(s) for this task. </p> <p>The arena contains three start zones: A, B &amp; C; each of a different colour, as well as a number of uniquely coloured beacons.  There is one beacon in the arena to match each of the three start zones, plus a couple more to act as red herrings! </p> <p> <p> </p> The beaconing arena for Task 4. <p></p> <p>You can launch the robot in any of the three start zones by using <code>roslaunch</code> as follows:</p> <pre><code>roslaunch com2009_simulations beaconing.launch start_zone:={}\n</code></pre> <p>...where <code>{}</code> can be replaced with either <code>a</code>, <code>b</code> or <code>c</code> to select the start zone that you want the robot to be located in when the simulation launches. You can therefore develop and test out your beaconing algorithms in three unique scenarios.</p> <p>Note</p> <ol> <li>The same arena will be used to assess your submission for this task.</li> <li>The colour of the start zones and beacons will change, but the shape, size and location of all the objects will stay the same.</li> <li>Once again, the start zone that your robot is launched in for the assessment will be selected at random.</li> </ol>"},{"location":"com2009/la2/task4/#marking","title":"Marking","text":"<p>There are 15 marks available for this task in total, awarded according to the criteria below. No partial credit will be awarded unless specifically stated against any of the criteria.</p> <p> Criteria Marks Details A: Identifying the target colour 2/15 Whilst the robot is still located within the start zone a ROS node within your package must print a message to the terminal to indicate the target colour that has been determined and that will subsequently be used to identify the target beacon. You will receive the full marks available here provided the terminal message is presented and formatted as specified here. B: Detecting the correct beacon 3/15 You will receive the full marks available here for ensuring a message is printed to the terminal to indicate that the target beacon has been identified within the environment. The terminal message must be formatted as specified here, and the robot must be looking directly at the beacon when this message is printed. C: Stopping in the correct stop zone 5/15 Your robot must stop inside the correct stop zone within the 90-second time limit and a message must be printed to the terminal to indicate that this has been done intentionally. You will receive the full marks available here provided this is achieved successfully, and the terminal message is formatted as specified here. If your robot manages to stop, but part of its body lies outside the stop zone then you will be awarded half-marks. D: An \"incident-free-run\" 5/15 If your robot completes the task (or the 90 seconds elapses) without it making contact with anything in the arena then you will be awarded the maximum marks here. Marks will be deducted for any contact made, to a minimum of 0 (i.e. no negative marking). Your robot must be moving within the arena continually to be eligible for these marks though, simply turning on the spot for 90 seconds is not enough! <p></p>"},{"location":"com2009/la2/task5/","title":"Task 5: Real-World Exploration","text":"<p>Develop the ROS node(s) that allow a real TurtleBot3 Waffle to autonomously explore the robot arena in Computer Room 3, navigating through a series of rooms as quickly as possible, documenting its exploration with a photo of a beacon and a map of the environment as it goes! </p> <p>Assessment Format: Real Robots Marks: 35/100</p>"},{"location":"com2009/la2/task5/#summary","title":"Summary","text":"<p>This final task combines a bit of everything that you have done in the previous ones. </p> <p>The task is to make a real TurtleBot3 Waffle explore the DIA-CR3 robot arena, which will contain a series of \"rooms\" each with a coloured, cylindrical beacon in it. The main aim is to safely explore each of the rooms in the shortest time possible (emphasis on \"safely\" here, meaning you need to also try not to crash into anything in the process!) There are then some more advanced features that you will need to try to implement as well.</p>"},{"location":"com2009/la2/task5/#simulation-resources","title":"Simulation Resources","text":"<p>While you'll need to do this on a real robot, we've put together a simulation to help you develop your package functionality. This also helps to illustrate the nature of the task. You can launch the simulation from the <code>com2009_simulations</code> package with the following <code>roslaunch</code> command:</p> <pre><code>roslaunch com2009_simulations exploration.launch\n</code></pre> <p> <p> </p> The \"exploration\" development arena for Task 5. <p></p> <p>The Computer Room 3 robot arena might look something like this for the real assessment, where (similarly to the simulated environment) \"rooms\" will be constructed of wooden walls 180 mm tall, 10 mm thick and either 440 mm or 880 mm in length, and each room will contain a cylindrical beacon of 200 mm diameter and 250 mm height.</p> <p>Note</p> <p>The layout of the real robot arena will be different to the simulation for the assessment:</p> <ol> <li>Rooms will be different shapes and sizes and in different locations, but there will always be four of them.</li> <li>The robot might not necessarily be located at the same starting point as in the simulation; it could start anywhere.</li> <li>Beacons will be the same shape, size and colour as those in the simulation (yellow, red, green and blue). Detecting colours is a lot harder in the real-world than it is in simulation though, so you'll need to do a lot of testing on a real robot if you want to get this working robustly (you will have access to all the beacons during the lab sessions).</li> </ol>"},{"location":"com2009/la2/task5/#details","title":"Details","text":"<p>The robot will have 3 minutes (180 seconds) in total to complete this task.</p> <ol> <li>The arena floor will be marked out with 9 equal-sized zones, and (much like in Task 2) your robot will need to try to enter as many of these zones in the time available.</li> <li>In addition to this, the robot will need to try to explore the four rooms that will also be present in the arena. There will be marks available not only for the number of rooms that the robot manages to explore, but also the speed with which it manages to explore them all (see the marking section below for more details).</li> <li>Your robot will need to do this whilst minimising the number of contacts made with anything in the environment (\"incidents\"). Once an incident has taken place, we'll move the robot away slightly so that it is free to move again, but after five incidents have occurred the assessment will be stopped.</li> <li> <p>Your team's ROS package must contain a launch file named <code>task5.launch</code>,such that (for the assessment) we are able to launch all the nodes that you have developed for this task via the following command:</p> <pre><code>roslaunch com2009_team{} task5.launch\n</code></pre> </li> </ol> <p>Having developed the core functionality for the task, as above, you will then need to think about a couple of more advanced features...</p>"},{"location":"com2009/la2/task5/#advanced-feature-1-a-photo-of-a-beacon","title":"Advanced Feature 1: A photo of a beacon","text":"<p>As with all previous tasks, we will launch the ROS node(s) from within your package for this task using <code>roslaunch</code>. For this one however, we will also attempt to supply an additional argument when we make the command-line call:</p> <pre><code>roslaunch com2009_team{} task5.launch target_colour:={colour}\n</code></pre> <p>...where <code>{colour}</code> will be replaced with either <code>yellow</code>, <code>red</code>, <code>green</code> or <code>blue</code> (the target colour will be selected randomly). Based on this input, your robot will need to capture an image of the beacon in the arena of the same colour!</p> <p>Remember</p> <p>You should know from this the fact-finding mission that the camera image topic name is different on the real robot!</p> <p>The root of your package directory must contain a directory called <code>snaps</code>, and the image must be saved into this directory with the file name: <code>the_beacon.jpg</code>. </p> <p>You will therefore need to define your launch file to accommodate the <code>target_colour</code> command-line argument. In addition to this, inside your launch file you'll also need to pass the value of this to a ROS node within your package, so that the node knows which beacon to actually look for (i.e. your node needs to know whether to look for a yellow, red, green or blue beacon). We didn't actually cover this kind of launch file functionality in Lab Assignment #1, but there are a whole load of additional resources available in the Launch Files section of this course site, which should help you with this.</p> <p></p> <p>We will test whether your launch file has been correctly built to accept the <code>target_colour</code> command-line argument using autocomplete in the terminal. After typing the first four characters of the argument name, i.e.: <code>targ</code>, the rest of the name should be completed for us when we press the Tab key, as illustrated below: </p> <p> <p></p> <p></p> <p></p> <p>To illustrate that the value of the <code>target_colour</code> command-line argument has been correctly passed to a ROS Node within your package, you should configure your Task 5 Node (or any one of your nodes, if you have multiple) to print a message to the terminal as soon as it starts. The message should be formatted exactly as follows:</p> <pre><code>TASK 5 BEACON: The target is {colour}.\n</code></pre> <p>...where <code>{colour}</code> (including the curly brackets!) is replaced with the actual colour that was passed to your <code>task5.launch</code> file (<code>yellow</code>, <code>red</code>, <code>green</code> or <code>blue</code>). </p>"},{"location":"com2009/la2/task5/#advanced-feature-2-mapping-with-slam","title":"Advanced Feature 2: Mapping with SLAM","text":"<p>Marks are also available if, whilst your robot is completing the task, you can also run SLAM and generate a map of the environment in the background.</p> <p>In Week 3, Exercise 3 of Lab Assignment #1 we launched SLAM using the following <code>roslaunch</code> command:</p> <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch\n</code></pre> <p>Once you've had a look at the Launch Files section it should be clear how to launch other launch files from within your own. Alternatively, you could navigate to the <code>turtlebot3_slam</code> package in WSL-ROS and have a look at the content of the launch file that we executed from within this package for Exercise 3, to see how you might be able launch the same (or similar) functionality within your own <code>task5.launch</code> file.</p> <p>When it comes to saving the map that has been generated by SLAM, think about how we did this in the Week 3 Exercise. This involved calling a <code>map_saver</code> node from the command-line. It's possible, however, to call nodes (and indeed launch files too) from within other ROS nodes using the roslaunch Python API.</p> <p>The root of your package directory must contain a directory called <code>maps</code>, and the map file that you obtain must be saved into this directory with the name: <code>task5_map</code>.</p>"},{"location":"com2009/la2/task5/#marking","title":"Marking","text":""},{"location":"com2009/la2/task5/#criterion-a-arena-exploration","title":"Criterion A: Arena exploration","text":"<p>Marks: 8/35</p> <p>You will be awarded 1 mark for each zone of the arena that your robot manages to enter (excluding the one it starts in). As with Task 1, the robot only needs to enter each zone once, but its full body must be inside the zone marking to be awarded the associated mark.</p>"},{"location":"com2009/la2/task5/#criterion-b-room-exploration","title":"Criterion B: Room exploration","text":"<p>Marks: 10/35</p> <p>Marks will be awarded based on the maximum number of rooms that your robot manages to explore within the 180-second time limit and the speed by which it does this. The marks available per room explored will be awarded as follows:</p> <p> Time (seconds) 1 room 2 rooms 3 rooms 4 rooms 160-180 1.0 4.0 7.0 10.0 140-159 1.4 4.4 7.4 10.0 120-139 1.8 4.8 7.8 10.0 100-119 2.2 5.2 8.2 10.0 80-99 2.6 5.6 9.0 10.0 60-79 3.0 6.0 9.0 10.0 &lt;60 3.5 6.5 9.0 10.0 <p></p>"},{"location":"com2009/la2/task5/#criterion-c-an-incident-free-run","title":"Criterion C: An \"incident-free-run\"","text":"<p>Marks: 5/35</p> <p>Once again, if your robot completes the task (or the 180 seconds elapses) without it making contact with anything in the arena then you will be awarded the maximum marks here, with deductions then applied for each unique \"incident\" (to a minimum of 0/5). Your robot must at least leave the zone that it starts in to be eligible for these marks. Once five incidents have been recorded then the assessment will be stopped.</p>"},{"location":"com2009/la2/task5/#criterion-d-advanced-features","title":"Criterion D: Advanced Features","text":"<p>Advanced Feature 1: A Photo of a Beacon Marks: 7/35 </p> <p> Criteria Details Marks D.1.a Your <code>task5.launch</code> file accepts the <code>target_colour</code> command line argument. Assessed by checking for autocompletion, as discussed above. 1 D.1.b A message is printed to the terminal to indicate that the correct target colour has been passed to a node in your package. This must occur straight away on executing your launch file, and the message format must be exactly as specified here. 1 D.1.c At the end of the assessment a single image file, called <code>the_beacon.jpg</code>, has been obtained from the robot's camera (during the course of the assessment), and this is located in a folder called <code>snaps</code> at the root of your package directory i.e.: <code>com2009_team{}/snaps/the_beacon.jpg</code>. 1 D.1.d Your <code>com2009_team{}/snaps/the_beacon.jpg</code> image file contains any part of the correct beacon. 2 D.1.e Your <code>com2009_team{}/snaps/the_beacon.jpg</code> image file contains the correct beacon, where the entire width of the beacon can be observed. 2 <p></p> <p>Advanced Feature 2: Mapping with SLAM Marks: 5/35</p> <p> Criteria Details Marks D.2.a At the end of the assessment, a map of the robot arena (or any part of it) has been generated. Two files should be generated: a <code>.pgm</code> and a <code>.yaml</code>, which must both be called <code>task5_map</code>, and these files must be located in a <code>maps</code> folder at the root of your package directory i.e. <code>com2009_team{}/maps/task4_map.pgm</code>. 1 D.2.b Your <code>com2009_team{}/maps/task4_map.pgm</code> map that is created during the assessment depicts at least one of the rooms in the arena. 2 D.2.c Your <code>com2009_team{}/maps/task4_map.pgm</code> map that is created during the assessment depicts two or more of the rooms in the arena. 2 <p></p>"},{"location":"extras/launch-files/","title":"Launch Files","text":""},{"location":"extras/launch-files/#overview","title":"Overview","text":"<p>As you'll know by now, we execute Nodes on a ROS network using the <code>rosrun</code> command:</p> <pre><code>rosrun {package name} {script name}\n</code></pre> <p>Alternatively though, we also have the option of using <code>roslaunch</code>:</p> <pre><code>roslaunch {package name} {launch file}\n</code></pre> <p>We learnt way back in Week 1 that a launch file is an <code>xml</code> file with a <code>.launch</code> file extension. Inside this we can ask ROS to do a number of different things from one single <code>roslaunch</code> command-line call.</p> <p><code>roslaunch</code> offers a number of advantages over <code>rosrun</code>:</p> <ul> <li>Multiple nodes can be executed simultaneously.</li> <li><code>roslaunch</code> will launch the ROS Master for us, if isn't already running (so we don't have to manually call <code>roscore</code>).</li> <li>From within one <code>.launch</code> file, we can launch other <code>.launch</code> files.</li> <li>We can pass in additional arguments to launch certain things conditionally, or pass specific arguments to certain ROS nodes.</li> <li>As if that wasn't enough, there is also a <code>roslaunch</code> Python API, allowing us to launch nodes from within other (Python) nodes!</li> </ul> <p>In this section we'll talk about some of these advanced features that you may find useful.</p>"},{"location":"extras/launch-files/cla-to-node/","title":"Passing Command-line Arguments to Python Nodes via ROSlaunch","text":"<p>Having built a CLI for our Python Node on the previous page, we'll now look at how this all works when we call a node using <code>roslaunch</code> instead. </p> <ol> <li> <p>First, create a launch file called <code>publisher_cli.launch</code>. It probably makes sense to create this one inside your <code>week1_pubsub</code> package:</p> <p><pre><code>roscd week1_pubsub/launch\n</code></pre> <pre><code>touch publisher_cli.launch\n</code></pre></p> </li> <li> <p>Inside this add the following:</p> <pre><code>&lt;launch&gt;\n&lt;arg name=\"roslaunch_colour\" default=\"Black\" /&gt;\n&lt;arg name=\"roslaunch_number\" default=\"0.999\" /&gt;\n&lt;node pkg=\"tuos_ros_examples\" type=\"publisher_cli.py\"\nname=\"publisher_cli_node\" output=\"screen\"\nargs=\"-colour $(arg roslaunch_colour) -number $(arg roslaunch_number)\" /&gt;\n&lt;/launch&gt;\n</code></pre> <p>Here, we are specifying command-line arguments for the launch file using <code>&lt;arg&gt;</code> tags, as discussed earlier:</p> <pre><code>&lt;arg name=\"roslaunch_colour\" default=\"Black\" /&gt;\n&lt;arg name=\"roslaunch_number\" default=\"0.999\" /&gt;    </code></pre> <p>Next, we use a <code>&lt;node&gt;</code> tag to launch the <code>publisher_cli.py</code> node from the <code>tuos_ros_examples</code> package. All of that should be familiar to you from Week 1. What's new here however is the additional <code>args</code> attribute:</p> <pre><code>args=\"-colour $(arg roslaunch_colour) -number $(arg roslaunch_number)\"\n</code></pre> <p>This is how we pass the <code>roslaunch</code> command-line arguments into the CLI of our ROS Node:</p> <ul> <li><code>-colour</code> and <code>-number</code> are the command-line arguments of the Python Node</li> <li><code>roslaunch_colour</code> and <code>roslaunch_number</code> are the command-line arguments of the launch file</li> </ul> </li> <li> <p>Run the launch file without passing any arguments first, and see what happens:</p> <pre><code>roslaunch week1_pubsub publisher_cli.launch\n</code></pre> </li> <li> <p>Then try again, this time setting alternative values for the available command-line arguments:</p> <pre><code>roslaunch week1_pubsub publisher_cli.launch roslaunch_colour:=purple roslaunch_number:=4\n</code></pre> </li> </ol>"},{"location":"extras/launch-files/launching-launch-files/","title":"Launching Other Launch Files","text":"<p>Think back to the <code>move_circle.py</code> node that you created in Week 2. We need a simulation of our robot active in order to run this, which we can enable with the following command (which you should be very familiar with by now):</p> <pre><code>$ roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\n</code></pre> <p>Suppose you wanted to save yourself some work and launch both the simulation and the <code>move_circle.py</code> node at the same time...</p> <ol> <li> <p>Firstly, return to your <code>week2_navigation</code> package and create a <code>launch</code> directory (if you haven't done so previously):</p> <p><pre><code>roscd week2_navigation\n</code></pre> <pre><code>mkdir launch\n</code></pre> <pre><code>cd launch/\n</code></pre></p> </li> <li> <p>Then create a launch file. You could call this whatever you want, but for the purposes of this example we'll create one called <code>circle.launch</code>:</p> <pre><code>touch circle.launch\n</code></pre> </li> <li> <p>Inside this file add the following:</p> <pre><code>&lt;launch&gt;\n&lt;include file=\"$(find turtlebot3_gazebo)/launch/turtlebot3_empty_world.launch\" /&gt;\n&lt;node pkg=\"week2_navigation\" type=\"move_circle.py\" name=\"circle_node\" output=\"screen\" /&gt;\n&lt;/launch&gt;\n</code></pre> <p>The <code>&lt;node&gt;</code> tag should already be familiar to you, but the <code>&lt;include&gt;</code> tag before it might not be...</p> <p>This is what we use to launch other launch files. Here, we are locating the <code>turtlebot3_gazebo</code> package (using <code>find</code>), and asking for the <code>turtlebot3_empty_world.launch</code> file to be executed.</p> </li> <li> <p>From the command-line, execute your newly created launch file as follows:</p> <pre><code>roslaunch week2_navigation circle.launch\n</code></pre> <p>The TurtleBot3 Waffle will be launched in the \"empty world\" simulation, and the robot will start moving in a circle straight away!</p> </li> </ol>"},{"location":"extras/launch-files/python-clis/","title":"Building Command-Line Interfaces (CLIs) for Python Nodes","text":"<p>Suppose we have a very simple node such as the <code>publisher.py</code> node that you created in Week 1. This node publishes <code>std_msgs/String</code> type messages to a topic called <code>\"chatter\"</code>. Let's have a look at an alternative version of that node that takes in command-line arguments and publishes those to the <code>\"chatter\"</code> topic instead.</p> <p>Take a look at the <code>publisher_cli.py</code> node from the <code>tuos_ros_examples</code> package. </p> <p>Here, we use the Python <code>argparse</code> module to allow us to work with arguments that are passed to the node from the command-line:</p> <pre><code>import argparse\n</code></pre> <p>We instantiate <code>argparse</code> in the <code>__init__()</code> method of the <code>Publisher()</code> class to build a command-line interface (CLI) for the node:</p> <p><pre><code>cli = argparse.ArgumentParser(...)\ncli.add_argument(...)\n</code></pre> (See the code for more details)</p> <p>Arguments that we define with a <code>-</code> at the front will be optional, i.e. we don't have to provide these every time we run the node. We do, however, need to assign a default value for each optional argument in cases where no value is supplied, e.g.:</p> <pre><code>cli.add_argument(\n\"-colour\",\nmetavar=\"COL\",\ndefault=\"Blue\", \n...\n</code></pre> <p>The final step is to grab any arguments that are passed to this node when it is called. We use the <code>rospy.myargv()</code> method here, so that this works regardless of whether we call the node using <code>rosrun</code> or <code>roslaunch</code>: </p> <pre><code>self.args = cli.parse_args(rospy.myargv()[1:])\n</code></pre> <p>Having defined the CLI above, <code>argparse</code> then automatically generates help text for us! Try running the following command to see this in action:</p> <pre><code>rosrun tuos_ros_examples publisher_cli.py -h\n</code></pre> <p>Warning</p> <p>You need to have a ROS Master running in order for this to work, so do this in another terminal instance by running the <code>roscore</code> command.</p> <p>Run the node as it is (using <code>rosrun</code>) and see what happens:</p> <pre><code>rosrun tuos_ros_examples publisher_cli.py\n</code></pre> <p>Stop the node (using <code>Ctrl+C</code>) and then run it again, but this time providing a value for the <code>number</code> variable, via the CLI:</p> <pre><code>rosrun tuos_ros_examples publisher_cli.py -number 1.5\n</code></pre> Info <p>We can assign values to Python command-line arguments using a space (as above) or the <code>=</code> operator (<code>-number=1.5</code>). </p> <p>Stop the node again (using <code>Ctrl+C</code>) and also stop the ROS Master that you enabled in another terminal instance.</p>"},{"location":"extras/launch-files/roslaunch-api/","title":"Using the ROSlaunch Python API","text":"<p>The ROSlaunch API allows you to launch nodes (and other launch files) from inside your own nodes! The official documentation for the roslaunch API is the best place to go to find out how to use this, but here's a quick example, if you're short on time. </p> <p>Recall the SLAM Exercise from Week 3. Here we had SLAM running in the background, while we drove our robot around a simulated environment (using the <code>turtlebot3_teleop</code> node). Once we'd generated a full map we used a <code>rosrun</code> command (in the terminal) to save a copy of this map: </p> <pre><code>rosrun map_server map_saver -f {map name}\n</code></pre> <p>Suppose we wanted to do this programmatically instead...  </p> <p>The following is an example of a very simple ROS node that does just that. Once the code below is executed it will launch the <code>map_saver</code> node from the <code>map_server</code> package in exactly the same way as above (but using the ROSlaunch API instead). The key difference here is that instead of <code>{map name}</code>, we need to define the full file path for the map that we want to generate.</p> map_saver.py<pre><code>#!/usr/bin/env python3\nimport roslaunch\nimport rospy\nmap_path = \"path/to/my_map\"\nrospy.init_node(\"map_getter\", anonymous=True)\nlaunch = roslaunch.scriptapi.ROSLaunch()\nlaunch.start()\nprint(f\"Saving map at time: {rospy.get_time()}...\")\nnode = roslaunch.core.Node(package=\"map_server\",\nnode_type=\"map_saver\",\nargs=f\"-f {map_path}\")\nprocess = launch.launch(node)\n</code></pre> <p>You could wrap this into one of your own ROS nodes, or even build a standalone node inside your package to call the <code>map_saver</code> node when required. Perhaps you could even wrap the last three of the above commands inside a while loop and get the node to update a map file at regular intervals (say 5 seconds?) while your robot explores its environment...</p>"},{"location":"extras/launch-files/roslaunch-clas/","title":"ROSlaunch Command-line Arguments","text":"<p>A lot of the launch files that we have used throughout the lab course actually have Command-Line Arguments (CLAs) that can be (optionally) supplied when making the <code>roslaunch</code> call. Consider the <code>turtlebot3_empty_world.launch</code> file for instance...</p> <p>Enter the full command and then press the <code>SPACE</code> and <code>TAB</code> keys on your keyboard like so:</p> <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch[SPACE][TAB][TAB]\n</code></pre> <p>This will reveal the available command-line arguments for this launch file:</p> <pre><code>model  x_pos  y_pos  z_pos\n</code></pre> <p>We can therefore optionally specify custom values for these parameters in order to change what happens when the launch file is executed. Try this, for example:</p> <pre><code>roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch x_pos:=1 y_pos:=1\n</code></pre> <p>When the Gazebo simulation is launched, the robot will be located at coordinates <code>(1, 1)</code> in the <code>X-Y</code> plane, rather than <code>(0, 0)</code>, as would usually be the case.</p> <p>Note</p> <p>We assign values to <code>roslaunch</code> command-line arguments using the <code>:=</code> operator. </p> <p>This is all made possible through the use of <code>&lt;arg&gt;</code> tags at the start of a launch file (just after the opening <code>&lt;launch&gt;</code> tag):</p> <pre><code>&lt;launch&gt;\n&lt;arg name=\"x_pos\" default=\"0.0\"/&gt;\n&lt;arg name=\"y_pos\" default=\"0.0\"/&gt;\n...\n</code></pre> <p>Within these tags we need to define two attributes:</p> <ol> <li><code>name</code>: the name we want to give to the CLA (<code>name=\"x_pos\"</code>, <code>name=\"y_pos\"</code> etc...)</li> <li> <p><code>default</code>: A default value in order to make the CLA optional</p> <p>(In the above example, the robot will be located at coordinates <code>(0, 0)</code> by default, unless we specify otherwise.)</p> </li> </ol>"},{"location":"others/amr31001/","title":"Industry 4.0","text":"<p>As part of this module you will take part in two lab sessions in the Diamond, where you will learn about how ROS can be used to program and control robots. You'll do some Python programming and look at how sensor data can be used to control a robot's actions. </p> <ul> <li>Lab 1: Mobile Robotics </li> <li>Lab 2: Feedback Control</li> </ul>"},{"location":"others/amr31001/lab1/","title":"Lab 1: Mobile Robotics","text":"<p>Info</p> <p>You should be able to complete exercises 1-6 on this page within a two-hour lab session.</p>"},{"location":"others/amr31001/lab1/#introduction","title":"Introduction","text":"<p>In this first AMR31001 'Industry 4.0' ROS Lab you will learn how to use ROS (the Robot Operating System) to control a robot's motion.</p> <p>ROS is an open-source, industry-standard robot programming framework, used in a range of applications such as agriculture, warehouse and factory automation and advanced manufacturing (the robot arms at the AMRC, for instance, are programmed and controlled using ROS!) </p> <p>ROS allows us to programme robots using a range of different programming languages, but we'll be using Python for these labs. In addition to this, ROS runs on top of a Linux operating system called 'Ubuntu', and so we'll also learn a bit about how to use this too.</p> <p>We'll be working with robots called 'TurtleBot3 Waffles', which you can find out a bit more about here. </p> <p>Pre-Lab Work</p> <p>You must have completed the Pre-Lab Test before you can make a start on this lab. This is available on the AMR31001 Blackboard Course Page.</p>"},{"location":"others/amr31001/lab1/#aims","title":"Aims","text":"<p>In this lab you'll learn how to use ROS to make a robot move, and we'll also look at how to create our own basic ROS application (or 'Node'), using Python.</p>"},{"location":"others/amr31001/lab1/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Control a TurtleBot3 Waffle Robot, from a laptop, using ROS.</li> <li>Launch ROS applications on the laptop and the robot using <code>roslaunch</code> and <code>rosrun</code>.</li> <li>Interrogate a ROS network using ROS command-line tools.</li> <li>Use ROS Communication Methods to publish messages.</li> <li>Use a Linux operating system and work within a Linux Terminal.</li> </ol>"},{"location":"others/amr31001/lab1/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Launching ROS and Making the Robot Move</li> <li>Exercise 2: Seeing the Waffle's Sensors in Action!</li> <li>Exercise 3: Visualising the ROS Network</li> <li>Exercise 4: Exploring ROS Topics and Messages</li> <li>Exercise 5: Publishing Velocity Commands to the <code>/cmd_vel</code> Topic</li> <li>Exercise 6: Creating a Python node to make the robot move</li> <li>Exercise 7 (Advanced): Alternative Motion Paths</li> </ul>"},{"location":"others/amr31001/lab1/#the-lab","title":"The Lab","text":""},{"location":"others/amr31001/lab1/#getting-started","title":"Getting Started","text":"<p>Before you do anything, you'll need to get your robot up and running, and make sure ROS is launched.</p>"},{"location":"others/amr31001/lab1/#ex1","title":"Exercise 1: Launching ROS and Making the Robot Move","text":"<p>You should have already been provided with a Robot and a Laptop (in fact, you're probably already reading this on the laptop!) </p> <ol> <li> <p>First, identify the robot that you have been provided with. Robots are named as follows:</p> <p><pre><code>dia-waffleNUM\n</code></pre> ... where <code>NUM</code> is a unique 'Robot Number' (a number between 1 and 50). Check the label printed on top of the robot to find out which one you have!</p> </li> <li> <p>Open up a terminal instance on the laptop, either by using the <code>Ctrl+Alt+T</code> keyboard shortcut, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p> </p> <p>(we'll refer to this as TERMINAL 1).</p> </li> <li> <p>In the terminal type the following command to pair the laptop and robot, so that they can work together:</p> <p>TERMINAL 1: <pre><code>waffle NUM pair\n</code></pre> Replacing <code>NUM</code> with the number of the robot that you have been provided with.</p> </li> <li> <p>Enter the password for the robot when requested (we'll tell you what this is in the lab).</p> <p>You may see a message like this early on in the pairing process:</p> <p> </p> <p>If so, just type <code>yes</code> and then hit <code>Enter</code> to confirm that you want to continue.</p> </li> <li> <p>Once the pairing process is finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>Then, in the same terminal (TERMINAL 1), enter the following command:</p> <p>TERMINAL 1: <pre><code>waffle NUM term\n</code></pre> (again, replacing <code>NUM</code> with the number of your robot).</p> <p>Any text that was in the terminal should now disappear, and a green banner should appear across the bottom of the terminal window:</p> <p> </p> <p>This is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> <li> <p>Now, launch ROS on the robot by entering the following command:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_tb3_tools ros.launch\n</code></pre></p> <p>After a short while, you should see a message like this:</p> <pre><code>[INFO] [#####] Calibration End  \n</code></pre> <p>ROS is now up and running on the robot, and we're ready to go!</p> <p>You should leave TERMINAL 1 alone now, just leave it running in the background for the rest of the lab.</p> </li> <li> <p>Next, open up a new terminal instance on the laptop (by pressing <code>Ctrl+Alt+T</code> or clicking the Terminal App desktop icon, as you did before). We'll call this one TERMINAL 2.</p> </li> <li> <p>In TERMINAL 2 enter the following command:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n</code></pre></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around using specific buttons on the keyboard:</p> <p> </p> </li> <li> <p>Enter <code>Ctrl+C</code> in TERMINAL 2 to stop the Teleop node when you've had enough fun.</p> </li> </ol>"},{"location":"others/amr31001/lab1/#packages-and-nodes","title":"Packages and Nodes","text":"<p>ROS applications are organised into packages. Packages are basically folders containing scripts, configurations and launch files (ways to launch those scripts and configurations).  </p> <p>Scripts tell the robot what to do and how to act. In ROS, these scripts are called nodes. ROS Nodes are executable programs that perform specific robot tasks and operations. These are typically written in C++ or Python, but it's possible to write ROS Nodes using other programming languages too.</p> <p>In Exercise 1 you actually launched a whole range of different nodes on the ROS Network (the wireless link between your robot and laptop) using just two commands: </p> <ol> <li><code>roslaunch tuos_tb3_tools ros.launch</code> (on the robot, in TERMINAL 1).</li> <li><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch</code> (on the laptop, in TERMINAL 2).</li> </ol> <p>These are <code>roslaunch</code> commands, and they have two parts to them (after the <code>roslaunch</code> bit):</p> <pre><code>roslaunch {[1] Package name} {[2] Launch file}\n</code></pre> <p>Part [1] specifies the name of the ROS package containing the functionality that we want to execute. Part [2] is a file within that package that tells ROS exactly what scripts ('nodes') that we want to launch.</p> <p>Post-lab Quiz</p> <p>What were the names of the two packages that we invoked in Exercise 1?</p>"},{"location":"others/amr31001/lab1/#ex2","title":"Exercise 2: Seeing the Waffle's Sensors in Action!","text":"<p>Our Waffles have some pretty sophisticated sensors on them, allowing them to \"see\" the world around them. We won't really make much use of these during this lab, but this next exercise will allow you to see how the data from these devices could be used to help our robots do some very advanced things (with some clever programming, of course!)</p>"},{"location":"others/amr31001/lab1/#part-1-the-camera","title":"Part 1: The Camera","text":"<ol> <li> <p>There shouldn't be anything running in TERMINAL 2 now, after you closed down the Teleop node at the end of the previous exercise (<code>Ctrl+C</code>). Return to this terminal and launch the <code>rqt_image_view</code> node:</p> <p>TERMINAL 2: <pre><code>rosrun rqt_image_view rqt_image_view\n</code></pre></p> <p>Notice how we're using the <code>rosrun</code> command here, instead of <code>roslaunch</code>? We use <code>rosrun</code> if we only want to launch a single node (like the <code>rqt_image_view</code> node) on the ROS network. <code>rosrun</code> has a similar two-part format to <code>roslaunch</code>:</p> <pre><code>rosrun {[1] Package name} {[2] Node name}\n</code></pre> <p>Post-lab Quiz</p> <p>Why did we have to type <code>rqt_image_view</code> twice?</p> </li> <li> <p>A new window should open. Maximise this (if it isn't already) and then select <code>/camera/color/image_raw</code> from the dropdown menu at the top-left of the application window.</p> </li> <li>Live images from the robot's camera should now be visible! Stick your face in front of the camera and see yourself appear on the laptop screen!</li> <li> <p>Close down the window once you've had enough. This should release TERMINAL 2 so that you can enter commands in it again.</p> <p>The camera on the robot is an Intel RealSense D435, and it's actually quite a clever device. Inside the unit is two separate camera sensors, giving it - effectively - both a left and right eye. The device then combines the data from both of these sensors and uses the combined information to infer depth from the images as well. Let's have a look at that in action now...</p> </li> <li> <p>In TERMINAL 2 enter the following command:</p> <p>TERMINAL 2: <pre><code>roslaunch tuos_tb3_tools rviz.launch\n</code></pre></p> <p>This will launch an application called RViz, which is a handy tool that allows us to visualise the data from all the sensors on-board our robots. When RViz opens, you should see something similar to the following:</p> <p> </p> <p>The strange wobbly sheet of colours in front of the robot is the live image stream from the camera with depth applied to it at the same time. The camera is able to determine how far away each image pixel is from the camera lens, and then uses that to generate this 3-dimensional representation. Nice eh!</p> </li> <li> <p>Again, place your hand or your face in front of the camera and hold steady for a few seconds (there may be a bit of a lag as all of this data is transmitted over the WiFi network). You should see yourself rendered in 3D in front of the robot! </p> </li> </ol>"},{"location":"others/amr31001/lab1/#part-2-the-lidar-sensor","title":"Part 2: The LiDAR Sensor","text":"<p>In RViz you may have also noticed a lot of red dots scattered around the robot. This is a representation of the laser displacement data coming from the LiDAR sensor (the black device on the top of the robot). The LiDAR sensor spins continuously, sending out laser pulses into the environment as it does so. When a pulse hits an object it is reflected back to the sensor, and the time it takes for this to happen is used to calculate how far away the object is.</p> <p>The LiDAR sensor spins and performs this process continuously, so a full 360\u00b0 scan of the environment can be generated. This data is therefore really useful for things like obstacle avoidance and mapping. We'll have a quick look at the latter now.</p> <ol> <li> <p>Close down RViz (click the \"Close without saving\" button, if asked).</p> </li> <li> <p>Head back to TERMINAL 2 and run the following command:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch\n</code></pre></p> <p>A new RViz screen will open up, this time showing the robot from a top-down view, and with the LiDAR data represented by green dots instead.</p> <p> </p> <p>Underneath the green dots you should notice black lines forming. ROS is using a process called SLAM (Simultaneous Localisation and Mapping) to generate a map of the environment, using the data from the LiDAR sensor.</p> </li> <li> <p>Open up a new terminal instance now, we'll call this one TERMINAL 3. Launch the Teleop node in this one, as you did earlier:</p> <p>TERMINAL 3: <pre><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n</code></pre></p> </li> <li> <p>Drive the robot around a bit and watch how the map in RViz is updated as the robot explores new parts of the environment.</p> </li> <li> <p>Enter <code>Ctrl+C</code> in TERMINAL 3 and then close down this terminal window, we won't need it anymore.</p> </li> <li> <p>Close down the RViz window, but keep TERMINAL 2 open for the next exercise...</p> </li> </ol> <p>We've now used both <code>roslaunch</code> and <code>rosrun</code> to launch ROS applications. These are both ROS command-line tools, and there are many others at our disposal as robotics engineers as well. </p> <p>Using <code>rosrun</code> and <code>roslaunch</code>, as we have done so far, it's easy to end up with a whole load of different processes or ROS Nodes running on the network, some of which we will interact with, but others may just be running in the background. It is often useful to know exactly what is running on the ROS network, and there are a number of ways to do this.</p>"},{"location":"others/amr31001/lab1/#ex3","title":"Exercise 3: Visualising the ROS Network","text":"<ol> <li> <p>There shouldn't be anything running in TERMINAL 2 now, so return to this terminal and use the <code>rosnode</code> command to list the nodes that are currently running on the robot:</p> <p>TERMINAL 2: <pre><code>rosnode list\n</code></pre></p> <p>You should see a list of 7 items.</p> </li> <li> <p>We can visualise the connections between the active nodes by using a ROS node called <code>rqt_graph</code>. Launch this as follows:</p> <p>TERMINAL 2: <pre><code>rosrun rqt_graph rqt_graph\n</code></pre></p> </li> <li> <p>In the window that opens, select <code>Nodes/Topics (active)</code> from the dropdown menu in the top left. </p> <p>What you should then see is a map of all the nodes in the list from above (as ovals), and arrows to illustrate the flow of information between them. This is a visual representation of the ROS network!</p> <p>Items that have a rectangular border are ROS Topics. ROS Topics are essentially communication channels, and ROS nodes can read (subscribe) or write (publish) to these topics to access sensor data, pass information around the network and make things happen.</p> </li> </ol> <p>A ROS Robot could have hundreds of individual nodes running simultaneously to carry out all its necessary operations and actions. Each node runs independently, but uses ROS communication methods to communicate and share data with the other nodes on the ROS Network.</p>"},{"location":"others/amr31001/lab1/#publishers-and-subscribers-a-ros-communication-method","title":"Publishers and Subscribers: A ROS Communication Method","text":"<p>ROS Topics are key to making things happen on a robot. Nodes can publish (write) and/or subscribe to (read) ROS Topics in order to share data around the ROS network. Data is published to topics using ROS Messages. We were actually publishing messages to a topic when we made the robot move using the Teleop node in the previous exercises.</p> <p>Let's have a look at this in a bit more detail...</p>"},{"location":"others/amr31001/lab1/#ex4","title":"Exercise 4: Exploring ROS Topics and Messages","text":"<p>Much like the <code>rosnode list</code> command, we can use <code>rostopic list</code> to list all the topics that are currently active on the network.</p> <ol> <li> <p>Close down the <code>rqt_graph</code> window if you haven't done so already. This will release TERMINAL 2 so that we can enter commands in it again. Return to this terminal window and enter the following:</p> <p>TERMINAL 2: <pre><code>rostopic list\n</code></pre></p> <p>A much larger list of items should be printed to the terminal now. See if you can spot the <code>/cmd_vel</code> item in the list.</p> <p>This topic is used to control the velocity of the robot ('command velocity').</p> </li> <li> <p>Let's find out more about this using the <code>rostopic info</code> command.</p> <p>TERMINAL 2: <pre><code>rostopic info /cmd_vel\n</code></pre></p> <p>This should provide an output similar to the following: </p> <pre><code>Type: geometry_msgs/Twist\n\nPublishers: None\n\nSubscribers:\n * /turtlebot3_core (http://dia-waffle[NUM]:#####/)\n</code></pre> <p>This tells us a few things: </p> <ol> <li>The <code>/cmd_vel</code> topic currently has no publishers (i.e. no other nodes are currently writing data to this topic).</li> <li>The <code>/turtlebot3_core</code> node is subscribing to the topic. The <code>/turtlebot3_core</code> node turns motor commands into actual wheel motion, so it monitors the topic (i.e. subscribes to it) to see when a velocity command is published to it.</li> <li> <p>The type of message used by the <code>/cmd_vel</code> topic is called: <code>geometry_msgs/Twist</code>. </p> <p>The message type has two parts: <code>geometry_msgs</code> and <code>Twist</code>. <code>geometry_msgs</code> is the name of the ROS package that this message belongs to and <code>Twist</code> is the actual message type. </p> <p>We have just learnt then, that if we want to make the robot move we need to publish <code>Twist</code> messages to the <code>/cmd_vel</code> topic. </p> </li> </ol> </li> <li> <p>We can use the <code>rosmsg</code> command to find out more about the <code>Twist</code> message:</p> <p>TERMINAL 2: <pre><code>rosmsg info geometry_msgs/Twist\n</code></pre></p> <p>From this, we should obtain the following:</p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x\n  float64 y\n  float64 z\ngeometry_msgs/Vector3 angular\n  float64 x\n  float64 y\n  float64 z\n</code></pre> <p>Hmmm, this looks complicated. Let's find out what it all means...</p> </li> </ol>"},{"location":"others/amr31001/lab1/#velocity-control","title":"Velocity Control","text":"<p>The motion of any mobile robot can be defined in terms of its three principal axes: <code>X</code>, <code>Y</code> and <code>Z</code>. In the context of our TurtleBot3 Waffle, these axes (and the motion about them) are defined as follows:</p> <p></p> <p>In theory then, a robot can move linearly or angularly about any of these three axes, as shown by the arrows in the figure. That's six Degrees of Freedom (DOFs) in total, achieved based on a robot's design and the actuators it is equipped with. Take a look back at the <code>rosmsg info</code> output in TERMINAL 2. Hopefully it's a bit clearer now that these topic messages are formatted to give a ROS Programmer the ability to ask a robot to move in any one of its six DOFs. </p> <pre><code>geometry_msgs/Vector3 linear\n  float64 x  &lt;-- Forwards (or Backwards)\n  float64 y  &lt;-- Left (or Right)\n  float64 z  &lt;-- Up (or Down)\ngeometry_msgs/Vector3 angular\n  float64 x  &lt;-- \"Roll\"\n  float64 y  &lt;-- \"Pitch\"\n  float64 z  &lt;-- \"Yaw\"\n</code></pre> <p>Our TurtleBot3 robot only has two motors, so it doesn't actually have six DOFs! These two motors can be controlled independently, which gives it what is called a \"differential drive\" configuration, but this still only allows it to move with two degrees of freedom in total, as illustrated below.</p> <p></p> <p>It can therefore only move linearly in the x-axis (Forwards/Backwards) and angularly in the z-axis (Yaw). </p> <p>Post-lab Quiz</p> <p>Take note of all this, there may be a question on it!</p>"},{"location":"others/amr31001/lab1/#ex5","title":"Exercise 5: Publishing Velocity Commands to the <code>/cmd_vel</code> Topic","text":"<p>We will use the <code>rostopic</code> command differently now, and actually publish messages from the terminal to make the robot move. In order to do this we need three key bits of information:</p> <ul> <li>The name of the topic that we want to publish to.</li> <li>The type of message that this topic uses.</li> <li>The data format that this message type uses.</li> </ul> <p>Post-lab Quiz</p> <p>We discovered all this in the previous exercise, take note of all three points.</p> <p>We can then use <code>rostopic</code> with the <code>pub</code> option as follows:</p> <pre><code>rostopic pub {topic_name} {message_type} {data}\n</code></pre> <p>As we discovered earlier, the <code>/cmd_vel</code> topic is expecting linear and angular data, each with an <code>x</code>, <code>y</code> and <code>z</code> component to represent a maximum of six DOFs. We have also established however, that our robot only actually has two DOFs, so a number of the <code>Twist</code> message components won't actually have any effect on our robot at all.</p> <p>In any case, the message that we need to publish will end up being quite long because of all the message parameters that need to be provided (regardless of how relevant they are to our own robots). Fortunately, we don't have to type the whole thing out manually though, and we can use the autocomplete functionality of the Linux terminal to do most of the work for us.</p> <ol> <li> <p>Enter the following into TERMINAL 2 but where it says <code>[Space]</code> or <code>[Tab]</code> actually press the corresponding key on the keyboard:</p> <p>TERMINAL 2: <pre><code>rostopic pub /cmd_vel[Space][Tab][Tab]\n</code></pre></p> <p>The full message should then be presented to us:</p> <pre><code>$ rostopic pub /cmd_vel geometry_msgs/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre> </li> <li> <p>First, you need to add the following text to the end of the message: <code>--rate=10</code>.</p> </li> <li> <p>Then, you can scroll back through the message and edit any of the <code>linear</code>/<code>angular</code> <code>x</code>/<code>y</code>/<code>z</code> values as appropriate. Do this by pressing the \u2190 key on your keyboard, deleting a <code>0.0</code> where appropriate and replacing it with a value of your choosing. In its final format, a message might (for example) look like this:</p> <p><pre><code>rostopic pub /cmd_vel geometry_msgs/Twist \"linear:\n  x: 0.0\n  y: 101.1\n  z: 0.0\nangular:\n  x: 22.8\n  y: 0.0\n  z: 0.0\" --rate=10\n</code></pre> 1. Using what you learnt above about the way your robot can actually move, change one of the message parameter values in order to make the robot rotate on the spot. Before you do this, it's worth noting the following things: 1. The unit of linear velocity is meters per second (m/s). 1. The unit of angular velocity is radians per second (rad/s). 1. Our TurtleBot3 robots can move with a maximum linear velocity of 0.26 m/s and a maximum angular velocity of 1.82 rad/s.</p> </li> <li> <p>Once you've edited the message hit <code>Enter</code> to publish this to the <code>/cmd_vel</code> topic and observe what your robot does!</p> </li> <li> <p>Enter <code>Ctrl+C</code> in TERMINAL 2 to stop the <code>rostopic pub</code> process (which will make the robot stop moving too).</p> </li> <li> <p>Next, find a velocity command that makes the robot move forwards. (Don't forget to press <code>Ctrl+C</code> afterwards.)</p> </li> <li> <p>Finally, enter a velocity command to make the robot move in a circle (you may need to change two parameter values here).</p> </li> <li> <p>Enter <code>Ctrl+C</code> in TERMINAL 2 to stop the robot.</p> </li> </ol>"},{"location":"others/amr31001/lab1/#ex6","title":"Exercise 6: Creating a Python node to make the robot move","text":"<p>Hopefully you can see now that, in order to make a robot move, it's simply a case of publishing the right ROS Message (<code>Twist</code>) to the right ROS Topic (<code>/cmd_vel</code>). Earlier on in the lab we used the Keyboard Teleop node to drive the robot around, a bit like a remote control car. In the background here all that was really happening was that the Teleop node was converting our keyboard button presses into velocity commands and publishing these to the <code>/cmd_vel</code> topic. In the previous exercise we looked at this in more detail by actually formatting the raw <code>Twist</code> messages ourselves, and publishing these to <code>/cmd_vel</code>, directly from the command-line. This approach was very manual though, and there's a limit to what we can really achieve by working in this way (circular and straight line motion is about it!)</p> <p>In reality, robots need to be able to move around complex environments autonomously, which is quite a difficult task, and requires us to build bespoke applications. We can build these applications using Python, and we'll look at the core concepts behind this now, building a simple Node that will allow us to make our robot a bit more \"autonomous\". What we will do here forms the basis of the more complex approaches used by robotics engineers to really bring robots to life!</p> <ol> <li> <p>First, create your own folder on the laptop to store your Python script(s) in. A folder is also known as a \"directory\", and we can make a new one from the command-line by using the <code>mkdir</code> (\"make directory\") command:</p> <p>TERMINAL 2: <pre><code>mkdir -p ~/amr31001/your_name </code></pre> Replacing <code>your_name</code> with, well... your name!</p> <p>Tip</p> <p>Don't use any spaces in your name, use underscores (<code>_</code>) instead!</p> </li> <li> <p>Next, navigate into this directory using the <code>cd</code> (\"change directory\") command:</p> <p>TERMINAL 2: <pre><code>cd ~/amr31001/your_name/ </code></pre> Again, replacing <code>your_name</code> accordingly.</p> </li> <li> <p>Then create a Python file called <code>move_square.py</code> using the <code>touch</code> command:</p> <p>TERMINAL 2: <pre><code>touch move_square.py\n</code></pre></p> </li> <li> <p>Now we want to edit it, and we'll do that using Visual Studio Code (VS Code):</p> <p>TERMINAL 2: <pre><code>code .\n</code></pre></p> <p>Note</p> <p>Don't forget to include the <code>.</code>, it's important!!</p> </li> <li> <p>Once VS Code launches, open up your <code>move_square.py</code> file, which should be visible in the file explorer on the left-hand side of the VS Code window. Paste the following content into the file:</p> move_square.py<pre><code>import rospy\nfrom geometry_msgs.msg import Twist\nnode_name = \"move_waffle\"\nmovement = \"state1\" # \"state2, state3 etc...\"\ntransition = True\nrospy.init_node(node_name, anonymous=True)\nrate = rospy.Rate(10) # hz\npub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)\nvel = Twist()\nrospy.loginfo(f\"The {node_name} node has been initialised...\")\ntimestamp = rospy.get_time()\nwhile not rospy.is_shutdown():\nelapsed_time = rospy.get_time() - timestamp\nif transition:\ntimestamp = rospy.get_time()\ntransition = False\nvel.linear.x = 0.0\nvel.angular.z = 0.0\nprint(f\"Moving to state: {movement}\")\nelif movement == \"state1\":\nif elapsed_time &gt; 2:\nmovement = \"state2\"\ntransition = True\nelse:\nvel.linear.x = 0.05\nvel.angular.z = 0.0\nelif movement == \"state2\":\nif elapsed_time &gt; 4:\nmovement = \"state1\"\ntransition = True\nelse:\nvel.angular.z = 0.2\nvel.linear.x = 0.0\npub.publish(vel)\nrate.sleep()\n</code></pre> </li> <li> <p>Now, go back to TERMINAL 2 and run the code.</p> <p>Note</p> <p>Make sure the robot is on the floor and has enough room to roam around before you do this!</p> <p>TERMINAL 2: <pre><code>python3 move_square.py\n</code></pre></p> <p>Observe what the robot does. When you've seen enough, enter <code>Ctrl+C</code> in TERMINAL 2 to stop the node from running, which should also stop the robot from moving.</p> </li> <li> <p>As the name may suggest, the aim here is to make the robot follow a square motion path. What you may have observed when you actually ran the code is that the robot doesn't actually do that! We're using a time-based approach to make the robot switch between two different states continuously: moving forwards and turning on the spot.</p> <p>Have a look at the code to work out how much time the robot will currently spend in each state.</p> </li> <li> <p>The aim here is to make the robot follow a 0.5m x 0.5m square motion path.  In order to properly achieve this you'll need to adjust the timings, or the robot's velocity, or both. Edit the code so that the robot actually follows a 0.5m x 0.5m square motion path!</p> </li> </ol>"},{"location":"others/amr31001/lab1/#ex7","title":"Exercise 7 (Advanced): Alternative Motion Paths","text":"<p>If you have time, why don't you have a go at this now...</p> <p>How could you adapt the code further to achieve some more interesting motion profiles?</p> <ol> <li> <p>First, make a copy of the <code>move_square.py</code> code using the <code>cp</code> command:</p> <p>TERMINAL 2: <pre><code>cp move_square.py move_alt.py\n</code></pre> Which will create a new version of the file called <code>move_alt.py</code></p> </li> <li> <p>See if you can modify the <code>move_alt.py</code> code to achieve either of the more complex motion profiles illustrated below.</p> <p> </p> <ol> <li>Profile (a): The robot needs to follow a figure-of-eight shaped path, where a linear and angular velocity command are set simultaneously to generate circular motion. Velocities will need to be defined in order to achieve a path diameter of 1m for each of the two loops. Having set the velocities appropriately, you'll then need to work out how long it would take the robot to complete each loop, so that you can determine when the robot should have got back to its starting point. At this point you'll need to change the turn direction, so that the robot switches from anti-clockwise to clockwise turning. </li> <li>Profile (b): The robot needs to start and end in the same position, but move through intermediate points 1-7, in sequence, to generate the stacked square profile as shown. Each of the two squares must be 1m x 1m in size, so you'll need to find the right velocity and duration pairs for moving forward and turning. You'll also need to change the turn direction once the robot reaches Point 3, and then again at Point 7!</li> </ol> </li> </ol>"},{"location":"others/amr31001/lab1/#wrapping-up","title":"Wrapping Up","text":"<p>Before you leave, please shut down your robot! Enter the following command in TERMINAL 2 to do so:</p> <p>TERMINAL 2: <pre><code>waffle NUM off\n</code></pre> ... again, replacing <code>NUM</code> with the number of the robot that you have been working with today.</p> <p>You'll need to enter <code>y</code> and then hit <code>Enter</code> to confirm this.</p> <p>Please then shut down the laptop, which you can do by clicking the battery icon in the top right of the desktop and selecting the \"Power Off / Log Out\" option in the drop-down menu.</p> <p></p> <p> <p>AMR31001 Lab 1 Complete! See you again next year for Lab 2!</p> <p></p>"},{"location":"others/amr31001/lab2/","title":"Lab 2: Feedback Control","text":""},{"location":"others/amr31001/lab2/#introduction","title":"Introduction","text":"<p>In Lab 1 we explored how ROS works and how to bring a robot to life. Let's quickly recap the key points:</p> <p>ROS Nodes</p> <ul> <li>Are executable programs (Python, C++ scripts) that perform specific robot tasks and operations.</li> <li>Typically, there'll be many ROS Nodes running on a robot simultaneously in order to make it work.</li> <li>We can create our own Nodes on top of what's already running, to add extra functionality.</li> <li>You may recall that we created our own ROS Node in Python, to make our TurtleBot3 Waffle follow a square motion path.</li> </ul> <p></p> <p>Topics and Messages</p> <ul> <li>All the ROS Nodes running on a network can communicate and pass data between one another using a Publisher/Subscriber-based Communication Principle.</li> <li>ROS Topics are key to this - they are essentially the communication channels (or the plumbing) on which all data is passed around between the nodes.</li> <li>Different topics communicate different types of information.</li> <li>Any Node can publish (write) and/or subscribe to (read) any ROS Topic in order to pass information around or make things happen.</li> <li>One of the key ROS Topics that we worked with last time was <code>/cmd_vel</code>, which is a topic that communicates velocity commands to make a robot move.</li> <li>We published <code>Twist</code> messages to this (both via the command line, and in Python) to make our TurtleBot3 Waffle move.</li> </ul> <p></p> <p>Open-Loop Control</p> <p>We used a time-based method to control the motion of our robot in order to get it to generate a square motion path. This type of control is open-loop: we hoped that the robot had moved (or turned) by the amount that was required, but had no feedback to tell us whether this had actually been achieved.</p> <p>In this lab we'll look at how this can be improved, making use of some of our robot's on-board sensors to tell us where the robot is or what it can see in its environment, in order to complete a task more reliably and be able to better adapt to changes and uncertainty in the environment.</p>"},{"location":"others/amr31001/lab2/#aims","title":"Aims","text":"<p>In this lab, we'll build some ROS Nodes (in Python) that incorporate data from some of our robot's sensors. This sensor data is published to specific topics on the ROS Network, and we can build ROS Nodes to subscribe to these. We'll see how the data from these sensors can be used as feedback to inform decision-making, thus allowing us to implement some different forms of closed-loop control, making our robot more autonomous.</p>"},{"location":"others/amr31001/lab2/#intended-learning-outcomes","title":"Intended Learning Outcomes","text":"<p>By the end of this session you will be able to:</p> <ol> <li>Interpret the data from a ROS Robot's Odometry System and understand what this tells you about a Robot's position and orientation within its environment.</li> <li>Use feedback from a robot's odometry system to control its position in an environment.</li> <li>Use data from a Robot's LiDAR sensor to make a robot follow a wall.</li> <li>Generate a map of an environment, using SLAM.</li> <li>Make a robot navigate an environment autonomously, using ROS navigation tools.</li> </ol>"},{"location":"others/amr31001/lab2/#quick-links","title":"Quick Links","text":"<ul> <li>Exercise 1: Exploring Odometry Data</li> <li>Exercise 2: Odometry-based Navigation</li> <li>Exercise 3: Wall following</li> <li>Exercise 4: SLAM and Autonomous Navigation</li> </ul>"},{"location":"others/amr31001/lab2/#the-lab","title":"The Lab","text":""},{"location":"others/amr31001/lab2/#getting-started","title":"Getting Started","text":""},{"location":"others/amr31001/lab2/#downloading-the-amr31001-ros-package","title":"Downloading the AMR31001 ROS Package","text":"<p>To start with, you'll need to download a ROS package to the Robot Laptop that you are working on today. This package contains all the resources that you'll need for the lab exercises.</p> <ol> <li> <p>Open up a terminal instance on the laptop, either by using the <code>Ctrl+Alt+T</code> keyboard shortcut, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p> </p> </li> <li> <p>In the terminal, run the following commands in order:</p> <p>Tip</p> <p>To paste the following commands into the terminal use <code>Ctrl+Shift+V</code></p> <p><pre><code>wget -O build.sh https://raw.githubusercontent.com/tom-howard/amr31001/main/scripts/build.sh\n</code></pre> <pre><code>chmod +x build.sh\n</code></pre> <pre><code>./build.sh\n</code></pre></p> </li> </ol>"},{"location":"others/amr31001/lab2/#launching-ros","title":"Launching ROS","text":"<p>Much the same as last time, you'll now need to get ROS up and running on your robot. </p> <ol> <li> <p>First, identify the number of the robot that you have been provided with.</p> <p>Robots are named: <code>dia-waffleNUM</code>, where <code>NUM</code> is a unique 'Robot Number' (a number between 1 and 50).</p> </li> <li> <p>In the terminal, type the following command to pair the laptop and robot:</p> <p><pre><code>waffle NUM pair\n</code></pre> Replacing <code>NUM</code> with the number of the robot that you have been provided with.</p> </li> <li> <p>You may see a message like this early on in the pairing process:</p> <p> </p> <p>If so, just type <code>yes</code> and then hit <code>Enter</code> to confirm that you want to continue.</p> </li> <li> <p>Enter the password for the robot when requested (if you can't remember what this is from last time then ask a member of the teaching team!)</p> <p>Remember</p> <p>You won't see anything change on the screen when you are entering the password. This is normal, just keep typing!!</p> </li> <li> <p>Once the pairing process is finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>In the same terminal, enter the following command:</p> <p><pre><code>waffle NUM term\n</code></pre> (again, replacing <code>NUM</code> with the number of your robot).</p> <p>A green banner should appear across the bottom of the terminal window:</p> <p> </p> <p>Remember, this is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> <li> <p>Now, launch ROS on the robot by entering the following command:</p> <pre><code>roslaunch tuos_tb3_tools ros.launch\n</code></pre> <p>After a short while, you should see a message like this:</p> <pre><code>[INFO] [#####] Calibration End  \n</code></pre> <p>ROS is now up and running, and you're ready to go!</p> </li> <li> <p>Close down this terminal instance. If you see the following message, just click \"Close Terminal.\"</p> <p> </p> </li> </ol>"},{"location":"others/amr31001/lab2/#odometry","title":"Odometry","text":"<p>First, let's look at our robot's odometry system, and what this is useful for.</p> <p>Odometry is the use of data from motion sensors to estimate change in position over time. It is used in robotics by some legged or wheeled robots to estimate their position relative to a starting location. 1</p> <p>Our robot can therefore keep track of its position (and orientation) as it moves around. It does this using data from two sources:</p> <ol> <li>Wheel encoders: Our robot has two wheels, each is equipped with an encoder that measures the number of rotations that the wheel makes. </li> <li>An Inertial Measurement Unit (IMU): Using accelerometers, gyroscopes and compasses, the IMU can monitor the linear and angular velocity of the robot, and which direction it is heading, at all times.</li> </ol> <p>This data is published to a ROS Topic called <code>/odom</code>. </p>"},{"location":"others/amr31001/lab2/#ex1","title":"Exercise 1: Exploring Odometry Data","text":"<p>In the previous lab we used some ROS commands to identify and interrogate active topics on the ROS network, let's give that another go now, but on the <code>/odom</code> topic this time.</p> <ol> <li> <p>Open up a new terminal instance on the laptop (by pressing <code>Ctrl+Alt+T</code>, or clicking the Terminal App desktop icon, as you did before). We\u2019ll call this one TERMINAL 1.</p> </li> <li> <p>As you may recall from last time, we can use the <code>rostopic</code> command to list all the topics that are currently active on the network. Enter the following in TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>rostopic list\n</code></pre></p> <p>A large list of items should appear on the screen. Can you spot the <code>/odom</code> topic?</p> </li> <li> <p>Let's find out more about this using the <code>rostopic info</code> command.</p> <p>TERMINAL 1: <pre><code>rostopic info /odom\n</code></pre></p> <p>This should provide the following output:</p> <pre><code>Type: nav_msgs/Odometry\n\nPublishers:\n  * /turtlebot3_core (http://dia-waffleNUM:#####/)\n\nSubscribers: None\n</code></pre> <p>Post-lab Quiz</p> <p>What does all this mean? We discussed this last time (in relation to the <code>/cmd_vel</code> topic), and you may want to have a look back at this to refresh your memory! </p> <p>One of the key things that this does tell us is that the <code>/odom</code> topic transmits data using a <code>nav_msgs/Odometry</code> message. All topics use standard message types to pass information around the ROS network. This is so that any node on the ROS network knows how to deal with the data, if it needs to. <code>nav_msgs/Odometry</code> is one of these standard message types. </p> </li> <li> <p>We can use the <code>rosmsg</code> command to find out more about this:</p> <p>TERMINAL 1: <pre><code>rosmsg info nav_msgs/Odometry\n</code></pre></p> <p>You'll see a lot of information there, but try to find the line that reads <code>geometry_msgs/Pose pose</code>: </p> <pre><code>geometry_msgs/Pose pose\n  geometry_msgs/Point position\n    float64 x\n    float64 y\n    float64 z\n  geometry_msgs/Quaternion orientation\n    float64 x\n    float64 y\n    float64 z\n    float64 w\n</code></pre> <p>Here's where we'll find information about the robot's position and orientation (aka \"Pose\") in the environment. Let's have a look at this data in real time...</p> </li> <li> <p>We can look at the live data being streamed across the <code>/odom</code> topic, using the <code>rostopic echo</code> command. We know that this topic uses <code>nav_msgs/Odometry</code> type messages, and we know which part of these messages we are interested in (<code>geometry_msgs/Pose pose</code>)</p> <p>TERMINAL 1: <pre><code>rostopic echo /odom/pose/pose\n</code></pre></p> </li> <li> <p>Now, let's drive the robot around a bit and see how this data changes as we do so. Open up a new terminal instance by pressing <code>Ctrl+Alt+T</code>, or clicking the Terminal App desktop icon, as you did before. We'll call this one TERMINAL 2.</p> </li> <li> <p>Remember that node that we used last time that allowed us to control the motion of the robot using different buttons on the keyboard? Let's launch that again now:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n</code></pre></p> </li> <li> <p>Follow the instructions provided in the terminal to drive the robot around:</p> <p>As you're doing this, look at how the <code>position</code> and <code>orientation</code> data is changing in TERMINAL 1, in real-time!</p> <p>Post-lab Quiz</p> <p>Which position and orientation values change (by a significant amount) when:</p> <ol> <li>The robot turns on the spot (i.e. only an angular velocity is applied)?</li> <li>The robot moves forwards (i.e. only a linear velocity is applied)?</li> <li>The robot moves in a circle (i.e. both a linear and angular velocity are applied simultaneously)?</li> </ol> <p>Make a note of the answers to these questions, as they may feature in the post-lab quiz!</p> </li> <li> <p>When you've seen enough enter <code>Ctrl+C</code> in TERMINAL 2 to stop the <code>turtlebot3_teleop_keyboard</code> node. Then, enter <code>Ctrl+C</code> in TERMINAL 1 as well, which will stop the live stream of Odometery messages from being displayed.</p> </li> </ol>"},{"location":"others/amr31001/lab2/#summary","title":"Summary","text":"<p>Pose is a combination of a robot's position and orientation in its environment.</p> <p>Position tells us the location (in meters) of the robot in its environment. Wherever the robot was when it was turned on is the reference point, and so the distance values that we observed in the exercise above were all quoted relative to this initial position.</p> <p>You should have noticed that (as the robot moved around) the <code>x</code> and <code>y</code> terms changed, but the <code>z</code> term should have remained at zero. This is because the <code>X-Y</code> plane is the floor, and any change in <code>z</code> position would mean that the robot was floating or flying above the floor! </p> <p>Orientation tells us where the robot is pointing in its environment, expressed in units of Quaternions; a four-term orientation system. Don't worry too much about this though, we'll convert this to Euler angles (in degrees/radians) for you, to make them a bit easier to work with for the following exercise.</p> <p></p>"},{"location":"others/amr31001/lab2/#ex2","title":"Exercise 2: Odometry-based Navigation","text":"<p>Now that we know about the odometry system and what it tells us, let's see how this could be used as a feedback signal to inform robot navigation. You may recall that last time you created a ROS Node to make your robot to follow a square motion path on the floor. This was time-based though: given the speed of motion (tuning or moving forwards) it was possible to determine the time it would take for the robot to move by a required distance. Having determined this, we then added timers to our node, to control the switch between moving forwards and turning on the spot, in order to generate the square motion path. </p> <p>In theory though, we can do all this with odometry instead, so let's have a go at that now...</p> <ol> <li> <p>Open up the <code>amr31001</code> ROS package that you downloaded earlier into VS Code using the following command in TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>code ~/catkin_ws/src/amr31001\n</code></pre></p> </li> <li> <p>In VS Code, navigate to the <code>src</code> directory in the File Explorer on the left-hand side, and click on the <code>ex2.py</code> file to display it in the editor.</p> </li> <li> <p>Have a look through the code and see if you can work out what's going on. There are a few things to be aware of:</p> <ol> <li> <p>Motion control is handled by an external Python module called <code>waffle</code>, which is imported on line 4:</p> <pre><code>import waffle\n</code></pre> <p>and instantiated on line 16:</p> <pre><code>motion = waffle.Motion()\n</code></pre> <p>In the main part of the code, this can then be used to control the velocity of the robot, using the following methods:</p> <ol> <li><code>motion.move_at_velocity(linear = x, angular = y)</code> to make the robot move at a linear velocity of <code>x</code> (m/s) and/or an angular velocity of <code>y</code> (rad/s).</li> <li><code>motion.stop()</code> to make the robot stop moving.</li> </ol> </li> <li> <p>Subscribing to the <code>/odom</code> topic and the processing of the <code>nav_msgs/Odometry</code> data is also handled by the <code>waffle</code> module, so you don't have to worry about it! This functionality is instantiated on line 17:</p> <pre><code>pose = waffle.Pose()\n</code></pre> <p>So all that you have to do in order to access the robot's odometry data in the main part of the code is call the appropriate attribute:</p> <ol> <li><code>pose.posx</code> to obtain the robot's current position (in meters) in the <code>X</code> axis.</li> <li><code>pose.posy</code> to obtain the robot's current position (in meters) in the <code>Y</code> axis.</li> <li><code>pose.yaw</code> to obtain the robot's current orientation (in degrees) about the <code>Z</code> axis.</li> </ol> </li> </ol> </li> <li> <p>Run the code in TERMINAL 1 and observe what happens:</p> <p>TERMINAL 1: <pre><code>rosrun amr31001 ex2.py\n</code></pre></p> <p>The robot should start turning on the spot, and you should see some interesting information being printed to the terminal. After it has turned by 45\u00b0, the robot should stop momentarily and then carry on turning again.</p> </li> <li> <p>Stop the Node by entering <code>Ctrl+C</code> in TERMINAL 1.</p> </li> <li> <p>What you need to do:</p> <ol> <li> <p>In the <code>while()</code> loop there is an <code>if</code> statement with a condition that handles the turning process: </p> <pre><code>elif movement == \"turn\":\n</code></pre> <p>Within this, look at how the robot's yaw angle is being monitored and updated as it turns. Then, look at how the turn angle is being controlled. See if you can adapt this to make the robot turn in 90\u00b0 steps instead.</p> </li> <li> <p>Ultimately, after the robot has turned by 90\u00b0 it needs to then move forwards by 0.5m, in order to achieve a 0.5x0.5m square motion path.</p> <p>Moving forwards is handled by an additional condition within the <code>if</code> statement:</p> <p><pre><code>elif movement == \"move_fwd\":\n</code></pre> See if you can adapt the code within this block to make the robot move forwards by the required amount (0.5 meters) in between each turn. </p> Hint <p>Consider how the turn angle is monitored and updated whist turning (<code>current_yaw</code>), and take a similar approach with the linear displacement (<code>current_distance</code>). Bear in mind that you'll need to consider the euclidean distance, which you'll need to calculate based on the robot's position in both the <code>x</code> and <code>y</code> axis.</p> <p> </p> </li> <li> <p>Make sure that you've saved any changes to the code (in VS Code) before trying to test it out on the robot!</p> <p>Do this by using the <code>Ctrl+S</code> keyboard shortcut, or going to <code>File &gt; Save</code> from the menu at the top of the screen.</p> </li> <li> <p>Once you've saved it, you can re-run the code at any time by using the same <code>rosrun</code> command as before:</p> <p>TERMINAL 1: <pre><code>rosrun amr31001 ex2.py\n</code></pre></p> <p>... and you can stop it at any time by entering <code>Ctrl+C</code> in the terminal.</p> </li> </ol> <p>Python Tips</p> <p>You'll need to do a bit of maths here (see the \"Hint\" above). Here's how to implement a couple of mathematical functions in Python:</p> <ol> <li> <p>To the power of...: Use <code>**</code> to raise a number to the power of another number (i.e. <code>23</code>):</p> <pre><code>&gt;&gt;&gt; 2**3\n8\n</code></pre> </li> <li> <p>Square Root: To calculate the square root of a number use the <code>sqrt()</code> function:</p> <pre><code>&gt;&gt;&gt; sqrt(4)\n2.0 \n</code></pre> </li> </ol> </li> </ol>"},{"location":"others/amr31001/lab2/#the-lidar-sensor","title":"The LiDAR Sensor","text":"<p>As you'll know, the black spinning device on the top of your robot is a LiDAR Sensor. As discussed previously, this sensor uses laser pulses to measure the distance to nearby objects. The sensor spins continuously so that it can fire these laser pulses through a full 360\u00b0 arc, and generate a full 2-dimensional map of the robot's surroundings.</p> <p>This data is published to a ROS Topic called <code>/scan</code>. Use the same methods that you used in Exercise 1 to find out what message type is used by this ROS Topic.</p> <p>Post-lab Quiz</p> <p>Make a note of this, there'll be a post-lab quiz question on it!</p> <p>Launch RViz, so that we can see the data coming from this sensor in real-time:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_tb3_tools rviz.launch\n</code></pre></p> <p></p> <p>The red dots illustrate the LiDAR data. Hold your hand out to the robot and see if you can see it being detected by the sensor... a cluster of red dots should form on the screen to indicate where your hand is located in relation to the robot. Move your hand around and watch the cluster of dots move accordingly. Move your hand closer and farther away from the robot and observe how the red dots also move towards or away from the robot on the screen. </p> <p>This data is really useful and (as we observed during the previous lab session) it allows us to build up 2-dimensional maps of an environment with considerable accuracy. This is, of course, a very valuable skill for a robot to have if we want it to be able to navigate autonomously, and we'll explore this further later on. For now though, we'll look at how we can use the LiDAR data ourselves to build Nodes that make the robot detect and follow walls!</p> <p>Once you're done, close down RViz by hitting <code>Ctrl+C</code> in TERMINAL 1. </p>"},{"location":"others/amr31001/lab2/#ex3","title":"Exercise 3: Wall following","text":"<ol> <li> <p>In VS Code, click on the <code>ex3.py</code> file in the File Explorer to display it in the editor.</p> </li> <li> <p>Have a look through the code and see if you can work out what's going on. Here's a few points to start with:</p> <ol> <li> <p>Velocity control is handled in the same way as in the previous exercise:</p> <ol> <li><code>motion.move_at_velocity(linear = x, angular = y)</code> to make the robot move at a linear velocity of <code>x</code> (m/s) and/or an angular velocity of <code>y</code> (rad/s).</li> <li><code>motion.stop()</code> to make the robot stop moving.</li> </ol> </li> <li> <p>The data from the LiDAR sensor has been preprocessed and encapsulated in an additional class from the <code>waffle</code> module. This functionality is instantiated on line 13:</p> <pre><code>lidar = waffle.Lidar()\n</code></pre> <p>This class splits up data from the LiDAR sensor into a number of different segments to focus on a number of distinct zones around the robot's body (to make the data a bit easier to deal with). For each of the segments (as shown in the figure below) a single distance value can be obtained, which represents the average distance to any object(s) within that particular angular zone:</p> <p> </p> <p>In the code, we can obtain the distance measurement (in meters) from each of the above zones as follows:</p> <ol> <li><code>lidar.distance.front</code> to obtain the average distance to any object(s) in front of the robot (within the frontal zone).</li> <li><code>lidar.distance.l1</code> to obtain the average distance to any object(s) located within LiDAR zone L1.</li> <li><code>lidar.distance.r1</code> to obtain the average distance to any object(s) located within LiDAR zone R1.     and so on...</li> </ol> </li> <li> <p>The code template has been developed to detect a wall on the robot's left-hand side.</p> <ol> <li>We use distance measurements from LiDAR zones L3 and L4 to determine the alignment of the robot to a left-hand wall.</li> <li> <p>This is determined by calculating the difference between the distance measurements reported from these two zones:</p> <p><pre><code>wall_rate = lidar.distance.l3 - lidar.distance.l4\n</code></pre>         1. If this value is close to zero, then the robot and the wall are well aligned. If not, then the robot is at an angle to the wall, and it needs to adjust its angular velocity in order to correct for this:</p> <p> </p> </li> </ol> </li> </ol> </li> <li> <p>Run the node, as it is, from TERMINAL 1:</p> <p>TERMINAL 1: <pre><code>rosrun amr31001 ex3.py\n</code></pre></p> <p>When you do this, you'll notice that the robot doesn't move at all (yet!), but the following data appears in the terminal:</p> <ol> <li>The distance measurements from each of the LiDAR zones.</li> <li>The current value of the <code>wall_rate</code> parameter, i.e. how well aligned the robot currently is to a wall on its left-hand side.</li> <li>The decision that has been made by the <code>if</code> statement on the appropriate action that should be taken, given the current value of <code>wall_rate</code>.</li> </ol> </li> <li> <p>What you need to do:</p> <ol> <li>First, place the robot on the floor with a wall on its left-hand side</li> <li> <p>Manually vary the alignment of the robot and the wall and observe how the information that is being printed to the terminal changes as you do so.</p> <p>Question</p> <p>The node will tell you if it thinks the robot needs to turn right or left in order to improve its current alignment with the wall. Is it making the correct decision?</p> </li> <li> <p>Currently, all velocity parameters inside the <code>while</code> loop are set to zero.</p> <ul> <li> <p>You'll need to set a constant linear velocity, so that the robot is always moving forwards. Set an appropriate value for this now, by editing the line that currently reads:</p> <pre><code>lin_vel = 0.0\n</code></pre> </li> <li> <p>The angular velocity of the robot will need to be adjusted conditionally, in order to ensure that the value of <code>wall_rate</code> is kept as low as possible at all times. Adjust the value of <code>ang_vel</code> in each of the <code>if</code> statement blocks so that this is achieved under each of the three possible scenarios.</p> <ol> <li>Hopefully, by following the steps above, you will get to the point where you can make the robot follow a wall reasonably well, as long as the wall remains reasonably straight! Consider what would happen however if the robot were faced with either of the following situations:</li> </ol> </li> </ul> <p> </p> <p>You may have already observed this during your testing... how could you adapt the code so that such situations can be achieved?</p> Hints <ol> <li>You may need to consider the distance measurements from some other LiDAR zones!</li> <li> <p>The <code>ex3.py</code> template that was provided to you uses an <code>if</code> statement with three different cases:</p> <pre><code>if ...:\nelif ...:\nelse:\n</code></pre> <p>You may need to add in some further cases to this to accommodate the additional situations discussed above, e.g.:</p> <pre><code>if ...:\nelif ...:\nelif ...:\nelif ...:\nelse:\n</code></pre> </li> </ol> </li> <li> <p>Finally, think about how you could adapt this algorithm to make the robot follow a wall on its right-hand side instead.  </p> </li> </ol> </li> </ol>"},{"location":"others/amr31001/lab2/#slam-and-autonomous-navigation","title":"SLAM and Autonomous Navigation","text":"<p>We've played around with the data from both the LiDAR sensor and the robot's Odometry System now, so hopefully you now understand what these two systems can tell us about our robot and its environment, and how this information is very valuable for robotic applications.</p> <p>To illustrate this further, we'll now have a look at two extremely useful tools that are build into ROS, and that use the data from these two sensors alone to achieve powerful results!</p> <p>Simultaneous Localisation and Mapping (SLAM)</p> <p>As you now know, the LiDAR sensor gives us a full 360\u00b0 view of our robot's environment. As the robot moves around it starts to observe different features in the environment, or perhaps the same features that it has already seen, but at a different perspective. Using the LiDAR data in combination with our robots Pose and Velocity (as provided by the Odometry System), we can actually build up a comprehensive 2-dimensional map of an environment and keep track of where our robot is actually located within that map, at the same time. This is a process called SLAM, and you may remember that we had a go at this in the previous lab session.</p> <p>Navigation</p> <p>Having built a complete map (using SLAM) our robot then knows exactly what its environment looks like, and it can then use the map to work out how to get from one place to another on its own! </p> <p>In the next exercise, we'll have a go at this: first we need to build a map, then we can use that map to implement autonomous navigation!</p>"},{"location":"others/amr31001/lab2/#ex4","title":"Exercise 4: SLAM and Navigation","text":"<p>Step 1: Mapping</p> <ol> <li> <p>Make sure none of your code from the previous exercise is still running now, TERMINAL 1 should be idle, ready to accept some new instructions!</p> </li> <li> <p>Place your robot in the enclosure that we have created in the lab. </p> </li> <li> <p>Then, in TERMINAL 1, run the following command:</p> <p>TERMINAL 1: <pre><code>roslaunch amr31001 ex4_slam.launch\n</code></pre></p> <p>An RViz screen will open up showing the robot from a top-down view, with the LiDAR data represented by green dots.</p> <p> </p> <p>SLAM is already working, and you should notice black lines forming underneath the green LiDAR dots to indicate the regions that SLAM has already determined as static and which therefore represent boundaries in the environment.</p> </li> <li> <p>In TERMINAL 2 launch the <code>turtlebot3_teleop_keyboard</code> node again, to drive the robot around:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n</code></pre></p> </li> <li> <p>Drive the robot around until SLAM has built up a complete map of the entire arena.</p> <p> </p> </li> <li> <p>Once you're happy with this, stop the <code>turtlebot3_teleop_keyboard</code> node by hitting <code>Ctrl+C</code> in TERMINAL 2. You can also stop SLAM now too, so head back to TERMINAL 1 and enter <code>Ctrl+C</code> to stop this too.</p> </li> <li> <p>While you were doing the above, a map file was being constantly updated and saved to the laptop's filesystem. Have a quick look at this now to make sure that the map that you have built is indeed a good representation of the environment:</p> <p>TERMINAL 1: <pre><code>eog ~/catkin_ws/src/amr31001/maps/my_map.pgm\n</code></pre></p> <p>If the map looks like the actual arena then you're good to move on, but if not then you may need to repeat the above steps again to give it another go.</p> </li> </ol> <p>Step 2: Navigating Autonomously</p> <p>Using the map that you've just created you should now be able to make your robot navigate around the real environment!</p> <ol> <li> <p>Launch the navigation processes in TERMINAL 1 by entering the following command:</p> <p>TERMINAL 1: <pre><code>roslaunch amr31001 ex4_nav.launch\n</code></pre></p> <p>RViz should then open up again, once again with a top-down view of the robot, but this time with the map that you generated earlier displayed underneath it (in black).  </p> </li> <li> <p>To begin with, the robot needs to know roughly where it is within this map, and we can tell it this by performing a manual \"2D Pose Estimation\".  </p> <ol> <li>Press the \"2D Pose Estimate\" button at the top of the RViz screen.  </li> <li>The map should be visible (in black) in the background underneath all the live LiDAR Data (displayed in green), but the two are probably not lined up properly. Move the cursor to the approximate point on the background map at which the robot is actually located.  </li> <li>Press and hold the left mouse button and a large green arrow will appear.  </li> <li>Whilst still holding down the left mouse button, rotate the green arrow to set the robot's orientation within the map.  </li> <li> <p>Let go of the left mouse button to set the pose estimation. If done correctly, the real-time LiDAR data should nicely overlap the background map after doing this:</p> <p> </p> <p>... if not, then have another go!</p> </li> </ol> </li> <li> <p>We then need to give the robot a chance to determine its pose more accurately. The navigation processes that are running in the background are currently generating what's called a \"localisation particle cloud\", which is displayed as lots of tiny green arrows surrounding the robot in RViz. The scatter of these green arrows indicates the level of certainty the robot currently has about its true pose (position and orientation) within the environment: a large scatter indicates a high level of uncertainty, and we need to improve this before we can make the robot navigate around autonomously.</p> <p>We can improve this by simply driving the robot around the environment a bit, so that it gets the opportunity to see different walls and obstacles from different perspectives. In the background, the navigation algorithms will be comparing the real time LiDAR data with the background map that was generated earlier, and when the two things line up well, the robot becomes more certain about its pose in the environment.</p> <p>Once again, launch the <code>turtlebot_teleop_keyboard</code> node in TERMINAL 2, and drive the robot around a bit:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n</code></pre></p> <p>As you're doing this, watch how the scatter in the localisation particle cloud reduces, and the small green arrows begin to converge underneath the robot.</p> <p> </p> </li> <li> <p>Now, click the \"2D Nav Goal\" button:  </p> <ol> <li>Move the cursor to the location that you want the robot to move to on the map. </li> <li>Click and hold the left mouse button and a large green arrow will appear again to indicate that the position goal has been set.  </li> <li>Whilst still holding the left mouse button, rotate the green arrow around to set the desired orientation goal.  </li> <li> <p>Release the mouse button to set the goal...</p> <p>The robot will then try its best to navigate to the destination autonomously!</p> </li> </ol> </li> </ol>"},{"location":"others/amr31001/lab2/#wrapping-up","title":"Wrapping Up","text":"<p>Before you leave, please turn off your robot! Enter the following command in TERMINAL 1 to do so:</p> <p>TERMINAL 1: <pre><code>waffle NUM off\n</code></pre> ... again, replacing <code>NUM</code> with the number of the robot that you have been working with today.</p> <p>You'll need to enter <code>y</code> and then hit <code>Enter</code> to confirm this.</p> <p>Please then shut down the laptop, which you can do by clicking the battery icon in the top right of the desktop and selecting the \"Power Off / Log Out\" option in the drop-down menu.</p> <p></p> <p> <p>AMR31001 Lab 2 Complete! </p> <p></p> <ol> <li> <p>https://en.wikipedia.org/wiki/Odometry\u00a0\u21a9</p> </li> </ol>"},{"location":"waffles/","title":"Working with the Real TurtleBot3 Waffles","text":"<p>Having completed our 6-week ROS course, you'll now be more than ready to work with ROS on our real robots. This section of the Course Site is here to help you apply your simulation-based ROS knowledge to the Real TurtleBot3 Waffle Robots in the Diamond!</p> <p></p>"},{"location":"waffles/exercises/","title":"Getting Started: Some Initial Exercises","text":"<p>Having completed the steps on the previous page, your robot and laptop should now be paired, and ROS should be up and running. The next thing to do is bring the robot to life! Here are a few simple exercises for you to have a go at, to get a feel for things... </p>"},{"location":"waffles/exercises/#ex1","title":"Exercise 1: Observing the robot's environment","text":"<p>You can use some ROS tools that you will be familiar with from simulation in order to see the real world through the eyes of the robot!</p> <ol> <li> <p>Open up a new terminal instance on the laptop (which we'll call TERMINAL 1). From here, enter the following command:</p> <p>TERMINAL 1: <pre><code>roslaunch tuos_tb3_tools rviz.launch\n</code></pre></p> Pro Tip <p>We've got a handy bash alias for this command: use <code>tb3_rviz</code> instead (if you want to)!</p> <p>This will launch RViz, which (as you may recall from your simulation work) is a ROS tool that allows us to visualise the data being measured by a robot in real-time.</p> <p> </p> <ul> <li> <p>The red dots scattered around the robot is the data from the LiDAR sensor.</p> </li> <li> <p>The strange wobbly sheet of colours in front of the robot is the live image stream from the camera with depth applied to it at the same time. The camera is able to determine how far away each image pixel is from the camera lens, and then uses that to render the image in 3-dimensions. Nice eh!?</p> </li> </ul> </li> <li> <p>Place your hand or your face in front of the camera and hold steady for a few seconds (there may be a bit of a lag as all of this data is transmitted over the WiFi network). You should see yourself rendered in 3D in front of the robot!</p> </li> <li> <p>Another tool that you'll be familiar with from Week 6 is <code>rqt_image_view</code>. Again, we can use this to view live image data being streamed to ROS image topics. Open another terminal instance on the laptop (TERMINAL 2) and launch this as follows:</p> <p>TERMINAL 2: <pre><code>rosrun rqt_image_view rqt_image_view\n</code></pre></p> <p>Select <code>/camera/color/image_raw</code> in the dropdown topic list to see the images being obtained and published by the robot's camera.</p> </li> </ol> <p>Use these tools to keep an eye on your robot's environment whilst performing the next exercise...</p>"},{"location":"waffles/exercises/#ex2","title":"Exercise 2: Driving the robot around using the laptop keyboard","text":"<p>We used the <code>turtlebot3_teleop_keyboard</code> node extensively in simulation to drive a Waffle around a range of simulated environments. This works in exactly the same way with a real robot in a real world!</p> <ol> <li> <p>Open another new terminal instance (TERMINAL 3) and enter exactly the same <code>roslaunch</code> command as you've used in simulation to launch the <code>turtlebot3_teleop</code> node:</p> <p>TERMINAL 3: <pre><code>roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\n</code></pre></p> Pro Tip <p>There's a bash alias for this one too: <code>tb3_teleop</code>!</p> </li> <li> <p>Drive the robot around using the laptop keyboard (as you did in simulation) taking care to avoid any obstacles as you do!</p> </li> <li> <p>Once you've spent a bit of time on this, close the teleop node down by entering <code>Ctrl+C</code> in TERMINAL 3.</p> </li> <li> <p>Close down RViz and the <code>rqt_image_view</code> nodes running in TERMINAL 2 and TERMINAL 1 as well, we won't need these for the next exercise.</p> </li> </ol>"},{"location":"waffles/exercises/#ex3","title":"Exercise 3: Creating a ROS package on the Laptop","text":"<p>This works exactly the same as in simulation too.</p> <p>A Catkin Workspace exists on the laptop's filesystem here:</p> <pre><code>/home/student/catkin_ws/\n</code></pre> <p>a.k.a.:</p> <pre><code>~/catkin_ws/\n</code></pre> <p>Remember</p> <p>Much like in simulation, always create ROS packages in the <code>catkin_ws/src/</code> directory!</p> <ol> <li> <p>In TERMINAL 1 navigate to the Catkin Workspace <code>src</code> directory:</p> <p>TERMINAL 1: <pre><code>cd ~/catkin_ws/src/\n</code></pre></p> </li> <li> <p>Create a new package here using the <code>catkin_create_pkg</code> tool. Don't forget to include <code>rospy</code> as a dependency! </p> <p>TERMINAL 1: <pre><code>catkin_create_pkg {your_package} rospy\n</code></pre></p> Using Git? <p>Already created a ROS package in simulation? Why not push it to GitHub (or GitLab, etc.) and <code>git clone</code> it here instead?</p> </li> <li> <p>Next, don't forget to run <code>catkin build</code>:</p> <p>TERMINAL 1: <pre><code>catkin build {your_package}\n</code></pre></p> </li> <li> <p>Then, re-source your environment:</p> <p>TERMINAL 1: <pre><code>source ~/.bashrc\n</code></pre></p> Pro Tip <p>You can also use the <code>src</code> alias, just like in simulation!</p> </li> </ol>"},{"location":"waffles/exercises/#ex4","title":"Exercise 4: Using SLAM to create a map of the environment","text":"<p>Remember how you used SLAM in Week 3 to create a map of a simulated environment? We'll do this now on a real robot in a real environment!</p> <ol> <li> <p>In TERMINAL 2 enter the following command to launch all the necessary SLAM nodes on the laptop:</p> <p>TERMINAL 2: <pre><code>roslaunch turtlebot3_slam turtlebot3_slam.launch\n</code></pre></p> Pro Tip <p>Also available as an alias: <code>tb3_slam</code>!</p> <p>This will launch RViz again, where you should now be able to see a model of the Waffle from a top-down view surrounded by green dots representing the real-time LiDAR data. The SLAM tools will already have begun processing this data to start building a map of the boundaries that are currently visible to your robot based on its location in the environment.</p> <p>Note</p> <p>To begin with your robot may just appear as a white shadow (similar to the left-hand image below). It may take some time for the robot to render correctly (like the right-hand image) as the SLAM processes and data communications catch up with one another. </p> <p> </p> <p>This can sometimes take up to a minute or so, so please be patient! If (after a minute) nothing has happened, then speak to a member of the teaching team.</p> </li> <li> <p>Return to TERMINAL 3 and launch the <code>turtlebot3_teleop_keyboard</code> node again. Start to drive the robot around slowly and carefully to build up a complete map of the area.</p> <p>Tip</p> <p>It's best to do this slowly and perform multiple circuits of the whole area to build up a more accurate map.</p> </li> <li> <p>Once you're happy that your robot has built up a good map of its environment, you can save this map using the <code>map_server</code> package (again, in exactly the same way as you did in simulation):</p> <ol> <li> <p>First, create a new directory within your <code>{your_package}</code> package on the laptop (to save maps in). You should still be in your package directory in TERMINAL 1, so head back to that one:</p> <ol> <li> <p>There's no harm in running this, just to make sure that you're in the right place to begin with:</p> <p>TERMINAL 1: <pre><code>roscd {your_package}\n</code></pre></p> </li> <li> <p>Create a directory in here called <code>maps</code>: </p> <p>TERMINAL 1: <pre><code>mkdir maps/\n</code></pre></p> </li> <li> <p>Navigate into this directory:</p> <p>TERMINAL 1: <pre><code>cd maps/\n</code></pre></p> </li> </ol> </li> <li> <p>Then, use <code>rosrun</code> to run the <code>map_saver</code> node from the <code>map_server</code> package to save a copy of your map:</p> <p>TERMINAL 1: <pre><code>rosrun map_server map_saver -f {map_name}\n</code></pre></p> <p>Replacing <code>{map_name}</code> with an appropriate name for your map. This will create two files: </p> <ol> <li>a <code>{map_name}.pgm</code> </li> <li>a <code>{map_name}.yaml</code> file</li> </ol> <p>...both of which contain data related to the map that you have just created.</p> </li> <li> <p>The <code>.pgm</code> file can be opened in <code>eog</code> on the laptop: </p> <p>TERMINAL 1: <pre><code>eog {map_name}.pgm\n</code></pre></p> </li> </ol> </li> <li> <p>Return to TERMINAL 2 and close down SLAM by pressing <code>Ctrl+C</code>. The process should stop and RViz should close down.</p> </li> <li> <p>Close down the <code>teleop</code> node in TERMINAL 3, if that's still going too.</p> </li> </ol>"},{"location":"waffles/fact-finding/","title":"Fact-Finding Missions!","text":"<p>For the most part, everything that you have done with ROS in simulation so far is directly applicable to the real robots too. There are a few subtle differences though, and you should make sure that you are aware of these before going much further.</p> <p>Complete the following fact-finding missions, which will help you to explore and identify the key differences between the way our TurtleBot3 Waffles work in the real world, compared to how they work in WSL-ROS (simulation).</p>"},{"location":"waffles/fact-finding/#mission-1-publishing-velocity-commands","title":"Mission 1: Publishing Velocity Commands","text":"<p>In Exercise 3 of Week 2 you learnt how to publish velocity commands (from the command-line) to make the robot move in simulation. Return to that exercise now and repeat this, in simulation again (and come back here when you're done!)</p> <p>Now, follow exactly the same steps, but this time on the robotics laptop, to make your real robot move instead.</p> <p>What do you notice about how the real robot moves when you issue a correctly formulated <code>rostopic pub</code> command, e.g.:</p> <pre><code>rostopic pub /cmd_vel geometry_msgs/Twist \"linear:\n  x: 0.0\n  y: 0.0\n  z: 0.0\nangular:\n  x: 0.0\n  y: 0.0\n  z: 0.0\"\n</code></pre> <p>(replacing some of the <code>0.0</code>s above with applicable values)</p> <p>Next, look at the usage information for the <code>rostopic pub</code> command by entering:</p> <pre><code>rostopic pub -h\n</code></pre> <p>Try to work out how to control the publishing rate by adding an additional command-line argument to the end of the <code>rostopic pub</code> commands. Use this to specify a publishing rate of 1 hz and see what happens (in terms of how the robot moves). Next, specify a rate of 10 hz and see how this changes things.</p> <p>When you stop the <code>rostopic pub</code> command what happens to the robot, and how does this differ to what happens when you do the same thing in simulation?</p> <p>Question</p> <p>Having answered the questions above, what implications might this have for any ROS nodes that you create to control the velocity of a real robot? </p>"},{"location":"waffles/fact-finding/#mission2","title":"Mission 2: The Camera Image Topic","text":"<p>In Week 6 you worked extensively with the robot's camera and its images, which were published to the <code>/camera/rgb/image_raw</code> topic.</p> <p>Warning</p> <p>The name of the camera image topic is not the same on the real robots!</p> <p>On the laptop, use the ROS command-line tools that you know and love by now to interrogate the real robot ROS Network and identify the name of the camera image topic on the real robots.</p>"},{"location":"waffles/fact-finding/#mission-3-camera-image-resolution","title":"Mission 3: Camera Image Resolution","text":"<p>At the beginning of Week 6 we looked at how to explore the messages published to the robot's camera image topic. Here you should have worked out which part of these messages told you about the resolution of the camera images (i.e.: the <code>height</code> and <code>width</code> of the images, in pixels). You may recall what this was, but if not, go back and interrogate this again in WSL-ROS to find out what resolution our robot's camera images are transmitted at, in simulation (you'll need to use <code>rostopic echo</code>). </p> <p>Warning</p> <p>The real robot's camera captures images at a different image resolution! </p> <p>Use <code>rostopic echo</code> again, but this time on the Robotics Laptop, to interrogate the real robot ROS network and identify the <code>height</code> and <code>width</code> of the camera images that are captured by our real robot's camera.</p> <p>Note</p> <p>This will have implications when you come to apply image cropping techniques... the same cropping procedures may not work the same way for nodes run in simulation, compared to those running on a real robot!</p>"},{"location":"waffles/fact-finding/#mission-4-out-of-range-lidar-data","title":"Mission 4: Out of Range LiDAR Data","text":"<p>The robot's LiDAR sensor can only obtain measurements from objects within a certain distance range. In Week 3 we looked at how to work out what this range is, using the <code>rostopic echo</code> command. Apply the same techniques to the real robot now to discover the maximum and minimum distances that the real robot's LiDAR sensor can measure.</p> <p>If the LiDAR sensor detects an object that falls within this range then it will report the exact distance to this object (in meters). Conversely, if it doesn't detect anything within this range then it will report a default out-of-range value instead. In simulation, the out-of-range value is <code>inf</code>.</p> <p>Warning</p> <p>The out-of-range value reported by the real robot's LiDAR sensor is not <code>inf</code>!</p> <p>Use the <code>rostopic echo</code> command to interrogate the ROS network running between your real Waffle and the robotics laptop, and find out what out-of-range value is used here.</p>"},{"location":"waffles/fact-finding/#mission-5-object-detection","title":"Mission 5: Object Detection","text":"<p>In general, image detection gets a little more challenging in the real-world, where the same object might appear (to a robot's camera) to have slightly different colour tones under different light conditions, from different angles, in different levels of shade, etc. In Exercise 3 of Week 6 you may well have built an extremely robust <code>colour_search.py</code> node to detect each of the four coloured pillars in the <code>tuos_ros_simulations/coloured_pillars</code> world. See how well this now works in the real world now by running the same code on your real Waffle.</p> <p>Questions</p> <ol> <li>Without changing any of your code, is the robot able to detect any of our real coloured pillars in the robot arena?</li> <li>Can you make any changes to adapt this and make it work more reliably in a real-world environment?</li> </ol>"},{"location":"waffles/fact-finding/#summary","title":"Summary","text":"<p>When working with the real robots we strongly recommend that you start off by developing your code in simulation, where it's a bit easier to test things out and less disastrous if things go wrong! Overall, you'll be able to develop things much faster this way. Whilst you're doing this though, keep in mind all the differences that you have identified during your fact finding missions above, so that there are less nasty surprises when you come to deploy your ROS applications on the real Waffles. </p> <p>Throughout the design phase, think about how your applications could be developed more flexibly to accommodate these variations, or how you could design things so that only small/quick changes/switches need to be made when you transition from testing in simulation, to deploying your applications in the real world. </p>"},{"location":"waffles/intro/","title":"Introduction","text":""},{"location":"waffles/intro/#handling-the-robots","title":"Handling the Robots","text":"<p>Health and Safety</p> <p>You must have completed a health and safety quiz before working with the robots for the first time. This quiz is available on Blackboard.</p> <p></p> <p>As you can see from the figure above, the robots have lots of exposed sensors and electronics and so you must take great care when handling them to avoid the robots becoming damaged in any way.  When handling a robot, always hold it by either the black Waffle Layers, or the vertical Support Pillars (as highlighted in the figure above).</p> <p>Important</p> <p>Do not pick the robot up or carry it by the camera or LiDAR sensor! These are delicate devices that could be easily damaged!</p> <p>A lot of the electronics are housed on the middle waffle layer. Try not to touch any of the circuit boards, and take care not to pull on any of the cabling or try to remove or rehouse any of the connections. If you have any concerns with any of the electronics or cabling, if something has come loose, or if your robot doesn't seem to be working properly then ask a member of the teaching team to have a look for you.</p> <p>The robots will be provided to you with a battery already installed and ready to go. Don't try to disconnect or remove the battery yourselves! The robot will beep when the battery is low, and if this happens ask a member of the team to get you a replacement (we have plenty).</p>"},{"location":"waffles/intro/#laptops","title":"The Robotics Laptops","text":"<p>You'll be provided with one of our pre-configured Robotics Laptops in the lab when working with the real Waffles. Much like the WSL-ROS environment, these Laptops (and the Robots) run Ubuntu 20.04 with ROS Noetic. </p> <p>On the laptops there is a \"student\" user account that you'll use when working in the lab. The laptop should log you into this user account automatically on startup, but we'll provide you with the account password as well, during the lab sessions, should you need it.</p>"},{"location":"waffles/intro/#dialab","title":"WiFi","text":"<p>The Robots and Laptops connect to a dedicated wireless network running in the Diamond called 'DIA-LAB'. There are a few things that you need to know about this:</p> <ul> <li>Laptops must be connected to the DIA-LAB network in order to establish a ROS network between them and the robots.</li> <li>Laptops do not have internet access when connected to DIA-LAB.</li> <li>You'll need to connect the laptop to eduroam (or use another computer) to access any external resources (such as the instructions on this Course Site).</li> </ul> <p>Credentials for DIA-LAB and eduroam have already been set on the laptops, allowing you to connect to either network straight away, but speak to a member of the teaching team if you are having any issues.</p>"},{"location":"waffles/intro/#ide-vs-code","title":"IDE: VS Code","text":"<p>Visual Studio Code is installed on the laptops for you to use working on your real-robot ROS applications. Launch VS Code from any terminal by simply typing <code>code</code>, or you could also launch it by clicking the icon in the favourites bar on the left-hand side of the screen:</p> <p></p>"},{"location":"waffles/launching-ros/","title":"Launching ROS","text":"<p>The first step is to launch ROS on the Waffle.</p> <p>Important</p> <p>Launching ROS on the Waffle enables the ROS Master. The ROS Master always runs on the Waffle. It's therefore important to complete the steps on this page in full before you do anything else, otherwise the ROS Master will not be running, the robot's core functionality won't be active, and you therefore won't be able to do anything with it! </p>"},{"location":"waffles/launching-ros/#step-1-identify-your-waffle","title":"Step 1: Identify your Waffle","text":"<p>Robots are named as follows:</p> <pre><code>dia-waffleX\n</code></pre> <p>... where <code>X</code> indicates the 'Robot Number' (a number between 1 and 50). Make sure you know which robot you are working with, or check the label printed on top of it!</p>"},{"location":"waffles/launching-ros/#step-2-pairing-your-waffle-to-a-laptop","title":"Step 2: Pairing your Waffle to a Laptop","text":"<p>As discussed earlier, you'll be provided with one of our Robotics Laptops to work with in the lab, and the robot needs to be paired with this in order for the two to work together.  </p> <ol> <li> <p>Open up a terminal instance on the laptop, either by using the <code>Ctrl+Alt+T</code> keyboard shortcut, or by clicking the Terminal App icon in the favourites bar on the left-hand side of the desktop:</p> <p> </p> </li> <li> <p>We'll use our purpose-built Waffle CLI to handle the pairing process. Run this in the terminal by entering the following command to pair the laptop and robot:</p> <p><pre><code>waffle X pair\n</code></pre> Replacing <code>X</code> with the number of the robot that you are working with.</p> </li> <li> <p>You may see a message like this early on in the pairing process:</p> <p> </p> <p>If so, just type <code>yes</code> and then hit <code>Enter</code> to confirm that you want to continue.</p> </li> <li> <p>Enter the password for the robot when requested (we'll tell you what this is in the lab!)</p> <p>Note</p> <p>You won't see anything change on the screen when you are entering the password. This is normal, just keep typing!!</p> </li> <li> <p>Once the pairing process is finished you should see a message saying <code>pairing complete</code>, displayed in blue in the terminal. </p> </li> <li> <p>Then, in the same terminal, enter the following command:</p> <p><pre><code>waffle X term\n</code></pre> (again, replacing <code>X</code> with the number of your robot).</p> <p>A green banner should appear across the bottom of the terminal window:</p> <p> </p> <p>This is a terminal instance running on the robot, and any commands that you enter here will be executed on the robot (not the laptop!)</p> </li> </ol>"},{"location":"waffles/launching-ros/#step-3-launching-ros","title":"Step 3: Launching ROS","text":"<p>Launch ROS (and the ROS Master) on the robot by entering the following command:</p> <pre><code>roslaunch tuos_tb3_tools ros.launch\n</code></pre> Pro Tip <p>To save you typing this command out in full all the time, we've created a handy bash alias for it! You can therefore use <code>tb3_bringup</code> instead.</p> <p>After a short while, you should see a message like this:</p> <pre><code>[INFO] [#####] Calibration End  \n</code></pre> <p> <p>ROS is now up and running, and you're ready to go!</p> <p></p> <p>You shouldn't need to interact with this terminal instance any more now. You can either keep it open in the background (to keep an eye on any status messages coming through), or you can close down the terminal instance entirely. If you want to close it down then hit the  button in the top right of the terminal window. You'll then see the following message:</p> <p></p> <p>... just click \"Close Terminal.\"</p> <p>Warning</p> <p>When you've finished working with a robot it's really important to shut it down properly before turning off the power switch. Please refer to the safe shutdown procedure detailed here.</p>"},{"location":"waffles/shutdown/","title":"Safe Shutdown","text":"<p>As you should know, the Waffles are powered by a Single Board Computer (SBC), which runs a full-blown operating system (Ubuntu 20.04). As with any operating system, it's important to shut it down properly, rather than simply disconnecting the power, to avoid any data loss or other issues. </p> <p>Therefore, once you've finished working with a robot during a lab session, follow the steps below to shut it down.</p> <ol> <li> <p>Open a new terminal instance on the laptop (<code>Ctrl+Alt+T</code>), and enter the following:</p> <p><pre><code>waffle X off\n</code></pre> ... replacing <code>X</code> with the number of the robot that you have been working with.</p> </li> <li> <p>You'll be asked to confirm that you want to shut the robot down: </p> <pre><code>[INPUT] Are you sure you want to shutdown dia-waffleX? [y/n] &gt;&gt;\n</code></pre> <p>Enter <code>y</code> and hit <code>Enter</code> and the robot's SBC will be shut down. </p> </li> <li> <p>Once the blue light on the corner of the SBC goes out, it's then safe to slide the power button to the left to completely turn off the device. </p> <p> </p> </li> <li> <p>Once you've turned off the robot, remember to shut down the laptop too! Do this by clicking the battery icon in the top right of the desktop and selecting the \"Power Off / Log Out\" option in the drop-down menu.</p> <p> </p> </li> </ol> <p> <p>Hand your robot and laptop back to a member of the teaching team who will put it away for you! </p> <p></p>"},{"location":"wsl-ros/","title":"The WSL-ROS Simulation Environment","text":"<p>To support these courses we've created a custom ROS (Noetic) and Ubuntu (20.04) environment which runs on Windows 10 using the Windows Subsystem for Linux (WSL). We call this \"WSL-ROS\", and you can find out more about it - and how to use it - here.</p> <p>WSL-ROS is installed on all the University of Sheffield Managed Desktop Computers located in Computer Rooms 1, 2, 3 &amp; 4 in the Diamond. It's also possible to access it on some remote-access machines (see \"Accessing the Environment Remotely\" for more details).</p> <p>Each time you launch WSL-ROS on an applicable University Machine the WSL-ROS environment will be installed from a custom OS image that we have created, which contains the Ubuntu 20.04 operating system, ROS and all the additional ROS packages that you will need for this lab course. The environment that you install will only remain on the machine that you install it on for a limited time, and won't be preserved across different computers that you log into. It's therefore really important that you follow some backup and restore procedures.</p>"},{"location":"wsl-ros/backup-restore/","title":"Backing-Up (and Restoring) your Data","text":"<p>Once again, every time you do any work in the WSL-ROS Environment it's really important that you run a backup script before you close it down and log out of the University Machine that you are working on.  To do so is very easy, simply run the command below from any WSL-ROS Terminal Instance:</p> <pre><code>wsl_ros backup\n</code></pre> <p>This will create an archive of your Linux Home Directory and save it to your University U: Drive. Whenever you launch a fresh WSL-ROS Environment again on another day, or on a different machine, simply run the following command to restore your work to it:</p> <pre><code>wsl_ros restore\n</code></pre> <p>Remember</p> <p>Your work will not be preserved within the WSL-ROS Environment between sessions unless you happen to log back in to the same computer again within a few hours!</p> <p>To make things a little easier, on launching WSL-ROS, the system will check to see if a backup file already exists from a previous session. If it does, then the system will ask you if you want to restore it straight away:</p> <p></p> <p>Enter <code>Y</code> to restore your data from this backup file, or <code>N</code> to leave the backup file alone and work from fresh (none of your previous work will be restored). </p>"},{"location":"wsl-ros/configure-vscode/","title":"Configuring Visual Studio Code","text":"<p>Visual Studio Code (or 'VS Code', for short) should be installed on all the University of Sheffield Managed Desktop Computers in the Diamond Computer Rooms. This is a great Integrated Development Environment (IDE) that we'll use extensively alongside WSL-ROS. We first need to make sure it's set up correctly though, so follow the steps below now to install the \"Remote - WSL\" VS Code extension, in preparation for later.</p> <p>Tip</p> <p>You should only ever need to do this bit once: the configurations should be saved to your user profile, and should be carried over to any other University Desktop Computer that you log into!</p> <ol> <li> <p>Click the Windows Start Menu button: </p> </li> <li> <p>Type <code>\"vscode\"</code> and the Visual Studio Code application shortcut should then appear in the list:</p> <p> </p> <p>Click on it to launch the application.</p> </li> <li> <p>In the left-hand toolbar click the \"Extensions\" icon (or use the <code>Ctrl+Shift+X</code> keyboard shortcut):</p> <p> </p> </li> <li> <p>In the search bar (where is says \"Search Extensions in Marketplace\") type <code>\"wsl\"</code>, find the \"WSL\" extension in the list and then click the blue \"Install\" button next to it:</p> <p> </p> <p>Once installed, you should see a page similar to the one below:</p> <p> </p> <p>On this page, it should state that <code>\"This extension is enabled globally\"</code> (as shown in red), and you should also see a green icon with two arrows in the bottom left-hand corner of the application window.</p> </li> <li> <p>You can close down VS Code now, we'll launch it again when we need it.</p> </li> </ol>"},{"location":"wsl-ros/first-run/","title":"Launching WSL-ROS for the First Time","text":"<p>Click the Windows Start Menu button: </p> <p>Then, start typing <code>\"wsl-ros\"</code> and click on the application shortcut that should then appear in the list:</p> <p></p> <p>You will be presented with the following screen:</p> <p></p> <p>WSL-ROS is now being installed from the custom OS image, which may take a couple of minutes to complete.  Once it's done, the Windows Terminal should automatically launch:</p> <p></p> <p>This is an Ubuntu Terminal Instance, giving us access to the Ubuntu 20.04 operating system that we have just installed. This is the WSL-ROS Environment!</p> <p>You're all set up and ready to go!</p>"},{"location":"wsl-ros/installing/","title":"Installing ROS on your own PC","text":"<p>We've set up the WSL-ROS environment specifically for these ROS Courses, to ensure that you have all the right packages and tools available to you. Of course, this requires you to work on a University Managed Computer and - naturally - you may want to be able to work through this course material (and explore further) on your own device instead.</p>"},{"location":"wsl-ros/installing/#install-ros","title":"Installing ROS and Course Dependencies on Ubuntu 20.04","text":"<p>If you already have Ubuntu 20.04 running on a machine then follow the steps below to install ROS and all the additional packages required for these ROS Courses:</p> <ol> <li> <p>Install ROS Noetic (the instructions that follow are largely taken from the ROS.org website):</p> <ol> <li> <p>Set up your computer to accept software from packages.ros.org:</p> <pre><code>sudo sh -c 'echo \"deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main\" &gt; /etc/apt/sources.list.d/ros-latest.list'\n</code></pre> </li> <li> <p>Set up your keys (using <code>curl</code>): </p> <pre><code>curl -s https://raw.githubusercontent.com/ros/rosdistro/master/ros.asc | sudo apt-key add -\n</code></pre> </li> <li> <p>Make sure your package index is up-to-date:</p> <pre><code>sudo apt update\n</code></pre> </li> <li> <p>Install the \"Desktop-Full\" version of ROS:</p> <pre><code>sudo apt install ros-noetic-desktop-full\n</code></pre> </li> </ol> </li> <li> <p>Set up your environment.</p> <ol> <li> <p>A script must be sourced in every bash terminal you use ROS in, so it's best to add a line to the end of your <code>~/.bashrc</code>:</p> <pre><code>source /opt/ros/noetic/setup.bash\n</code></pre> </li> <li> <p>Re-source your environment for the changes to take effect:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Install dependencies for building packages</p> <p>\"Up to now you have installed what you need to run the core ROS packages. To create and manage your own ROS workspaces, there are various tools and requirements that are distributed separately. For example, rosinstall is a frequently used command-line tool that enables you to easily download many source trees for ROS packages with one command.\"</p> <pre><code>sudo apt install python3-rosdep python3-rosinstall python3-rosinstall-generator python3-wstool build-essential\n</code></pre> </li> <li> <p>Initialize <code>rosdep</code>:</p> <p>\"Before you can use many ROS tools, you will need to initialize rosdep. rosdep enables you to easily install system dependencies for source you want to compile and is required to run some core components in ROS.\"</p> <p><pre><code>sudo rosdep init\n</code></pre> <pre><code>rosdep update\n</code></pre></p> </li> <li> <p>Install 'Catkin Tools'. This is optional. By default, <code>catkin_make</code> can be used to invoke CMake for building ROS packages, but we use <code>catkin build</code> instead (because it's a bit nicer!) See how to install Catkin Tools here, if you want to. </p> </li> <li> <p>Create and build a Catkin Workspace:</p> <ol> <li> <p>Make the directories:</p> <pre><code>mkdir -p ~/catkin_ws/src\n</code></pre> </li> <li> <p>Navigate into the root folder:</p> <pre><code>cd ~/catkin_ws/\n</code></pre> </li> <li> <p>Then if you installed Catkin Tools:</p> <pre><code>catkin build\n</code></pre> <p>... and if not, then its:</p> <pre><code>catkin_make\n</code></pre> </li> <li> <p>Then you need to add this to the end of your <code>~/.bashrc</code> as well:</p> <pre><code>source ~/catkin_ws/devel/setup.bash\n</code></pre> </li> <li> <p>And finally, re-source:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>Install the TurtleBot3 packages, based on the Robotis instructions:</p> <ol> <li> <p>First, install dependencies:</p> <pre><code>sudo apt-get install ros-noetic-joy ros-noetic-teleop-twist-joy \\\nros-noetic-teleop-twist-keyboard ros-noetic-laser-proc \\\nros-noetic-rgbd-launch ros-noetic-rosserial-arduino \\\nros-noetic-rosserial-python ros-noetic-rosserial-client \\\nros-noetic-rosserial-msgs ros-noetic-amcl ros-noetic-map-server \\\nros-noetic-move-base ros-noetic-urdf ros-noetic-xacro \\\nros-noetic-compressed-image-transport ros-noetic-rqt* ros-noetic-rviz \\\nros-noetic-gmapping ros-noetic-navigation ros-noetic-interactive-markers\n</code></pre> </li> <li> <p>Then, install the TurtleBot3 packages themselves:</p> <p><pre><code>sudo apt install ros-noetic-dynamixel-sdk\n</code></pre> <pre><code>sudo apt install ros-noetic-turtlebot3-msgs\n</code></pre> <pre><code>sudo apt install ros-noetic-turtlebot3\n</code></pre> <pre><code>sudo apt install ros-noetic-turtlebot3-simulations\n</code></pre></p> </li> <li> <p>And then add some environment variables to your <code>~/.bashrc</code> too, in order for ROS and the TurtleBot3 packages to launch correctly:</p> <pre><code>export TURTLEBOT3_MODEL=waffle\nexport ROS_MASTER_URI=http://localhost:11311\nexport ROS_HOSTNAME=localhost\n</code></pre> </li> </ol> </li> <li> <p>Next, install some other useful Python tools:</p> <ol> <li> <p>Install Pip:</p> <pre><code>sudo apt install python3-pip\n</code></pre> </li> <li> <p>And use Pip to install Pandas:</p> <pre><code>pip3 install pandas\n</code></pre> </li> </ol> </li> <li> <p>Finally, install the Course Repo:</p> <ol> <li> <p>Navigate to your Catkin Workspace:</p> <pre><code>cd ~/catkin_ws/src/\n</code></pre> </li> <li> <p>Download the repo from GitHub:</p> <pre><code>git clone https://github.com/tom-howard/COM2009.git\n</code></pre> </li> <li> <p>Then, if you installed Catkin Tools:</p> <pre><code>catkin build\n</code></pre> <p>... if not, do this:</p> <pre><code>cd ~/catkin_ws/ &amp;&amp; catkin_make\n</code></pre> </li> <li> <p>And finally, re-source again:</p> <pre><code>source ~/.bashrc\n</code></pre> </li> </ol> </li> <li> <p>For convenience, we use some Bash Aliases to make it easier to call some common ROS commands. You might want to create a <code>~/.bash_aliases</code> file with the following content (or add to an existing one):</p> <pre><code>alias tb3_teleop=\"roslaunch turtlebot3_teleop turtlebot3_teleop_key.launch\"\nalias tb3_world=\"roslaunch turtlebot3_gazebo turtlebot3_world.launch\"\nalias tb3_empty_world=\"roslaunch turtlebot3_gazebo turtlebot3_empty_world.launch\"\nalias tb3_slam=\"roslaunch turtlebot3_slam turtlebot3_slam.launch\"\nalias tb3_rviz=\"roslaunch tuos_ros_simulations rviz.launch\"\n# This one's quite useful too:\nalias src=\"echo 'Sourcing bashrc...' &amp;&amp; source ~/.bashrc\"\n</code></pre> </li> </ol>"},{"location":"wsl-ros/installing/#wsl","title":"Installing on Windows using WSL","text":"<p>Refer to the Windows Subsystem for Linux Documentation for instructions on how to install and use WSL on Windows 10 or 11.</p> <p>Ubuntu 20.04 should be installed by default when you install WSL but if not, or if you want to create an additional WSL distribution then see here.</p> <p>We'd also recommend installing the Windows Terminal App.</p> <p>Launch your Ubuntu 20.04 distro and then follow the steps for Installing ROS (and dependencies) above.</p> <p>Graphical Applications are only supported natively in WSL when running on Windows 11 so if you're running Windows 10, then you will need to follow the additional steps below to get GUI apps (such as Gazebo and RViz) working...</p>"},{"location":"wsl-ros/installing/#running-graphical-applications-in-wsl-on-windows-10","title":"Running Graphical Applications in WSL on Windows 10","text":"<p>First, you'll need to install the VcXsrv Windows X Server. You'll need to make sure you have this running before trying to launch any GUI applications from WSL (Gazebo simulations etc.) To make this easier, we've created a configuration file. Download this file, save it on your desktop and double click it to launch an X Server on your machine with the appropriate configurations. Once launched, an icon should be visible in your notification tray in the bottom right-hand corner of the Windows Desktop:</p> <p></p> <p>In Ubuntu, you'll need to then add the following lines to your <code>~/.bashrc</code>:</p> <pre><code>export DISPLAY=$(awk '/nameserver / {print $2; exit}' /etc/resolv.conf 2&gt;/dev/null):0\nexport LIBGL_ALWAYS_INDIRECT=\nexport GAZEBO_IP=127.0.0.1\n</code></pre>"},{"location":"wsl-ros/linux-term/","title":"A Quick Introduction to the Linux Terminal","text":"<p>As explained earlier, you'll be working extensively with the Linux Terminal throughout this lab course. An idle WSL-ROS terminal instance will look like this:</p> <p></p> <p>Here, the presence of the <code>$</code> symbol indicates that the terminal is ready to accept a command. Text before the <code>$</code> symbol has two parts separated by the <code>:</code> symbol:</p> <ul> <li>Text to the left of the <code>:</code> tells us the name of the Linux user (\"student\" in this case) followed by the WSL-ROS version that you are working with.</li> <li>Text to the right of the <code>:</code> tells us where in the Linux Filesystem we are currently located (<code>~</code> means \"The Home Directory\", which is an alias for the path: <code>/home/student/</code>).</li> </ul> <p>If you don't see the <code>$</code> symbol at all, then this means that a process is currently running. To stop any running process enter <code>Ctrl+C</code> simultaneously on your keyboard.</p>"},{"location":"wsl-ros/rdp/","title":"Accessing WSL-ROS Remotely","text":"<p>WSL-ROS is also available remotely on some University machines via the University Remote Desktop Service.</p> <p>Note</p> <p>You'll need to use your university account credentials to access this service and will also need to have multifactor authentication (MFA) set up on your university account.</p> <ol> <li>The environment has been installed on all the machines in Virtual Classroom 1 (University of Sheffield sign-in required).</li> <li>From here, you should see a long list of PCs. Any that have a blue \"Connect\" button next to them are available to use.</li> <li>Click on the \"Connect\" button next to an available machine.  This will download a <code>.rdp</code> file to your computer.</li> <li>Once downloaded, double-click on the <code>.rdp</code> file and log in to the PC using your university account credentials. (Look out for an MFA notification on your MFA-linked device.)</li> <li>Once you're logged into the remote machine follow the steps in the Launching WSL Section to launch the environment. </li> <li>When you're finished, close the RDP connection and then close the Remote Desktop app on your machine.</li> </ol> <p>Tip</p> <p>Only the computers in Virtual Classroom 1 have the WSL-ROS environment installed, so make sure you only use this room if you want to work on this remotely.</p>"},{"location":"wsl-ros/rdp/#its","title":"Remote Access Support","text":"<p>Note</p> <p>We (the Teaching Team) are not able to deal with issues relating to the University Remote Desktop Service!</p> <p>If you are facing any issues then you should have a look at this IT Services support page. You can also contact the IT Services Helpdesk for help and support (Monday-Friday between 8am-5pm).</p> <p>If you're having issues related to WSL-ROS however, then please let the Teaching Team know.</p>"},{"location":"wsl-ros/vscode/","title":"Launching Visual Studio Code in WSL-ROS","text":"<p>Info</p> <p>On the University Managed Desktops, you should follow these steps to launch VS Code every time you need to use it!</p>"},{"location":"wsl-ros/vscode/#the-top","title":"Procedure","text":"<ol> <li> <p>Launch VS Code from the Windows Application Menu by clicking the Windows Start Menu button: </p> </li> <li> <p>Type <code>\"vscode\"</code> (or just <code>\"code\"</code> works as well) and click on the application shortcut that should then appear in the list:</p> <p> </p> </li> <li> <p>You should have already installed the \"WSL\" extension. If so, then a green icon should be visible in the bottom left-hand corner of the application window:</p> <p> </p> <p>Click the green icon and then click the <code>\"New WSL Window using Distro...\"</code> option in the menu that appears:</p> <p> </p> </li> <li> <p>Then, click on <code>\"WSL-ROS\"</code> to select it (it should be the only one in the list).</p> <p> </p> </li> <li> <p>A new VS Code instance should launch, and in this one you should see the green \"Remote\" icon in the bottom left-hand corner again, but this time the icon should also contain the text: <code>\"WSL: WSL-ROS\"</code>, indicating that the remote extension has been launched inside WSL-ROS:</p> <p> </p> </li> <li> <p>Keep this new VS Code instance open, but close down the original one behind it.</p> </li> <li> <p>Next, access the WSL-ROS filesystem by:</p> <ol> <li>Clicking the \"Explorer\" icon in the left-hand toolbar (or use the <code>Ctrl+Shift+E</code> keyboard shortcut),</li> <li>Clicking the blue <code>\"Open Folder\"</code> button,</li> <li>Clicking <code>\"OK\"</code> to select the default <code>/home/student/</code> filesystem location.</li> </ol> <p> </p> </li> <li> <p>Finally (we're nearly there, I promise!), you should be presented with a pop-up, asking if you trust the <code>/home/student [WSL: WSL-ROS]</code> folder. Tick the checkbox and click on the blue <code>\"Yes, I trust the authors\"</code> button:</p> <p> </p> </li> <li> <p>You can now navigate the WSL-ROS filesystem in the explorer window on the left-hand side of the VS Code screen. You'll need to use this to locate the packages and scripts that you create throughout this course!</p> <p> </p> </li> </ol>"},{"location":"wsl-ros/vscode/#verify","title":"Always make sure that the \"WSL\" extension is enabled!!","text":"<p>Check that you can always see this icon in the bottom left-hand corner of your VS Code screen:</p> <p></p> <p>Warning</p> <p>If you don't see this then start again from the top!</p>"},{"location":"wsl-ros/wsl-ros-return/","title":"Re-Launching the Environment","text":"<p>As discussed earlier, the WSL-ROS Environment that you created earlier will only be preserved for a limited time on the machine that you installed it on!</p> <p>Warning</p> <p>Any work that you do within WSL-ROS will not be preserved between sessions or across different University machines automatically!</p> <p>At the beginning of each practical session (or any other time you want to work in WSL-ROS) you'll need to re-install the environment from the OS image. You must therefore make sure that you back up your work to your University U: Drive every time you finish working in the environment, so that you can then restore it the next time you return (we'll remind you about this at the start and end of every practical session, just in case you forget). </p> <p>The WSL-ROS Environment will, however, be preserved for a limited time if you happen to log in to the same University machine within a few hours. If this is the case, then on launching WSL-ROS you will be presented with the following message:</p> <p></p> <p>Enter <code>Y</code> to continue where you left things previously, or <code>N</code> to start from a fresh installation.</p> <p>Warning</p> <p>If you select <code>N</code> then any work that you have created in the existing environment will be deleted! Always make sure you back up your work using the procedure outlined below!</p>"},{"location":"wsl-ros/wt-settings/","title":"Windows Terminal Settings","text":"<p>We use the Windows Terminal alongside WSL-ROS, to interact with our Ubuntu and ROS environment, and we've created a custom settings file to configure this appropriately. If you've used the Windows Terminal before on a University Machine, or if you have used the WSL-ROS environment previously and have modified the settings file that we have provided, then you'll be presented with the following prompt:</p> <p></p> <p>Enter <code>Y</code> to use the settings file that we have provided and overwrite whatever was there previously (recommended). Alternatively, enter <code>N</code> to preserve your own settings, but note that your experience will then differ to that presented throughout this Wiki, and some instructions may no longer work in the same way.</p>"}]}